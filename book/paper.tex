\documentclass{report}
\usepackage{fullpage}
\usepackage{imakeidx}\makeindex
\usepackage[nottoc,notlot,notlof]{tocbibind}
\usepackage[toc]{glossaries}\makeglossaries
\usepackage{graphicx}
\usepackage{framed}
\usepackage{tcolorbox}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{etoolbox}
% \usepackage{bold-extra}
\usepackage{xcolor}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue]{hyperref}
\def\itemautorefname{Exercise}%
\def\chapterautorefname{Chapter}%
\def\figureautorefname{Figure}%
\def\sectionautorefname{Section}%

\AtBeginEnvironment{enumerate}{\everymath{\displaystyle}}
\newlist{problems}{enumerate}{2}
\setlist[problems]{wide=0pt}
\setlist[problems,1]{label=\thechapter.\arabic*, font=\bfseries, wide=0pt}
\setlist[problems,2]{label=(\alph*), wide =0.5em, topsep=2pt, itemsep=2pt}

\widowpenalty 10000
\clubpenalty 10000
\tolerance=9999
\newcommand{\tm}{\raisebox{.9ex}{\tiny tm}}

\newcommand{\harmonysource}[1]{
\begin{tabbing}
XX\=XXX\=XXX\kill
    \input{sources/#1.tex}
\end{tabbing}
}

\newcommand{\harmonylink}[1]{%
[\href{https://www.cs.cornell.edu/home/rvr/harmony/#1}{\underline{#1}}]%
}

\newcommand{\harmonyref}[2]{%
\href{https://www.cs.cornell.edu/home/rvr/harmony/output/#1}{\underline{#2}}%
}

\newglossaryentry{actor model}
{
  name=actor model,
  description={is a concurrency model where there are no shared variables, only
      threads with private variables that communicate through message passing}
}
\newglossaryentry{atomicity}
{
  name=atomicity,
  description={describes that a certain machine instruction or sequence of machine
    instructions is executed indivisibly by a thread and cannot be interleaved with 
    machine instructions of another thread}
}
\newglossaryentry{barrier synchronization}
{
  name=barrier synchronization,
  description={is when a set of threads execute in rounds, waiting for one another
    to complete each round}
}
\newglossaryentry{blocked thread}
{
  name=blocked thread,
  description={is a thread that cannot change the state or terminate or
      can only do so after another thread changes the state first.
      For example, a thread that is waiting for a lock to become available}
}
\newglossaryentry{busy waiting}
{
  name=busy waiting,
  description={(aka spin-waiting) is when a thread waits in a loop for some
      application-defined condition instead of blocking}
}
\newglossaryentry{concurrent execution}
{
  name=concurrent execution,
  description={(aka parallel execution) is when there are multiple threads executing and
      their machine instructions are interleaved in an unpredictable manner}
}
\newglossaryentry{condition variable}
{
  name=condition variable,
  description={a variable that keeps track of which threads are waiting for a
    specific application-level condition.  The variable can be waited on as well
    as signaled or notified}
}
\newglossaryentry{context}
{
  name=context,
  description={(aka continuation) describes the state of a running thread,
    including its program counter, the values of its variables (stored in its
    register), and the contents of its stack}
}
\newglossaryentry{conditional critical section}
{
  name=conditional critical section,
  description={is a critical section with, besides mutual exclusion, additional conditions on
      when a thread is allowed to enter the critical section}
}
\newglossaryentry{critical section}
{
  name=critical section,
  description={(aka critical region) is a set of instructions that only one thread
    is allowed to execute at a time.  The instructions are, however, not executed
    atomically, as other threads can continue to execute and access shared
    variables}
}
\newglossaryentry{deadlock}
{
  name=deadlock,
  description={is when there are two or more threads waiting indefinitely for one
    another to release a resource}
}
\newglossaryentry{determinism}
{
  name=determinism,
  description={is when the outcome of an execution is uniquely determined by the
    initial state}
}
\newglossaryentry{fairness}
{
  name=fairness,
  description={is when each thread eventually can access each resource it needs to access
    with high probability}
}
\newglossaryentry{interlock instruction}
{
  name=interlock instruction,
  description={a machine instruction that involves multiple memory load and/or store
    operations, executed atomically}
}
\newglossaryentry{invariant}
{
  name=invariant,
  description={is a binary predicate over states that must hold for every
  reachable state of a thread}
}
\newglossaryentry{linearizable}
{
  name=linearizable,
  description={is a consistency condition for concurrent access to an object, requiring
    that each access must appear to execute atomically sometime between the invocation
    of the access and its completion}
}
\newglossaryentry{lock}
{
  name=lock,
  description={an object that can be owned by at most one thread at a time.  Useful
    for implementing mutual exclusion}
}
\newglossaryentry{machine instruction}
{
  name=machine instruction,
  description={is an atomic operation on the Harmony virtual machine, executed by a thread}
}
\newglossaryentry{model checking}
{
  name=model checking,
  description={is a formal verification method that explores all possible executions
    of a program, which must have a finite number of states}
}
\newglossaryentry{monitor}
{
  name=monitor,
  description={is a programming language paradigm that supports mutual exclusion
    as well as waiting for resources to become available}
}
\newglossaryentry{mutual exclusion}
{
  name=mutual exclusion,
  description={is the property that two threads never enter the same critical section}
}
\newglossaryentry{non-blocking synchronization}
{
  name=non-blocking synchronization,
  description={(aka wait-free synchronization) is when access to a shared resource can be
      guaranteed in a bounded number of steps even if other threads are not making progress}
}
\newglossaryentry{thread}
{
  name=thread,
  description={is code in execution.  We do not make the distinction
      between threads and threads.  A thread has a current context and
      updates its context every time it executes a machine instruction}
}
\newglossaryentry{thread variable}
{
  name=thread variable,
  description={is a variable that is private to a single thread and stored in
        its register}
}
\newglossaryentry{producer/consumer problem}
{
  name=producer/consumer problem,
  description={is a synchronization problem whereby one or more producing threads
    submit items and one or more consuming threads want to receive them.  No item
    can get lost or forged or be delivered to more than one consumer, and producers
    and consumers should block if resources are exhausted}
}
\newglossaryentry{property}
{
  name=property,
  description={describes a set of execution traces or histories that are allowed by
    a program.  Safety properties are properties in which ``no bad things happen,''
    such as violating mutual exclusion in a critical section.  Liveness properties are
    properties where ``something good eventually happens,'' like threads being
    able to enter the critical section if they want to}
}
\newglossaryentry{race condition}
{
  name=race condition,
  description={describes when multiple threads access shared state concurrently,
    leading to undesirable outcomes}
}
\newglossaryentry{reader/writer lock}
{
  name=reader/writer lock,
  description={is a lock on a resource that can be held by multiple threads if they all 
    only read the resource}
}
\newglossaryentry{semaphore}
{
  name=semaphore,
  description={is a counter that can be atomically incremented and decremented,
    but blocks the thread until the counter is larger than zero first}
}
\newglossaryentry{sequential execution}
{
  name=sequential execution,
  description={is when there is just one thread executing, as opposed to concurrent
    execution}
}
\newglossaryentry{shared variable}
{
  name=shared variable,
  description={is a variable that is stored in the memory of the Harmony virtual machine and
      shared between multiple threads, as opposed to a thread variable}
}
\newglossaryentry{spinlock}
{
  name=spinlock,
  description={is an implementation of a lock whereby a thread loops until the
    lock is available, at which point the thread atomically obtains the lock}
}
\newglossaryentry{stack machine}
{
  name=stack machine,
  description={is a model of computing where the state of a thread is kept on
    a stack.  Harmony uses a combination of a stack machine and a register-based machine}
}
\newglossaryentry{starvation}
{
  name=starvation,
  description={is when a thread cannot make progress because it is continuously
    losing a competition with other threads to get access to a resource}
}
\newglossaryentry{state}
{
  name=state,
  description={an assignment of values to variables.  In a Harmony virtual machine, this includes
      the contents of its shared memory and the set of contexts}
}
\newglossaryentry{step}
{
  name=step,
  description={is the execution of a machine instruction by a thread, updating
        its state.  Harmony distinguishes micro steps (machine instructions) and macro
        step (a sequence of instructions with at most one effect visible by
        other threads)}
}
\newglossaryentry{thread safety}
{
  name=thread safety,
  description={is when the implementation of a data structure allows concurrent access
    with well-defined semantics}
}
\newglossaryentry{trace}
{
  name=trace,
  description={is a sequence of steps, starting from an initial state.  An infinite
    trace is also called a \emph{behavior}}
}

% \renewcommand{\topfraction}{.99}
% \renewcommand{\bottomfraction}{.99}
% \renewcommand{\textfraction}{.01}
% \renewcommand{\floatpagefraction}{.9}
% \renewcommand{\dbltopfraction}{.99}
% \renewcommand{\dblfloatpagefraction}{.9}

\newenvironment{code}{
\tcolorbox
}{
\endtcolorbox
}

\title{Concurrent Programming in Harmony}
\author{Robbert van Renesse}

\begin{document}
\maketitle

Permission is granted to copy, distribute and/or modify this
document under the terms of the
Creative Commons AttributionNonCommercial-ShareAlike 4.0 International
(CC BY-NC-SA 4.0) at
\url{http://creativecommons.org/licenses/by-nc-sa/4.0}.

\tableofcontents

\chapter{On Concurrent Programming}

Programming with concurrency is hard.  On the one hand concurrency
can make programs faster than sequential ones, but having multiple
\index{thread}%
threads read and update shared variables
\index{shared variable}%
concurrently and synchronize with one another makes programs more
complicated than programs where only one thing happens at a time.
%
\glsadd{concurrent execution}%
\glsadd{shared variable}%
\glsadd{sequential execution}%
\glsadd{determinism}%
\glsadd{atomicity}%
%
Why are concurrent
programs more complicated than sequential ones?
There are, at least, two reasons:
\begin{itemize}
\item The execution of a sequential
\index{sequential}%
program is mostly \emph{deterministic}.
\index{determinism}%
If you run it twice with the same input, the same output will be produced.
Bugs are typically easily reproducible and easy to track down, for example
by instrumenting the program.
On the other hand,
the output of running concurrent programs depends on how the
execution of the various threads are \emph{interleaved}.
Some bugs may occur only occasionally and
may never occur when the program is instrumented to find them
(so-called \emph{Heisenbugs}---overhead caused by instrumentation
leads to timing changes that makes such bugs less likely to occur).
\index{Heisenbug}%
\item In a sequential program, each statement and each function can be
thought of as happening \emph{atomically} (indivisibly)
\index{atomicity}%
because there is no other activity interfering with their execution.
Even though a statement or function may
be compiled into multiple machine instructions, they are executed back-to-back
until completion.  Not so with a concurrent program, where other threads
may update memory locations while a statement or function is being executed.
\end{itemize}
The lack of determinism and atomicity in concurrent programs make them
not only hard to reason about, but also hard to test.
\index{test}%
Running the same test of concurrent code twice is likely to produce
two different results.  More problematically, a test may trigger a
bug only for certain ``lucky'' executions.  Due to the probabilistic
nature of concurrent code, some bugs may be highly unlikely to get
triggered even when running a test millions of times.  And even if
a bug does get triggered, the source of the bug may be hard to find
because it is hard to reproduce.

\glsadd{model checking}%

This book is intended to help people with understanding and
developing concurrent code.  In particular, it uses a new tool
called Harmony that helps with \emph{testing} concurrent code.
The approach is based on \emph{model checking}:
\index{model checking}%
instead of relying
on luck, Harmony will run \emph{all possible executions} of a particular
test program.  So even if a bug is unlikely to occur, if the test
\emph{can} expose the bug it \emph{will}.  Moreover, if the bug is
found, the model checker precisely shows how to trigger the bug in
the smallest number of steps.

Model checking is not a replacement for formal verification.
\index{formal verification}%
Formal verification proves that a program is correct.  Model checking only
verifies that a program is correct for some \emph{model}.  Think of
a model as a test program.
Because model checking tries every possible execution, the test
program needs to be simple. Otherwise it may take longer than we
care to wait for or run out of memory.
In particular, the model needs to have a relatively small number of
reachable states.

So if model checking does not prove a program correct, why is it
useful?
To answer that question, let us consider a sorting algorithm.
Suppose we create a test program, a model, that tries sorting
\emph{all} lists of up to five numbers chosen from the set
\{ 1, 2, 3, 4, 5 \}.  Model checking proves that for those particular
scenarios the sorting algorithm works: the output is a sorted
permutation of the input.  In some sense it is an excellent test:
it will have considered all \emph{corner cases},
\index{corner case}%
including lists where all
numbers are the same, lists that are already sorted or reversely
sorted, etc.  If there is a bug in the sorting algorithm, most
likely it would be triggered and the model checker would produce a
scenario that would make it easy to find the source of the bug.

However, if the model checker does not find any bugs, we do not
know for sure that the algorithm works for lists with more than
five numbers or for lists that have values other than the numbers
1 through 5.  Still, we would expect that the likelihood that there
are bugs remaining in the sorting algorithm is small.
That said, it would be easy to write a program
that sorts all lists of up to five numbers correctly but fails to
do so for a list of 6 numbers.  (Hint: simply use an \textbf{if}
statement.)

\glsadd{invariant}%

While model checking does not in general prove an algorithm correct,
it can help with proving an algorithm correct.
The reason is that many correctness properties can be proved using
\emph{invariants}:
\index{invariant}%
predicates that must hold for every state in the
execution of a program.  A model checker can find violations of
proposed invariants when evaluating a model and provide valuable early
feedback to somebody who is trying to construct a proof, even an
informal one.
We will include examples
of such invariants as they often provide excellent insight into
why a particular algorithm works.

So what is Harmony?
Harmony is a concurrent programming language.  It was designed to teach
the basics of concurrent programming, but it is also useful for
testing new concurrent algorithms or even sequential and distributed
algorithms.  Harmony programs are not intended to be ``run'' like programs
in most other programming languages---instead Harmony programs are
model checked to test that the program has certain desirable
properties and does not suffer from bugs.

The syntax and semantics of Harmony is similar to that of Python.
Python is familiar to many programmers and is easy to learn and
use.  We will assume that the reader is familiar with the basics
of Python programming.  We also will assume that the reader
understands some basics of machine architecture and how programs
are executed.  For example, we assume that the reader is familiar
with the concepts of CPU, memory, register, stack, and machine
instructions.

Harmony is heavily influenced by Leslie Lamport's work on
TLA+, TLC, and PlusCal~\cite{Lamport02, Lamport09}.
Harmony is designed to have a lower learning curve than those
systems, but is not as powerful.  When you finish this book
and want to learn more, we strongly encourage checking
those out.
Another excellent resource is Fred Schneider's book ``On
Concurrent Programming''~\cite{Schneider97}.
(This chapter is named after that book.)

The book proceeds as follows:

\begin{itemize}
\item \autoref{ch:harmonyintro} introduces the Harmony programming
language, as it provides the language for presenting synchronization
problems and solutions.
\item \autoref{ch:concurrent} illustrates the problem of
concurrent programming through a simple example in which two threads
are concurrently incrementing a counter.
\item \autoref{ch:harmonymachine} presents the
Harmony virtual machine to understand the problem
underlying concurrency better.
\item \autoref{ch:critical} introduces the concept of a
\emph{critical section} and presents various flawed implementations
of critical sections to demonstrate that implementing a critical section
is not trivial.
\item \autoref{ch:peterson} introduces \emph{Peterson's Algorithm}, an
elegant solution to implementating a critical section.
\item \autoref{ch:method} gives some more details on the Harmony
language needed for the rest of the book.
\item \autoref{ch:spinlock} introduces ``hardware'' \emph{locks}
for implemented critical sections.
\item \autoref{ch:synch} presents Harmony modules
that supports locks and other synchronization primitives.
\item \autoref{ch:cds} gives an introduction to building concurrent
data structures.
\item \autoref{ch:condwait} talks about threads having to wait for
certain conditions.  As examples, it presents the reader/writer lock
problem and the bounded buffer problem.
\item \autoref{ch:sbs} presents \emph{Split Binary Semaphores}, a
general technique for solving synchronization problems.
\begin{comment}
\autoref{ch:rdwrbusy} also discusses \emph{busy-waiting}, which is
an easy but undesirable approach to synchronize threads.
\item \autoref{ch:semaphore} introduces \emph{semaphores},
a generalization of locks
that is good not only for mutual exclusion but also for waiting for
certain application-level conditions.
\item \autoref{ch:bb} and \autoref{ch:sbs} introduce the
\emph{bounded buffer problem} (aka \emph{producer/consumer problem})
and various solutions.
\end{comment}
\item \autoref{ch:starvation} talks about \emph{starvation}:
the problem that in some
synchronization approaches threads may not be able to get access to a
resource they need.
\item \autoref{ch:monitors} presents
\emph{monitors} and \emph{condition variables},
another approach to thread synchronication.
\item \autoref{ch:deadlock} describes \emph{deadlock}
where a set of threads are indefinitely waiting for one another to
release a resource.
\item \autoref{ch:actor} presents the \emph{actor model}
and \emph{message passing} as an approach to synchronization.
\item \autoref{ch:interrupts} discusses how to handle interrupts,
a problem closely related to synchronization among multiple threads.
\item \autoref{ch:abp} presents a problem and a solution to the distributed
systems problem of having two threads communicate reliably over an unreliable
network.
\item \autoref{ch:nonblocking} introduces \emph{non-blocking} or
\emph{wait-free} synchronization algorithms,
which prevent threads waiting for one another more than a bounded number of
steps.
\item \autoref{ch:barrier} describes \emph{barrier synchronization},
useful in high-performance computing applications such as parallel simulations.
\end{itemize}

\chapter{Introduction to Harmony}
\label{ch:harmonyintro}

Like Python, Harmony is an imperative,
dynamically typed, and garbage collected programming language.
There are also some important differences:
\begin{itemize}
\item Every statement in Harmony must be terminated by a semicolon
(even \textbf{def} statements).
In Python semicolons are optional and rarely used.
There is no syntactic significance to indentation in Harmony.
Correct indentation in Harmony is encouraged but not enforced.
\item Harmony only supports basic operator precedence or associativity.
Use parentheses liberally to remove ambiguity.
\item Harmony does not (currently) support floating point, iterators, or I/O;
Harmony does support \textbf{for} loops and various ``comprehensions.''
\item Python is object-oriented, supporting classes with methods and
inheritance; Harmony has objects but does not support classes.  Harmony supports
pointers to objects and methods.
\end{itemize}
There are also less important ones that you will discover as
you get more familiar with programming in Harmony.

\begin{figure}
\begin{code}
\harmonysource{triangle}
\end{code}
\caption{\harmonylink{code/triangle.hny} Computing triangle numbers.}
\label{fig:triangle}
\end{figure}

\autoref{fig:triangle} shows a simple example of a Harmony program.
(The code for examples in this book can be found in the \texttt{code} folder under
the name listed in the caption of the example.)
The example is sequential and has a method \texttt{triangle} that takes
an integer number as argument.  Each method has a variable called
\textit{result} that eventually contains the result of the
method (there is no \textbf{return} statement in Harmony).  The method
also has a variable called \textit{n} containing the value of the
argument.  The \{ \textit{x..y} \} notation generates a set containing the numbers
from~\textit{x} to~\textit{y} (inclusive).  The last two lines in the program are
the most interesting.
The first assigns to \textit{x} some unspecified value in the range \texttt{0..N}
and the second verifies that $\mathtt{triangle}(x)$ equals $x(x+1)/2$.

``Running'' this Harmony program will try all possible executions, which
includes all possible values for $x$.  Try it out (here \texttt{\$}
represents a shell prompt):

\begin{code}
\begin{verbatim}
$ harmony triangle.hny
#states = 13 diameter = 1
#components: 13
no issues found
$
\end{verbatim}
\end{code}

(For this to work, make sure \texttt{harmony} is in your command shell's search path.)
Essentially, the $\texttt{choose}($S$)$
\index{choose operator}%
operator provides the input to the program by selecting some value from the
set~$S$, while the $\textbf{assert}$ statement checks that the output is
correct.  If the program is correct, the output of Harmony is the size of the
``state graph'' (13 states in this case).  If not, Harmony also
reports what went wrong, typically by displaying a summary of an execution in
which something went wrong.

In Harmony, constants have a default value,
but those can be overridden on the command
line using the \texttt{-c} option.
\index{constant}%
For example, if you want to test the code for \texttt{N = 100}, run:
\begin{code}
\begin{verbatim}
$ harmony -c N=100 triangle.hny
#states = 103 diameter = 1
#components: 103
no issues found
$
\end{verbatim}
\end{code}

\section*{Exercises}
\begin{problems}
\item See what happens if, instead of initializing \textit{result} to 0,
you initialize it to 1.  (You do not need to understand the error report at this time.  They will be explained in more detail in \autoref{ch:harmonymachine}.)
\item Write a Harmony program that computes squares by repeated adding.  So the program
should compute the square of $x$ by adding $x$ to an initial value of 0 $x$ times.
\end{problems}

\chapter{The Problem of Concurrent Programming}
\label{ch:concurrent}

\glsadd{thread}%

Concurrent programming, aka multithreaded programming, involves multiple
threads
\index{thread}%
running in parallel while sharing variables.
\autoref{fig:inc} presents a simple example.
The program
initializes two shared variables: an integer \textit{count} and
an array \textit{done} with two booleans.
Method \texttt{incrementer} takes a parameter called \textit{self}.
It increments \textit{count} and sets \textit{done}[\textit{self}] to \texttt{True}.
Method \texttt{main} waits until all \textit{done} flags are set to \texttt{True}.
(\textbf{await} $c$ is semantically the same as \textbf{while not} $c$: \textbf{pass}.)
After that, method \texttt{incrementer}
verifies that the value of \textit{count} equals 2.
Like Python, Harmony supports \texttt{assert} statements with two arguments:
the first is a Boolean condition, and the second is a value that is
reported if the condition fails, in this case
the actual value of \textit{count}.

The program spawns three threads.
The first runs \texttt{incrementer(0)}, the second runs
\texttt{incrementer(1)}, and the last runs \texttt{main()}.
Note that although the threads are \emph{spawned} one at a time,
they will execute concurrently.  It is, for example, quite possible
that \texttt{incrementer(1)} finishes before \texttt{incrementer(0)}
even gets going.
And because Harmony tries every possible execution, it will consider
such executions as well.

\begin{figure}[h]
\begin{code}
\harmonysource{Up}
\end{code}
\caption{\harmonylink{code/Up.hny} Incrementing twice in parallel.}
\label{fig:inc}
\end{figure}

\begin{quote}
\begin{itemize}
\item Before you run the program, what do you think will happen?  Is the
program correct in that \textit{count} will always end up being 2?
(You may assume that \texttt{load} and \texttt{store} instructions of the
underlying machine architecture are atomic (indivisible)---in fact they are.)
\end{itemize}
\end{quote}

\glsadd{machine instruction}%

What is going on is that the Harmony program is compiled to machine instructions,
\index{machine instruction}%
and it is the machine instructions that are executed by the underlying Harmony
machine.  The details of this appear in \autoref{ch:harmonymachine},
but suffice it to
say that the machine has instructions that load values from memory and store
values into memory.  Importantly, it does not have instructions to atomically
increment or decrement values in memory locations.
So to increment a value in memory,
the machine must do at least three machine instructions.  Conceptually:
\begin{enumerate}
\item load the value from the memory location;
\item add 1 to the value;
\item store the value to the memory location.
\end{enumerate}

When running multiple threads, each essentially runs an instantiation of
the machine, and they do so in parallel.  As they execute, their machine
instructions are interleaved
\index{interleaving}%
in unspecified and often random ways.
A program is correct if it works for any interleaving of threads.
Harmony will try all possible interleavings of the threads
executing machine instructions.

If the threads run one at a time, then \textit{count} will be incremented
twice and ends up being 2.  However, the following is also
a possible interleaving of incrementers~0 and~1:
\begin{enumerate}
\item incrementer 0 loads the value of \textit{count}, which is 0;
\item incrementer 1 loads the value of \textit{count}, which is still 0;
\item incrementer 1 adds one to the value that it loaded (0), and
stores $1$ into \textit{count};
\item incrementer 0 adds one to the value that it loaded (0), and
stores $1$ into \textit{count};
\item incrementer 0 sets \textit{done}[0] to \texttt{True};
\item incrementer 1 sets \textit{done}[1] to \texttt{True}.
\end{enumerate}

The result in this particular interleaving is that \textit{count} ends up
being 1.
This is known as a \emph{race condition}.
\index{race condition}%
When running Harmony, it will
report violations of assertions.  It also provides an example
of an interleaving, like the one above, in which an assertion fails.

\glsadd{race condition}%

If one thinks of the assertion as providing the specification of the
program, then clearly its implementation does not satisfy its specification.
Either the specification or the implementation (or both) must have a bug.
We could change the specification by changing the assertion as follows:

\begin{code}
\textbf{assert} (\textit{count} == 1) or (\textit{count} == 2);
\end{code}

This would fix the issue, but more likely it is the program that must
be fixed, not the specification.

\section*{Exercises}

The following exercises are intended to show you that while it is just
as easy to write concurrent programs in Python, it is much easier to
find concurrency bugs using Harmony.

\begin{problems}
\item Harmony programs can usually be easily translated into Python.  For example,
\autoref{fig:incpy} is a Python version of \autoref{fig:inc}.
\begin{enumerate}
\item Run \autoref{fig:incpy} using Python.  Does the assertion fail?
\item Using a script, run \autoref{fig:incpy} 1000 times.
For example, if you are using the bash shell (in Linux or Mac OS X, say), you can do the following:
\begin{code}
\begin{verbatim}
for i in {1..1000}
do
    python Up.py
done
\end{verbatim}
\end{code}
If you're using Windows, the following batch script does the trick:
\begin{code}
\begin{verbatim}
FOR /L %%i IN (1, 1, 1000) DO python Up.py
PAUSE
\end{verbatim}
\end{code}
How many times does the assertion fail (if any)?
\end{enumerate}
\item \autoref{fig:incmany} is a version of \autoref{fig:incpy} that has each
incrementer thread increment \textit{count} \texttt{N} times.  Run \autoref{fig:incmany}
10 times (using Python).
Report how many times the assertion fails and what the value of \textit{count}
was for each of the failed runs.
Also experiment with lower values of \texttt{N}.
How large does \texttt{N} need to be for assertions to fail?
(Try powers of 10 for \texttt{N}.)
\item Can you think of a fix to \autoref{fig:inc}?  Try one or two different fixes
and run them through Harmony.  Do not worry about having to come up with a correct fix at this
time---the important thing is to develop an understanding of concurrency.
\end{problems}

\begin{figure}
\begin{code}
\VerbatimInput[xleftmargin=5mm,numbers=left]{../python/Up.py}
\end{code}
\caption{\harmonylink{python/Up.py} Python implementation of \autoref{fig:inc}.}
\label{fig:incpy}
\end{figure}

\begin{figure}
\begin{code}
\VerbatimInput[xleftmargin=5mm,numbers=left]{../python/UpMany.py}
\end{code}
\caption{\harmonylink{python/UpMany.py} Incrementing \texttt{N} times.}
\label{fig:incmany}
\end{figure}

\chapter{The Harmony Virtual Machine}
\label{ch:harmonymachine}
\index{Harmony Virtual Machine}%

Harmony programs are compiled to Harmony \emph{bytecode}
\index{bytecode}%
(a list of machine instructions for a virtual machine),
which in turn is executed by the Harmony virtual machine (HVM).
\index{virtual machine}%
\index{Harmony Virtual Machine}%
\index{HVM}%
To understand the problem of concurrent computing, it
is important to have a basic understanding of the HVM.

\section*{Harmony Values}
\label{ap:harmonyvalues}

Harmony programs, and indeed the HVM,  manipulate Harmony values.
Harmony values are recursively defined:
they include booleans (\texttt{False} and \texttt{True}),
integers (but not floating point numbers),
strings (enclosed by double quotes),
sets of Harmony values,
and dictionaries
\index{dictionary}%
that map Harmony values to other Harmony values.
%
Another type of Harmony value is the \emph{atom}.
\index{atom}%
It is essentially
just a name.  An atom is denoted using a period followed by the
name.  For example, \texttt{.main} is an atom.

Harmony makes extensive use of dictionaries.
A dictionary maps values, known as \emph{keys}, to values.
The Harmony dictionary syntax and properties are a little different from Python.
Unlike Python, any Harmony value can be a key, including another
dictionary.
Harmony dictionaries are written as
$\mathtt{dict}\{ k_0: v_0, ~ k_1: v_1, ~ ... \}$.
If $d$ is a dictionary, and $k$ is a key, then the
following expression retrieves the Harmony value that $k$ maps to in $d$:
\begin{code}
$d$ $k$
\end{code}
This is unfamiliar to Python programers, but in Harmony square brackets can be used
in the same way as parentheses, so you can express the same thing in the form
that is familiar to Python programmers:
\begin{code}
$d$[$k$]
\end{code}
However, if $d$ = \texttt{dict\{ .count: 3 \}}, then you can write
$d$.\texttt{count} (which has value 3) instead of having to write
$d$\texttt{[.count]} (although both will work).
Thus using atoms, a dictionary can be made to look much like a Python object.
The meaning of $d$ $a$ $b$ $...$ is ((($d$ $a$) $b$) $...$).

Tuples are special forms of dictionaries where the keys are
the indexes into the tuple.  For example, the tuple
\texttt{(5, False)} is the same Harmony value as
\texttt{dict\{ 0:5, 1:False \}}.
The empty tuple \texttt{()} is the same value as \texttt{dict\{\}}.
Note that this is different from the empty set, which is \texttt{\{\}}.
As in Python, you can create singleton tuples by including a comma.
For example, \texttt{(1,)}.

Again, square brackets and parentheses work the same in Harmony, so
\texttt{[a, b, c]} (which looks like a Python list)
is the same Harmony value as \texttt{(a, b, c)} (which looks like a Python tuple),
which in turn is the same Harmony value as \texttt{dict\{ 0:a, 1:b, 2:c \}}.
So if $x$ = \texttt{[False, True]},
then $x$[0] = \texttt{False} and $x$[1] = \texttt{True}, just like in Python.
However, when creating a singleton list, make sure you include the
comma, as in \texttt{[False,]}.  The expression \texttt{[False]} just means
\texttt{False}.

Harmony is not an object-oriented language, so objects don't have
built-in methods.  However, Harmony does have some powerful operators to
make up for some of that.
For example, dictionaries have two handy unary operators.
If $d$ is a
dictionary, then \texttt{keys}~$d$ (or equivalently \texttt{keys}($d$))
returns the set of keys and \texttt{len}~$d$ returns the size of
this set.

Appendix~\ref{ap:harmonyvalues} provides details on all the values that
Harmony currently supports.

\section*{Harmony Bytecode}

A Harmony program is translated into HVM bytecode.
The HVM is not an ordinary virtual machine, but its architecture
is nonetheless representative of conventional computers and
virtual machines such as the Java Virtual Machine.

Instead of bits and bytes, a HVM manipulates Harmony values.
A HVM has the following components:
\begin{itemize}
\item Code:  This is an immutable and finite list of HVM instructions,
generated from a Harmony program.  The types of instructions will be described later.
\item Shared memory: A HVM has just one memory location containing
a Harmony value.
\item Threads:  Any thread
can spawn an unbounded number of other threads and threads may terminate.
Each thread has an immutable (but not necessarily unique) \emph{name tag},
\index{name tag}%
a program counter,
\index{program counter}%
a stack of Harmony values,
and a single mutable general purpose \emph{register}
\index{register}%
that contains a Harmony value.
\end{itemize}

A name tag consists of the name of the main method of the thread,
along with an optional tag specified in the \texttt{spawn}
\index{spawn statement}%
\index{tag}%
statement.
The default tag is the argument to the method.
A name tag is represented as a dictionary with keys \texttt{.name}
and \texttt{.tag}, but we shall often abbreviate it using the notation
\texttt{name/tag}.
For example, in \autoref{fig:inc}, the created threads have name tags
\texttt{incrementer/0}, \texttt{incrementer/1}, and \texttt{main/()}.

\glsadd{context}%

The state of a thread is called a \emph{context} (aka \emph{continuation}):
\index{context}%
\index{continuation}%
it contains the values of
its name tag, program counter, stack, and register.
The state of a HVM
consists of the value of its memory and the multiset (or \emph{bag})
\index{bag}%
\index{multiset}%
of contexts.  It is a multiset of contexts because two different threads can
have the same context.
The initial state of the Harmony memory is the empty dictionary, \texttt{dict\{\}}.
The context bag has an initial context in it with name tag
\texttt{\_\_init\_\_/()}, pc 0, register \texttt{()}, and an empty stack.
Each machine instruction updates the state in some way.
% \autoref{fig:incstate} shows an example of a reachable state for the program in \autoref{fig:inc}.

\begin{figure}
\begin{code}
\begin{verbatim}
code/Up.hny:1 count = 0;
   0 Push 0
   1 Store count
code/Up.hny:2 done = [ False, False ];
   2 Push [False, False]
   3 Store done
code/Up.hny:4 def incrementer(self):
   4 Jump 31
   5 Frame incrementer self
code/Up.hny:5     count = count + 1;
   6 Load count
   7 Push 1
   8 2-ary +
   9 Store count
code/Up.hny:6     done[self] = True;
   10 Push ?done
   11 LoadVar self
   12 Address
   13 Push True
   14 Store
   15 Return
code/Up.hny:8 def main():
   16 Jump 31
   17 Frame main ()
code/Up.hny:9     await all(done);
   18 Load done
   19 1-ary all
   20 JumpCond False 18
code/Up.hny:10     assert count == 2, count;
   21 ReadonlyInc
   22 AtomicInc
   23 Load count
   24 Push 2
   25 2-ary ==
   26 Load count
   27 Assert2
   28 AtomicDec
   29 ReadonlyDec
   30 Return
code/Up.hny:13 spawn incrementer(0);
   31 Push 0
   32 Dup
   33 Push PC(5)
   34 Spawn
\end{verbatim}
\end{code}
\caption{The first part of the HVM bytecode corresponding to \autoref{fig:inc}.}
\label{fig:inccode}
\end{figure}

It may seem strange that there is only one memory location and that each
thread has only one register.  However, this is not a limitation because
Harmony values are unbounded trees.
Both the memory and the register of a thread always contain
a dictionary that maps atoms to Harmony values.  We call this a \emph{directory}.
\index{directory}%
A directory represents the state of a collection of variables named by the atoms.
%
Because directories are Harmony values themselves,
directories can be organized into a tree.
Each node in a directory tree is then identified
by a sequence of atoms, like a path name in the file system hierarchy.  We call
such a sequence the \emph{address}
\index{address}%
of a Harmony value, and it is relative to the
``root'' of the directory tree.
For example, in \autoref{fig:inc} the memory is a dictionary with two
entries: \texttt{.count} and \texttt{.done}.  And the value of entry
\texttt{.done} is a dictionary with keys 0 and 1.

\begin{figure}
\begin{code}
\begin{verbatim}
#states = 71 diameter = 5                                                                              ==== Safety violation ====
P0: __init__/()   [0-4,31-43] 43 { .count: 0, .done: [False, False] }
P1: incrementer/0 [5-8]        9 { .count: 0, .done: [False, False] }
P2: incrementer/1 [5-15]      15 { .count: 1, .done: [False, True ] }
P1: incrementer/0 [9-15]      15 { .count: 1, .done: [True,  True ] }
P3: main/()       [17-27]     27 { .count: 1, .done: [True,  True ] }
>>> Harmony Assertion (file=code/Up.hny, line=10) failed: 1
Open file:///Users/rvr/github/harmony.new/harmony.html for more information
\end{verbatim}
\end{code}
\caption{The text output of running Harmony on \autoref{fig:inc}.}
\label{fig:incoutput}
\end{figure}

\glsadd{stack machine}%

Compiling the code in \autoref{fig:inc} results in the HVM bytecode
listed in \autoref{fig:inccode}.
You can obtain this code by invoking \texttt{harmony} with the \texttt{-a} flag
like so:
\begin{code}
\begin{verbatim}
harmony -a Up.hny
\end{verbatim}
\end{code}
Each thread in the HVM is predominantly a \emph{stack machine},
\index{stack machine}%
but it also has a register.
All instructions are atomically executed.
Most instructions pop values from the stack or push values onto the stack.
At first there is one thread, with name tag \texttt{\_\_init\_\_/()},
that initializes the state.
It starts executing at instruction 0 and keeps executing until the last
execution in the program.
In this case, it executes instructions 0 through 4 first.
The last instruction is a \texttt{JUMP} instruction that sets the
program counter to 31 (skipping over the code for the methods).
At program counter 5 is the code for the \texttt{incrementer} method.
All methods start with a \texttt{Frame} instruction and end with a \texttt{Return}
instruction.

Appendix~\ref{ap:harmonybytecode} provides a list of all HVM machine instructions,
in case you want to read about the details.
The \texttt{Frame} instruction lists the name of the method and the
names of its arguments.
The code generated from $\mathit{count} := \mathit{count} + 1$ in line~6 of
\texttt{Up.hny} is as follows (see \autoref{fig:inccode}):

\begin{enumerate} \setcounter{enumi}{5}
\item The \texttt{Load} instruction pushes the value of the
\textit{count} variable onto the stack.
\item the \texttt{Push} instruction pushes the constant 1
onto the stack of the thread.
\item \texttt{2-ary} is a \texttt{+} operation with 2 arguments.
It pops two values from the stack (the value of \textit{count} and 1),
adds them, and pushes the result back onto the stack.
\item The \texttt{Store} instruction pops
a Harmony value (the sum of the \textit{count} variable and 1) and
stores it in the \textit{count} variable.
\end{enumerate}

Once initialization completes, any threads that were spawned can run.
You can think of Harmony as trying every possible interleaving of threads executing
instructions.
\autoref{fig:incoutput} shows the output produced by running Harmony on the
\texttt{Up.hny} program.

\begin{figure}
\includegraphics[width=\textwidth]{figures/Up1.pdf}
\caption{The
\harmonyref{Up.html}{HTML output of running Harmony on \autoref{fig:inc}}.
There are three sections.
The top shows the steps to the problematic state.
The bottom left shows the source code and bytecode
of the program.
The bottom right shows the state of each thread after the selected step.
Each row in the stack trace of a thread shows the current program counter,
the method that is being evaluated, and the line of code that is
being evaluated.}
\label{fig:inchtml1}
\end{figure}

Harmony can report a variety of failure types:
\begin{itemize}
\item \texttt{Safety violation}: this means something went wrong with
at least one of the executions of the program that it tried.  This
can include a failing assertion, divide by zero, using an uninitialized
or non-existent variable, dividing a set by an integer, and so on.
It also includes certain infinite loops.
Harmony will print at trace of the shortest bad execution that it found.
\item \texttt{Non-terminating States}: Harmony found one or more states
from which there does not exist an execution such that all threads
terminate.  Harmony will not only print the non-terminating state with
the shortest trace, but also the list of threads
at that state, along with their name tags and program counters.
\item \texttt{Stopped States}: Similar to non-terminating states,
these are states in which some threads are left that cannot
terminate.  \autoref{ch:synch} will explain what it means for
a thread to be stopped.
\end{itemize}

In our case, Harmony reports a safety violation, in particular it
reports that the assertion and that
\textit{count} has value 1 instead of 2.
Next it reports an execution that failed this assertion.
The program got to the faulty execution in 5 ``steps.''
The output has four columns:
\begin{enumerate}
\item The name tag of the thread;
\item The sequence of program counters of the HVM instructions that the thread executed;
\item The current program counter of the thread;
\item The contents of the shared memory.
\end{enumerate}

The first step is initialization.
It sets shared variable \textit{count} to 0 and
shared variable \textit{done} to \texttt{[False, False]}.
If we look at the next three steps, we see that:
\begin{itemize}
\item Thread \texttt{incrementer/0}
executed instructions 5 through 8, loading the value of
\textit{count} but stopping just before storing 1 into \textit{count};
\item Thread \texttt{incrementer/1}
executed instructions 5 through 15, storing 1 into
into \textit{count} and storing \texttt{True} into \textit{done}[1];
\item Thread \texttt{incrementer/0}
continues execution, executing instructions 9 through 15
storing value 1 into \textit{count}
and storing \texttt{True} into \textit{done}[0].
\end{itemize}
Finally, thread \texttt{main()} runs and finds the problem.
This makes precise the concurrency issue that we encountered.

Harmony also generates an HTML file that allows exploring more details
of the execution interactively.
Open the suggested HTML file and you should see something like
\autoref{fig:inchtml1}.
If you hover the mouse over a machine instruction, it provides a
brief explanation.

\section*{Exercises}

\begin{problems}
\item \autoref{fig:incenter} shows an attempt at trying to fix the code of
    \autoref{fig:inc}.  Run it through Harmony and see what happens.  Based on
    the error output, describe in English what is wrong with the code by describing,
    in broad steps, how running the program can get into a bad state.
\item What if we moved line~6 of \autoref{fig:incenter} to after the \textbf{if}
    statement (between lines~9 and~10)?  Do you think that would work?  Run it through
    Harmony and describe either why it works or why it does not work.
\end{problems}

\begin{figure}
\begin{code}
\harmonysource{UpEnter}
\end{code}
\caption{\harmonylink{code/UpEnter.hny} Broken attempt at fixing the code of \autoref{fig:inc}.}
\label{fig:incenter}
\end{figure}

\chapter{Critical Sections}
\label{ch:critical}

\begin{figure}
\begin{code}
\harmonysource{csbarebones}
\end{code}
\caption{\harmonylink{code/csbarebones.hny} A bare bones critical section with two threads.}
\label{fig:csbarebones}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{cs}
\end{code}
\caption{\harmonylink{code/cs.hny} Harmony model of a critical section.}
\label{fig:cs}
\end{figure}

Hopefully you have started thinking of how to solve the concurrency
problem and you may already have prototyped some solutions.
In this chapter we will go through a few reasonable but broken attempts.
At the heart of the problem is that we would like make sure that, when
the \textit{count} variable is being updated, no other thread is
trying to do the same thing.  This is called a \emph{critical section}
(aka critical region)~\cite{EWD123}:
a set of instructions where only one thread is allowed to execute at a
time.
\index{critical section}%
\index{critical region}%

\glsadd{critical section}%
\glsadd{thread safety}%

Critical sections are useful when accessing a shared data
structure, particularly when that access requires multiple underlying
machine instructions.  A counter is a very simple example of
a data structure, but as we have seen it too requires multiple instructions.
A more involved one would be accessing a binary tree.
Adding a node to a binary tree, or re-balancing a tree, often requires
multiple operations.  Maintaining ``consistency'' is certainly much easier
(although not necessarily impossible) if during this time no other
thread also tries to access the binary tree.
Typically, you want some invariant property of the data structure to hold
at the beginning and at the end of the critical section, but in the middle
the invariant may be temporarily broken---this is no issue as critical
sections guarantee that no other
thread will be able to see it.
An implementation of a data structure that can be safely accessed by multiple
threads (or threads) and free of race conditions is called \emph{thread-safe}.
\index{thread safety}%
\index{thread}%

\glsadd{mutual exclusion}%

A critical section is often modeled as threads in an infinite loop
entering and exiting the critical section.
\autoref{fig:csbarebones} shows the Harmony code.
Here \texttt{@cs} is a \emph{label},
\index{label}%
identifying a location in the HVM bytecode.  The first thing we need to
ensure is that there can never be two threads in the critical section.
This property is called \emph{mutual exclusion}.
\index{mutual exclusion}%
We would like to place an assertion at the \texttt{@cs} label that
specifies that only the current thread can be there.

Harmony in fact supports this.
It has an operator \textbf{atLabel}~$L$,
\index{atLabel operator}%
where $L$
is the atom containing the name of the label (in this case, \texttt{.cs}).
The operator returns a bag (multiset) of name tags of threads executing at that
label.  The bag is represented by a dictionary that maps each element
in the bag to the number of times the element appears in the bag.
Method \textbf{atLabel} only exists for specification purposes---do not
use it in normal code.
The assertion also makes use of the \textbf{nametag}() operator
\index{nametag operator}%
that returns the name tag of the current thread.
If you run the code through Harmony, the assertion should fail because
there is no code yet for safely entering and exiting the critical section.

However, mutual exclusion by itself is easy to ensure.
For example, we could insert the following code to enter the
critical section:
\begin{code}
\texttt{await False;}
\end{code}
This code will surely prevent two or more threads from being
at label \texttt{@cs} at the same time.
But it does so by preventing \emph{any} thread from reaching
the critical section.
We clearly need another property besides mutual exclusion.

Mutual exclusion is an example of a \emph{safety property},
\index{safety property}%
a property that ensures that \emph{nothing bad will happen}, in this case
two threads being in the critical section.
What we need now is a \emph{liveness property}:
\index{liveness property}%
we want to ensure that
\emph{eventually something good will happen}.
There are various possible liveness properties we could use,
but here we will propose the following informally: if
(1) there exists a non-empty
set $S$ of threads that are trying to enter the critical section and
(2) threads in the critical section always leave eventually, then
eventually one thread in $S$ will enter the critical section.
We call this \emph{progress}.
\index{progress}%

In order to detect violations of progress, and other liveness problems in
algorithms in general, Harmony requires that every execution must be
able to reach a state in which all threads have terminated.
Clearly, even if mutual exclusion holds in \autoref{fig:csbarebones},
the spawned threads never terminate.  We
will instead model threads in critical sections using the framework in
\autoref{fig:cs}: a thread can \emph{choose} to enter a
critical section more than once, but it can also choose to terminate, even
without entering the critical section ever.
(Recall that Harmony will try every possible execution, and so it will evaluate
both choices.)

\begin{comment}
\begin{figure}
\begin{center}
\includegraphics[width=3.5in]{figures/mutex-crop.pdf}
\end{center}
\caption{High-level state diagram specification of mutual exclusion with up to two threads.
The first number in a state gives the number of threads; the second number is the
number of threads in the critical section.}
\label{fig:mutex}
\end{figure}

\autoref{fig:mutex} shows a high-level state diagram
\index{state diagram}%
of mutual exclusion and progress.
The circles represent states, while the arrows represent possible state
transitions (high-level steps).  The label in each circle summarizes the state;
the label on an arrow describes the transition.  In this case,
the label contains two numbers: the total number of threads and the number
of threads in the critical section.
\end{comment}

A thread that is in the critical
section cannot terminate until after leaving the critical section.
We will now consider various approaches toward implementing this
specification.

\begin{figure}
\begin{code}
\harmonysource{naiveLock}
\end{code}
\caption{\harmonylink{code/naiveLock.hny} Na\"{\i}ve implementation of a shared lock.}
\label{fig:uplock}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{figures/naiveLock.png}
\caption{The \harmonyref{naiveLock.html}{HTML output of running Harmony on \autoref{fig:uplock}}.}
\label{fig:naiveLockhtml}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{naiveFlags}
\end{code}
\caption{\harmonylink{code/naiveFlags.hny} Na\"{\i}ve use of flags to solve mutual exclusion.}
\label{fig:upflags}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{figures/naiveFlags.png}
\caption{The \harmonyref{naiveFlags.html}{HTML output of running Harmony on \autoref{fig:upflags}}.}
\label{fig:naiveflagshtml}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{naiveTurn}
\end{code}
\caption{\harmonylink{code/naiveTurn.hny} Na\"{\i}ve use of turn variable to solve mutual exclusion.}
\label{fig:upturn}
\end{figure}

\glsadd{lock}%

You may already have heard of the concept of a \emph{lock}
\index{lock}%
and have realized that
it could be used to implement a critical section.
The idea is that the lock is like a baton that at most one thread can own
(or hold) at a time.
A thread that wants to enter the critical section at a time must obtain the
lock first and release it upon exiting the critical section.
% Note that the word ``lock'' does not describe the concept well---it really
% is more like a baton or token that only one thread can hold.

Using a lock is a good thought, but how does one implement one?
\autoref{fig:uplock} presents a mutual exclusion attempt based on a
na\"{\i}ve (and, as it turns out, broken) implementation of a lock.
Initially the lock is not owned, indicated by \textit{lockTaken} being \texttt{False}.
To enter the critical section, a thread waits until \textit{lockTaken} is \texttt{False}
and then sets it to \texttt{True} to indicate that the lock has been taken.
The thread then executes the critical section.  Finally the thread
releases the lock by setting \textit{lockTaken} back to \texttt{False}.

Unfortunately, if we run the program through Harmony,
we find that the assertion still fails.
\autoref{fig:naiveLockhtml} shows the Harmony output.
Thread~0 finds that the lock is available, but just before it stores
\texttt{True} in \textit{lockTaken} in instruction~9, thread~1 gets
to run.
(Recall that you can hover your mouse over a machine instruction in order
to see what it does.)
Because \textit{lockTaken} is still \texttt{False}, it too
believes it can acquire the lock, and stores \texttt{True} in
\texttt{lockTaken} and moves on to the critical section.
Finally, thread~0 moves on, also stores \texttt{True} and also moves
into the critical section.  Thread~0 is the one that detects the
problem.
The \textit{lockTaken} variable
suffers from the same sort of race condition as the \textit{count} variable
in \autoref{fig:inc}: testing and setting the lock
consists of several instructions.  It is thus possible
for both threads to believe the lock is available and to obtain the lock
at the same time.

Preventing multiple threads from updating the same variable,
\autoref{fig:upflags} presents a solution based on each thread having
a flag indicating that it is trying to enter the critical section.
A thread can write its own flag and read the flag of its peer.
After setting its flag, the thread waits until the other thread
($1 - \mathit{self}$) is not trying to enter the critical section.
If we run this program, the assertion does not fail.  In fact, this
solution does prevent both threads being in the critical section at
the same time.

To see why, first note the following invariant: if thread $i$ is in the
critical section, then \textit{flags}[$i$] == \texttt{True}.
Without loss of generality,
suppose that thread~0 sets \textit{flags}[0] at time $t_0$.
Thread 0 can only reach the critical section if at some time $t_1$,
$t_1 > t_0$, it finds that \textit{flags}[1] == \texttt{False}.
Because of the invariant, \textit{flags}[1] == \texttt{False} implies that
thread~1 is not in the critical section at time $t_1$.
Let $t_2$ be the time at which thread~0 sets \textit{flags}[0]
to \texttt{False}.  Thread~0 is in the critical section sometime
between $t_1$ and $t_2$.
It is easy to see that thread~1 cannot enter the critical section
between $t_1$ and $t_2$, because \textit{flags}[1] == \texttt{False} at
time $t_1$.  To reach the critical section between $t_1$ and $t_2$,
it would first have to set \textit{flags}[1] to \texttt{True} and
then wait until \textit{flags}[0] == \texttt{False}.  But that does not happen
until time $t_2$.

However, if you run the program through Harmony (\autoref{fig:naiveflagshtml}),
it turns out the solution
does have a problem: if both try to enter the critical section at the same
time, they may end up waiting for one another indefinitely.  Thus the
solution violates \emph{progress}.

The final na\"{\i}ve solution that we propose
is based on a variable called \textit{turn}.
Each thread politely lets the other thread have a turn first.
When \textit{turn} = $i$, thread~$i$ can
enter the critical section, while thread $1-i$ has to wait.
An invariant of this solution is that while thread~$i$ is in the critical
section, \textit{turn} == $i$.
Since \textit{turn} cannot be 0 and 1 at
the same time, mutual exclusion is satisfied.
The solution also has the nice property that the thread that has been waiting the
longest to enter the critical section can go next.

Run the program through Harmony.  It turns out that this solution also violates
\emph{progress}, albeit for a different reason:
if thread~$i$ terminates instead of entering the critical section,
thread $1-i$, politely, ends up waiting indefinitely for its turn.
Too bad, it would have been a great solution if both threads try to enter the
critical section continually.

\section*{Exercises}
\begin{problems}
\item Run \autoref{fig:cs} using Harmony.  As there is no protection of the critical
section, mutual exclusion is violated,
the assertion should fail, and a trace should be reported.
Now insert
\begin{code}
\texttt{await False;}
\end{code}
just before entering the critical section
in \autoref{fig:cs} and run Harmony again.
Mutual exclusion is guaranteed but progress is violated.
Harmony should print a trace
to a state from which a terminating state cannot be reached.
Describe in English the difference in the failure reports before
and after inserting the code.
\item See if you can come up with some different approaches that satisfy both
mutual exclusion and progress.  Try them with Harmony and see if they work or not.
If they don't, try to understand why.
Do not despair if you can't figure out how to develop a solution that satisfies both
mutual exclusion and progress---as
we will find out, it is possible but not easy.
\end{problems}

\chapter{Peterson's Algorithm}
\label{ch:peterson}
\index{Peterson's Algorithm}%

\begin{figure}
\begin{code}
\harmonysource{Peterson}
\end{code}
\caption{\harmonylink{code/Peterson.hny} Peterson's Algorithm}
\label{fig:peterson}
\end{figure}

In 1981, Gary L.~Peterson came up with a beautiful solution to the mutual exclusion
problem, now known as ``Peterson's Algorithm''~\cite{Peterson81}.
The algorithm is an amalgam of the (incorrect) algorithms in
\autoref{fig:upflags} and \autoref{fig:upturn}, and is presented
in \autoref{fig:peterson}.
A thread first indicates its interest in entering the critical section
by setting its flag.
It then politely gives way to the other thread should it also want to
enter the critical section---if both do so at the same time one will
win because writes to memory in Harmony are atomic.
The thread continues to be polite, waiting
until either the other thread is nowhere near the critical section
($\mathit{flag}[1 - \mathit{self}] == \mathtt{False}$) or has given way
($\mathit{turn} == \mathit{self}$).
Running the algorithm with Harmony shows that it satisfies both mutual
exclusion and progress.

\begin{figure}
\begin{center}
\includegraphics[width=6in]{figures/states-crop.pdf}
\end{center}
\caption{Venn diagram classifying all states and a trace.}
\label{fig:states}
\end{figure}

\glsadd{state}%
\glsadd{thread variable}%

Why does it work?  We will focus here on how one might go about proving
mutual exclusion for an algorithm such as Peterson's.
For that, we have to understand a little bit more about how the Harmony
virtual machine (HVM) works.
In \autoref{ch:harmonymachine} we talked about the concept of \emph{state}:
\index{state}%
at any point in time the HVM is in a specific state.
A state is comprised of the values of the shared variables as well as
the values of the thread variables
\index{thread variable}%
of each thread, including its
program counter and the contents of its stack.
Each time a thread executes a HVM machine instruction, the
state changes (if only because the program counter of the thread
changes).  We call that a \emph{step}.
\index{step}%
Steps in Harmony are atomic.

\glsadd{step}%
\glsadd{trace}%

The HVM starts in an initial state in which there is only
one thread and its program counter is~0.  A \emph{trace}
\index{trace}%
is a sequence of steps starting from the initial state.
When making a step, there are two sources of non-determinism
\index{non-determinism}%
in Harmony.
One is when
there is more than one thread that can make a step.  The other is
when a thread executes a \texttt{choose} operation and there is
more than one choice.
Because there is non-determinism, there are multiple possible traces.
We call a state \emph{reachable}
\index{reachable state}%
if it is either the initial state
or it can be reached from the initial state through a trace.
A state is final
when there are no threads left to make state changes.

It is often useful to classify states.
\emph{Initial}, \emph{final}, and \emph{reachable}, and \emph{unreachable}
are all examples of classes of states.
\autoref{fig:states} depicts a Venn diagram of various classes of states
and a trace.
One way to classify states is to define a predicate over states.
All states in which $x$ = 1, or all states where
there are two or more threads executing, are examples of such predicates.
For our purposes, it is useful to define a predicate that says that at
most one thread is in the critical section.  We shall call such states
\emph{exclusive}.

An \emph{invariant} of a program
\index{invariant}%
is a predicate that holds over all states that are reachable by that program.
We want to show that exclusivity is an invariant because mutual exclusion means
that all reachable states are exclusive.
In other words, we want to show that the set of reachable states of executing
the program
is a subset of the set of states where there is at most one thread in the critical
section.

One way to prove that a predicate is an invariant is through induction
on the number of steps.  First you prove that the predicate holds over
the initial state.  Then you prove that for every reachable state,
and for every step from that reachable state, the predicate also holds
over the resulting state.
For this to work you would need a predicate that describes exactly which
states are reachable.
But we do not have such a predicate: we know how to define the set
of reachable states, but given an arbitrary state it is not easy to
see whether it is reachable or not.

To solve this problem, we will use what is called an
\emph{inductive invariant}.
\index{inductive invariant}%
An inductive invariant $\mathcal{I}$ is a predicate over states that satisfies the following:
\begin{itemize}
\item $\mathcal{I}$ holds in the initial state.
\item For any state in which $\mathcal{I}$ holds (including unreachable ones) and for any
thread in the state, if the thread takes a step, then $\mathcal{I}$ also holds in the
resulting state.
\end{itemize}

One candidate for such a predicate is exclusivity itself.
After all, it certainly holds over the initial state.
And as Harmony has already determined, exclusivity is an invariant:
it holds over every reachable state.
Unfortunately, exclusivity is not an \emph{inductive} invariant.
To see why, consider the following state $s$: let thread~0 be at label \texttt{@cs}
and thread~1 be at the start of the \textbf{await} statement.
Also, in state $s$, $\mathit{turn} = 1$.  Now let
thread~1 make a sequence of steps.  Because $\mathit{turn} = 1$,
thread~1 will stop waiting and also enter the critical
section, entering a state that is not exclusive.
So exclusivity is an invariant (holds over every reachable state, as demonstrated
by Harmony),
but not an inductive invariant (it will turn out that $s$ is not reachable).

So we are looking for an inductive invariant that \emph{implies} exclusivity.
In other words, the set of states where the inductive invariant holds
must be a subset of the set of states where there is at most one thread in
the critical section.

Let us begin with considering the following important property:
$\mathcal{F}(i) = \mathtt{thread}(i)@[8 \cdots 15] \Rightarrow \mathit{flags}[i]$,
that is, if thread~$i$ is executing in lines 8 through 15, then $\mathit{flags}[i]$
is set.
Although it does not, by itself, imply exclusivity, we can show that
$\mathcal{F}(i)$ is an inductive invariant (for both threads 1 and 2).
To wit, it holds in the initial state, because in the initial state thread $i$ does
not even exist yet.
Now we have to show that if $\mathcal{F}(i)$ holds in some state, then
$\mathcal{F}(i)$ also holds in a next state.
Since only thread~$i$ ever changes $\mathit{flags}[i]$, we only need to
consider steps by thread~$i$.
Since $\mathcal{F}(i)$ holds, there are two cases to consider:
\begin{enumerate}
\item states in which $\mathit{flags}[i] = \texttt{true}$
\item states in which $\lnot \mathtt{thread}(i)@[8 \cdots 15]$
\end{enumerate}
In each case we need to show that if thread $i$ takes a step, 
$\mathcal{F}(i)$ still holds.
In the first case, there is only one step that thread $i$ can take that would
set $\mathit{flags}[i]$ to false: the step in line 15.  But executing the line
would also take the thread out of lines $8 \cdots 15$, so $\mathcal{F}(i)$ continues to hold.
In the second case (thread $i$ is not executing in lines $8 \cdots 15$), the only step
that would cause thread $i$ to execute in lines $8 \cdots 15$ would be the step in line 7.
But in that case $\mathit{flags}[i]$ would end up being true, so
$\mathcal{F}(i)$ continues to hold as well.
So $\mathcal{F}(i)$ is an inductive invariant (for both threads~0 and~1).

While $\mathcal{F}(i)$ does not imply mutual exclusion, it does imply the following useful
invariant: $\mathtt{thread}(i)@cs \Rightarrow \mathit{flags}[i]$: when thread $i$ is
at the critical section, $\mathit{flags}[i]$ is set.  This seems obvious from the code,
but now you know how to prove it.

We need a stronger inductive invariant than $\mathcal{F}(i)$ to prove mutual exclusion.
What else do we know when thread $i$ is in the critical section?
Let $\mathcal{C}(i) = \lnot\mathit{flags}[1 - i] \lor \mathit{turn} = i$, that is,
the condition on the \textbf{await} statement for thread~$i$.
In a sequential program, $\mathcal{C}(i)$ would clearly hold if thread $i$ is in
the critical section: $\mathtt{thread}(i)@cs \Rightarrow \mathcal{C}(i)$.
However, because thread $1-i$ is executing concurrently, this property does not
hold.  In particular, suppose thread 0 is at the critical section, $\mathit{flags}[0]$ = true,
$\mathit{turn} = 1$, and thread 1 just finished the step in line 7,
setting $\mathit{flags}[1]$ to true.  Notice that $C(0)$ is violated.

\begin{figure}
\begin{code}
\harmonysource{PetersonInductive}
\end{code}
\caption{\harmonylink{code/PetersonInductive.hny} Peterson's Algorithm with Inductive Invariant}
\label{fig:petersonproof}
\end{figure}

Instead, we will use the following property: $\mathcal{G}(i) =
\mathtt{thread}(i)@cs \Rightarrow \mathcal{C}(i) \lor \mathtt{thread}(1-i)@[8]$.
That is, if thread $i$ is at the critical section, then
either $\mathcal{C}(i)$ holds or thread $1-i$ is about to execute line 8.
\autoref{fig:petersonproof} formalizes $\mathcal{G}(i)$ in Harmony.
The label \texttt{@gate} refers to the step that sets \textit{turn} to $1-i$.
You can run \autoref{fig:petersonproof} to determine
that $\mathcal{G}(i)$ is an invariant for $i = 0, 1$.
Moreover, if $\mathcal{F}(i)$ and $\mathcal{G}(i)$ both hold for $i = 0, 1$,
then mutual exclusion holds.  We can show this using proof by
contradiction.  Suppose mutual exclusion is violated and thus both threads are in
the critical section.  By $\mathcal{F}$ it must be the case that both
\textit{flags} are true.  By $\mathcal{G}$ and the fact that neither thread
is at label \texttt{@gate}, we know that both $C(0)$ and $C(1)$ must hold.
This then implies that $\mathit{turn} = 0 \land \mathit{turn} = 1$, providing
the desired contradiction.

We claim that $\mathcal{G}(i)$ is an inductive invariant.
First, since neither thread exists in the initial state, it is clear that
$\mathcal{G}(i)$ holds in the initial state.
Without loss of generality, suppose $i=0$ (a benefit from the fact that the algorithm is
symmetric for both threads).  We still have to show that if we are in a state
in which $\mathcal{G}(0)$ holds, then any step will result in a
state in which $\mathcal{G}(0)$ still holds.

First consider the case that thread~0 is at label \texttt{@cs}.  If thread~0
were to take a step, then in the next state thread~0 would be no longer
at that label and $\mathcal{G}(0)$ would hold trivially over the next state.
Therefore we only need to consider a step by thread~1.
%
From $\mathcal{G}$ we know that one of the following three cases must hold before
thread~1 takes a step:
\begin{enumerate}
\item \textit{flags}[1] = \texttt{False};
\item \textit{turn} = 0;
\item thread~1 is at label \texttt{@gate}.
\end{enumerate}

Let us consider each of these cases.
We have to show that if thread~1 takes a step, then one of those
cases must hold after the step.
In the first case, if thread~1 takes a step, there are two possibilities:
either $flags[1]$ will still be \texttt{False} (in which case the first case
continues to hold), or $flags[1]$ will be \texttt{True}
and thread~1 will be at label \texttt{@gate} (in which case the third case
will hold).
We know that thread~1 never sets \textit{turn} to 1, so
if the second case holds before the step, it will also hold after the step.
Finally, if thread~1 is at label \texttt{@gate} before the step, then after
the step \textit{turn} will equal 0, and therefore the second case will hold
after the step.

Now consider the case where thread~0 is not in the critical section,
and therefore $\mathcal{G}(0)$ holds trivially because false implies
anything.
There are three cases to consider:
\begin{enumerate}
\item Thread~1 takes a step.  But then thread~0 is still not in the critical
section and $\mathcal{G}(0)$ continues to hold;
\item Thread~0 takes a step but still is not in the critical section.
Then again $\mathcal{G}(0)$ continues to hold.
\item Thread~0 takes a step and ends up in the critical section.
Because thread~0 entered the critical section, we know that
\textit{flags}[1] = \texttt{False} or \textit{turn} == 0 because
of the \texttt{await} condition.
And hence $\mathcal{G}(0)$ continues to hold in that case as well.
\end{enumerate}

We have now demonstrated mutual exclusion in Peterson's Algorithm in two
different ways: one by letting Harmony explore all possible executions, the
other using inductive invariants and proof by induction.  The former
is certainly easier, but it does not provide intuition for why the
algorithm works.  The second provides much more insight.

Even though they are not strictly necessary, we encourage to include invariants
in your Harmony code. They can provide important insights into why the code works.

A cool anecdote is the following.  When the author of Harmony had to teach
Peterson's Algorithm, he refreshed his memory by looking at the Wikipedia
page.  The page claimed that the following predicate is invariant:
if thread $i$ is in the critical section, then $\mathcal{C}(i)$ (i.e.,
$\mathcal{G}$ without the disjunct that thread $1-i$ is at label \texttt{@gate}).
To demonstrate that this predicate is not invariant, you can remove the
disjunct from \autoref{fig:petersonproof} and run it to get a
counterexample.

This anecdote suggests the following.  If you need to do a proof by induction
of an algorithm, you have to come up with an inductive invariant.
Before trying to prove the algorithm, you can check that the predicate is
at least invariant by testing it using Harmony.  Doing so could potentially
avoid wasting your time on a proof that will not work because the
predicate is not invariant, and therefore not an inductive invariant either.
(The author fixed the Wikipedia page with the help of Fred B.~Schneider.)

\section*{Exercises}
\begin{problems}
\item \autoref{fig:csonebit} presents another correct solution to the
mutual exclusion problem.  It is similar to the one in
\autoref{fig:upflags}, but has a thread \emph{back out and try again}
if it finds that the other thread is either trying to enter the critical
section or already has.  Compare this algorithm with Peterson's.  What are
the advantages and disadvantages?
\item
Can you find one or more inductive invariants for the algorithm in
\autoref{fig:csonebit} to prove it correct?
Here's a pseudo-code version of the algorithm to help you.  Each line
is an atomic action:
\begin{code}
\begin{verbatim}
    initially: flagX = flagY = False

    thread X:                          thread Y:
        X0: flagX = True                   Y0: flagY = True
        X1: if not flagY goto X4           Y1: if not flagX goto Y4
        X2: flagX = False                  Y2: flagY = False
        X3: goto X0                        Y3: goto Y0
        X4: ...critical section...         Y4: ...critical section...
        X5: flagX = False                  Y5: flagY = False
\end{verbatim}
\end{code}
\item A colleague of the author asked if the first two assignments in
Peterson's algorithm (setting \textit{flags}[\textit{self}])
to \texttt{True} and \textit{turn} to $1 - \mathit{self}$) can be reversed.
After all, they are different variables assigned independent values---in a
sequential program one could surely swap the two assignments.
See if you can figure out for yourself if the two assignments can be
reversed.  Then run the program in \autoref{fig:peterson} after reversing
the two assignments and describe in English what happens.
\item Bonus question:
Can you generalize Peterson's algorithm to more than two threads?
\item Bonus question:
Implement
\href{https://en.wikipedia.org/wiki/Dekker%27s_algorithm}{Dekker's Algorithm},
\href{https://en.wikipedia.org/wiki/Eisenberg_%26_McGuire_algorithm}{Eisenstein and McGuire's Algorithm},
\href{https://en.wikipedia.org/wiki/Szymanski%27s_algorithm}{Szymaski's Algorithm}, or the
\href{https://en.wikipedia.org/wiki/Lamport%27s_bakery_algorithm}{Lamport's Bakery Algorithm}.
Note that the last one uses unbounded state, so you should modify the threads so they
only try to enter the critical section a bounded number of times.
\end{problems}

\begin{figure}
\begin{code}
\harmonysource{csonebit}
\end{code}
\caption{\harmonylink{code/csonebit.hny} Mutual exclusion using a flag per thread.}
\label{fig:csonebit}
\end{figure}

\chapter{Harmony Methods and Pointers}
\label{ch:method}
\index{Harmony method}%

A method \texttt{m} with argument $a$ is invoked in its
most basic form as follows (assigning the result to~\texttt{r}).
\begin{code}
$r$ = m $a$;
\end{code}
That's right, no parentheses are required.  In fact, if you invoke
\texttt{m}($a$), the argument is ($a$), which is the same
as $a$.
If you invoke \texttt{m()}, the argument is \texttt{()},
which is the empty tuple.
If you invoke \texttt{m}($a$, $b$), the argument is ($a$, $b$),
the tuple consisting of values $a$ and $b$.

You may note that all this looks familiar.  Indeed, the syntax
is the same as that for dictionaries (see \autoref{ch:harmonymachine}).
Both dictionaries and methods map Harmony values to Harmony values,
and their syntax is indistinguishable.
If \texttt{f} is either a method or a
dictionary, and $x$ is an arbitrary Harmony value, then
\texttt{f} $x$, \texttt{f}($x$), and \texttt{f}[$x$] are all
the same expression in Harmony.

\begin{figure}
\begin{code}
\harmonysource{PetersonMethod}
\end{code}
\caption{\harmonylink{code/PetersonMethod.hny} Peterson's Algorithm accessed through methods.}
\label{fig:petersonmethods}
\end{figure}

Harmony does not have a \textbf{return} statement.  (You can assign a return value
to a method by setting the \textit{result} variable.)  Neither does it support
\textbf{break} or \textbf{continue} statements in loops.  One reason for their absence is
that, particularly in concurrent programming, such control flow directions are highly
error-prone.  It's easy to forget to, say, release a lock when returning a value in the
middle of a method, a major source of bugs in practice.

Harmony is not an object-oriented language like Python is.  In Python
you can pass a reference to an object to a method, and that method
can then update the object.  In Harmony, it is also sometimes convenient
to have a method update a shared variable specified as an argument.
For this, as mentioned in \autoref{ch:harmonymachine},
each shared variable has an \emph{address}.
\index{address}%
If $x$ is a shared variable, then the expression ?$x$ is the address of $x$.
If a variable contains an address, we call that variable a \emph{pointer}.
\index{pointer}%
If $p$ is a pointer to a shared variable, then the
expression \texttt{!}$p$ is the value of the shared variable.
In particular, \texttt{!?}$x$ == $x$.
This is similar to how C pointers work (\texttt{*\string&}$x$ == $x$).

Often, pointers point to dictionaries, and so if $p$ is such a pointer,
then $!(p).\mathtt{field}$ would evaluate to the specified field in the dictionary.
Note that the parentheses in this expression
are needed, as \texttt{!}\textit{p}\texttt{.field} would wrongly evaluate
\texttt{!}(\textit{p}\texttt{.field}).
$!(p).\mathtt{field}$ is such a common expression that, like C, Harmony supports the
shorthand $p-$$>$\texttt{field}, which greatly improves readability.

\autoref{fig:petersonmethods} again shows Peterson's algorithm,
but this time with methods defined to enter and exit the critical
section.
The name \textit{mutex} is often used to denote a variable or value
that is used for mutual exclusion.
\texttt{P\_mutex} is a method that returns a ``mutex,'' which, in this
case, is a dictionary that contains Peterson's Algorithm's state:
a turn variable and two flags.
Both methods \texttt{P\_enter} and \texttt{P\_exit} take two arguments:
a pointer to a mutex and the thread identifier (0 or 1).
$\mathit{pm}\rightarrow\mathtt{turn}$ is the value of the \texttt{.turn} key
in the dictionary that \textit{pm} points to. 

You can put the first three methods in its own Harmony source file
and include it using the Harmony \texttt{import} statement.
\index{import statement}%
\index{module}%
This would
make the code re-usable by other applications.

\chapter{Spinlock}
\label{ch:spinlock}
\index{spinlock}%

\glsadd{spinlock}%

\begin{figure}
\begin{code}
\harmonysource{spinlock}
\end{code}
\caption{\harmonylink{code/spinlock.hny} Mutual Exclusion using a ``spinlock'' based on test-and-set.}
\label{fig:tas}
\end{figure}

\autoref{fig:uplock} showed a faulty attempt at solving mutual
exclusion using a lock.  The problem with the implementation of the
lock is that checking the lock and setting it if it is available is
not \emph{atomic}.  Thus multiple threads contending for the lock
can all ``grab the lock'' at the same time.  While Peterson's
algorithm gets around the problem, it is not efficient, especially
if generalized to multiple threads.  Instead, multi-core processors provide
so-called \emph{interlock instructions}:
\index{interlock instruction}%
special machine instructions
that can read memory and then write it in an indivisible step.

\glsadd{interlock instruction}%

While the HVM does not have any specific built-in interlock instructions,
it does have support for executing multiple instructions atomically.
This feature is available in the Harmony language in two ways.
First, any Harmony statement can be made atomic by placing a label in front
of it.  Second, a group of Harmony statements can be made atomic
through the \textbf{atomic}
\index{atomic statement}%
statement.
We can use \textbf{atomic} statement blocks to implement a wide variety of
interlock operations.
For example, we could fix the program in \autoref{fig:inc} by
constructing an atomic increment operation for a counter, like so:
\begin{code}
\harmonysource{atomicinc}
\end{code}

Many CPUs have an atomic ``test-and-set'' (TAS)
\index{test-and-set}%
\index{TAS}%
operation.
Method \texttt{tas} in \autoref{fig:tas} shows its specification.
Here $s$ points to a shared Boolean variable and $p$
to a private Boolean variable, belonging to some thread.
The operation copies the value of the shared variable to the
private variable (the ``test'')
and then sets the shared variable to \texttt{True} (``set'').

\autoref{fig:tas} goes on to implement mutual exclusion for
a set of \texttt{N} threads.
The approach is called \emph{spinlock},
\index{spinlock}%
because each thread is ``spinning'' (executing a tight loop) until
it can acquire the lock.
The program uses $\mathtt{N}+1$ variables.
Variable \textit{shared} is initialized to \texttt{False} while
\textit{private}[$i$] for each thread $i$ is initialized to \texttt{True}.
An important invariant, $\mathcal{I}_1$, of the program is that at any time at most
one of these variables is \texttt{False}.
Another invariant, $\mathcal{I}_2(i)$, is that if thread $i$ is executing between
lines $18 \cdots 25$ (which includes the critical section),
then \textit{private}[$i$] == \texttt{False}.
Between the two (i.e., $\mathcal{I}_1 \land \forall i: \mathcal{I}_2(i)$),
it is clear that only one thread can be in the
critical section at the same time.

$\mathcal{I}_1 \land \forall i: \mathcal{I}_2(i)$ is an inductive invariant.
To see that invariant $\mathcal{I}_1$ is maintained, note that
\texttt{!}$p$ == \texttt{True} upon entry of \texttt{tas}
(because of the condition on the \textbf{while} loop that the
\texttt{tas} method is invoked in).
So there are two cases:
\begin{enumerate}
\item \texttt{!}$s$ is \texttt{False} upon entry to \texttt{tas}.
Then upon exit \texttt{!}$p$ == \texttt{False} and \texttt{!}$s$ == \texttt{True},
maintaining the invariant.
\item \texttt{!}$s$ is \texttt{True} upon entry to \texttt{tas}.
Then upon exit nothing has changed, maintaining the invariant.
\end{enumerate}
Invariant $\mathcal{I}_1$ is also easy to verify for exiting the critical section
because we can assume, by the induction hypothesis, that $\textit{private}[i]$ is
\texttt{True} just before exiting the critical section.
Invariant $\mathcal{I}_2(i)$ is obvious as (i) thread $i$ only proceeds to the critical
section if \textit{private}[$i$] is \texttt{False}, and (ii) no other thread modifies
\textit{private}[$i$].

\begin{figure}
\begin{code}
\harmonysource{spinlockInv}
\end{code}
\caption{\harmonylink{code/spinlockInv.hny} Checking invariants.}
\label{fig:tasinv}
\end{figure}

Harmony can check these invariants as well.  \autoref{fig:tas} already
has the code to check $\mathcal{I}_2(i)$.  But how would one go about checking an
invariant like $\mathcal{I}_1$?  Invariants must hold for every state.
For $\mathcal{I}_2$ we only need an assertion at label \texttt{@cs} because the
premisse is that there is a thread at that label.  However, we would
like to check $\mathcal{I}_1$ in \emph{every state} (after the variables have
been initialized).

We can do this by adding another thread that
checks the invariant.  \autoref{fig:tasinv} shows the code.
Method \texttt{checkInvariant()} checks to see if the invariant holds
in a state.  It introduces a new feature of Harmony: the ability to have
variables local to a method.  In this case, the thread variable \textit{sum}
is used to compute the number of shared variables that have value
\texttt{False}.
The method is invoked by a thread that runs alongside
the other threads.
In Harmony, \textbf{assert} statements are executed atomically, so the
evaluation of the assertion is not interleaved with the execution
of other threads.
Because Harmony tries every possible execution, the thread is guaranteed
to find violations of the invariant if it does not hold.

\section*{Exercises}
\begin{problems}
\item Implement an atomic swap operation.  It should take two pointer arguments
and swap the values.
\item \label{ex:swap} Implement a spinlock using the atomic swap operation.
\item For the solution to \autoref{ex:swap},
write out the invariants that need to hold and check them using Harmony.
\item People who use an ATM often first check their balance and then withdraw
a certain amount of money not exceeding their balance.  A negative balance
is not allowed.  \autoref{fig:atm} shows two operations on bank accounts:
one to check the balance and one to withdraw money.
Note that all operations on accounts are carefully protected by a lock
(i.e., there are no data races).
The \texttt{customer}
method models going to a particular ATM and withdrawing money not exceeding
the balance.  Method \texttt{checker} looks for negative balances.
Running the code through Harmony reveals that there is a bug.
It is a common type of concurrency bug known as \emph{Time Of Check Time Of
Execution} (TOCTOE).
\index{Time Of Check Time Of Execution}
\index{TOCTOE}
In this case, by the time the withdraw operation is performed,
the balance can have changed.
\begin{enumerate}
\item Fix the code in \autoref{fig:atm}.  Note, you should leave the
customer code the same. You are only allowed to change the
\texttt{atm\_} methods, and you cannot use the \textbf{atomic}
statement.
\item Is it necessary to obtain a lock in \texttt{atm\_check\_balance()}?
\end{enumerate}
\end{problems}

\begin{figure}
\begin{code}
\harmonysource{atm}
\end{code}
\caption{\harmonylink{code/atm.hny} Withdrawing money from an ATM.}
\label{fig:atm}
\end{figure}

\chapter{Blocking}
\label{ch:synch}
\index{lock}%
\index{synch module}%

\begin{figure}
\begin{code}
\harmonysource{csTAS}
\end{code}
\caption{\harmonylink{code/csTAS.hny} Fixed version of \autoref{fig:uplock} using test-and-set.}
\label{fig:tas2}
\end{figure}

In \autoref{fig:tas} we have shown a solution based on a shared
variable and a private variable for each thread.   The private
variables themselves are actually implemented as shared variables,
but they are accessed only by their respective threads.
A thread usually does not keep explicit track of whether it has a lock
or not because it is usually implied by the control flow of the program.
So there is no need to keep \textit{private} as a shared
variable---we only did so to be able to show and check the invariants.
\autoref{fig:tas2} shows a more straightforward implementation of spinlock.
The solution is similar to the na\"{i}ve solution of \autoref{fig:uplock},
but uses test-and-set to check and set the lock variable atomically.
This approach is general for any number of threads.

It is important to appreciate the difference between an
\emph{atomic section} (the statements executed within an
\textbf{atomic} statement) and a \emph{critical section}
(protected by a lock of some sort).
The former ensures that while the
\texttt{atomic} statement is executing no other thread can execute.
The latter allows multiple threads to run concurrently,
just not within the critical section.
The former is rarely available to a programmer (e.g., none of
Python, C, or Java support it), while the latter
is very common.

In Harmony, \texttt{atomic} statements allow you to \emph{implement} your own
interlock primitives such as test-and-set.  Other common examples
include compare-and swap and fetch-and-add.  \texttt{atomic} statements
are not intended to \emph{replace} locks or other synchonization primitives.
When solving synchronization problems you should not directly use
\texttt{atomic} statements but use the synchronization primitives that are available
to you.  But if you want to design a new synchronization primitive, then
use \textbf{atomic} by all means.
You can also use \textbf{atomic} statements in your test code.
In fact, as mentioned before, \texttt{assert} statements are executed atomically.

\begin{figure}
\begin{code}
\harmonysource{lockintf}
\end{code}
\caption{\harmonylink{modules/synch.hny} The binary semaphore interface and implementation in the \texttt{synch} module.}
\label{fig:spinlocks}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{UpLock}
\end{code}
\caption{\harmonylink{code/UpLock.hny} Program of \autoref{fig:inc} fixed with a lock.}
\label{fig:incfixed}
\end{figure}

Locks are probably the most prevalent and basic form of synchronization
in concurrent programs.  Typically, whenever you have a shared data
structure, you want to protect the data structure with a lock and
acquire the lock before access and release it immediately afterward.
In other words, you want the access to the data structure to be a
critical section.
That way, when a thread makes modifications to the data structure that take
multiple steps, other threads will not see the intermediate inconsistent
states of the data structure.
When there is a bug in a program because some code omitted obtaining
a lock before accessing the data structure, that is known as a
\emph{data race}.
\index{data race}

\index{binary semaphore}
Locks are a special case of \emph{binary semaphores}.
Like locks, binary semaphores are either in an \emph{acquired} state
or a \emph{released} state.
Processes trying to acquire an already acquired binary semaphore
have to wait until the binary semaphore is released.
It is illegal to release a binary semaphore that is in a released
state.
A lock is a binary semaphore that, by convention, is initialized to a
released state, and is always released by the same thread that acquired it.
A binary semaphore can be initialized to an acquired state and can be
released by threads other than the thread that last acquired the
binary semaphore.

Harmony has a module called \texttt{synch} that includes support for
binary semaphores and locks.
\autoref{fig:spinlocks} shows how they are implemented, and
\autoref{fig:incfixed} gives an example of how they may be used,
in this case to fix the program of \autoref{fig:inc}.
Notice that the module hides the implementation of
binary semaphores.
The \texttt{synch} module includes a variety of other useful
synchronization primitives, which will be discussed in later
chapters.

\glsadd{blocked thread}%

We call a thread \emph{blocked}
\index{blocked thread}%
if a thread cannot change the state or terminate unless
another thread changes the state first.
A thread trying to
acquire a test-and-set spinlock held by another thread is a good example
of a thread being blocked.
The only way forward is if the other thread releases the spinlock.
A thread that is in an infinite loop is also considered blocked,
although Harmony can often detect infinite loops specifically.

\begin{figure}
\begin{code}
\harmonysource{locksusp}
\end{code}
\caption{\harmonylink{modules/syncS.hny} The binary semaphore interface in the \texttt{synchS} module uses suspension.}
\label{fig:suspension}
\end{figure}

In most operating systems, threads are virtual (as opposed to ``raw CPU cores'')
and can be suspended until some condition changes.
For example, a thread that is trying to acquire a lock can be suspended until the lock is
available.
In Harmony, a thread can suspend itself and save its context (state) in a
list.  Recall that the context of a thread consists of its name tag,
its program counter, and the contents of its register and stack.
A context is a regular (if complex) Harmony value.
The syntax of the expression is as follows:

\begin{code}
\textbf{stop} $L$
\end{code}

Here $L$ is a shared variable containing a list.
Another thread can revive the thread using the \textbf{go}
\index{go statement}%
statement:

\begin{code}
\textbf{go} $C$ $R$;
\end{code}

Here $C$ is a context and $R$ is a Harmony value.
It causes a thread with context $C$ to be added to the state that has
just executed the \textbf{stop}
\index{stop expression}%
expression.  The \textbf{stop} expression returns the value $R$.

There is a second version of the \texttt{synch} module, called \texttt{synchS},
that uses suspension instead of spinlocks.
\autoref{fig:suspension} shows the same interface implemented
using suspension.
The interface is implemented as follows:
\begin{itemize}
\item A binary semaphore maintains both a boolean indicating whether the
binary semaphore is currently acquired and a list of contexts of threads that want to
acquire the binary semaphore.
\item
\texttt{acquire}()
\index{acquire}%
acquires the binary semaphore if available and suspends the invoking thread if not.
Note that \textbf{stop} is called within an \textbf{atomic} statement---this is
the only exception to an atomic statement running to completion.  While the
thread is running no other threads can run, but when the thread suspends itself
other threads can run.
\item
\texttt{release}()
\index{release}%
checks to see if any threads are waiting to acquire the binary semaphore.
If so, it uses the \texttt{head} and \texttt{tail}
methods from the \texttt{list} module (see Appendix~\ref{ap:module})
to resume the first thread that got
suspended and to remove its context from the list.
\end{itemize}
Selecting the first thread is a design choice.  Another implementation could
have picked the last one, and yet another implementation could have used
\texttt{choose} to pick an arbitrary one.  Selecting the first is a common
choice in binary semaphore implementations as it prevents \emph{starvation}:
\index{starvation}%
every thread
gets a chance to acquire the binary semaphore (assuming every thread eventually releases
it).  \autoref{ch:starvation} will talk more about starvation and how
to prevent it.

Harmony allows you to select which version of the \texttt{synch} module you would
like to use with the \texttt{-m} flag.
\index{module}%
For example,

\begin{code}
\begin{verbatim}
harmony -m synch=synchS x.hny
\end{verbatim}
\end{code}

runs the file \texttt{x.hny} using the suspension version of the \texttt{synch} module.
You will find that using the \texttt{synchS} module often leads to
the model checker searching a
significantly larger state space than using the \texttt{synch} module.
Part of the reason is that the \texttt{synchS} module keeps track of the order
in which threads wait to acquire a binary semaphore, while the \texttt{synch} module does not.

\section*{Exercises}
\begin{problems}
\item
Run \autoref{fig:incfixed} using (i) \texttt{synch} and (ii) \texttt{synchS}.
Report how many states were explored by Harmony for each module.
\item \label{ex:xy} \autoref{fig:xy} shows a Harmony program with two variables $x$
(initially 0) and $y$ (initially 100) that can be accessed through methods
\texttt{setX} and \texttt{getXY}.  An application invariant is that \texttt{getXY}
should return a pair that sums to 100.  Add the necessary synchronization code.
\item \label{ex:trylock} Implement \texttt{tryAcquire}(\textit{b}) as an additional
interface for both the \texttt{synch} and \texttt{synchS} modules.
This interface is like \texttt{acquire}(\textit{b}) but never blocks.  It
returns \texttt{True} if the binary semaphore was available (and now acquired) or \texttt{False}
if the binary semaphore was already acquired.
Hint: you do not have to change the existing code.
\end{problems}

\begin{figure}
\begin{code}
\harmonysource{xy}
\end{code}
\caption{\harmonylink{code/xy.hny} Incomplete code with desired invariant $x + y = 100$.}
\label{fig:xy}
\end{figure}

\chapter{Concurrent Data Structures}
\label{ch:cds}

The most common use for locks is in building concurrent data structures.
By way of example, we will demonstrate how to build a concurrent queue.
The \texttt{queue} module will have the following API:
\begin{itemize}
\item $q = \mathtt{Queue}()$: allocate a new queue;
\item $\mathtt{enqueue}(q, v)$: add $v$ to the tail of the queue;
\item $r = \mathtt{dequeue}(q)$: returns $r = ()$ if $q$ is empty or $r = (v,)$ (a singleton tuple)
if $v$ was at the head of the queue.
\end{itemize}
See \autoref{fig:queuetest} for a simple test program that uses the queue.

We will first implement the queue as a linked list.
The implementation in \autoref{fig:queue}
uses the \texttt{alloc} module for dynamic allocation
\index{dynamic allocation}%
of nodes in the list using \texttt{malloc}() and \texttt{free}().
\texttt{malloc}($v$) returns a new memory location initialized to $v$,
which should be released with \texttt{free}() when it is no longer in use.
The queue maintains a \texttt{head} pointer to the first element in the list
and a \texttt{tail} pointer to the last element in the list.
The \texttt{head} pointer is \textbf{None} if and only if the queue is empty.
(\textbf{None} is a special address value that is not the address of any
memory location.)

\texttt{queue.Queue()} returns a new queue object consisting of a \textbf{None} head
and tail pointer and a lock.
$\mathtt{queue.enqueue}(q, v)$ and $\mathtt{queue.dequeue}(q)$ both take a pointer $q$ to the
queue object because both may modify the queue.
Before they access the value of the head or tail of the queue they first obtain
the lock.
When they are done, they release the lock.

The implementation in \autoref{fig:queueMS}~\cite{MS98} is similar but uses
separate locks for the head and tail, allowing an enqueue and a dequeue operation
to proceed concurrently.  To avoid contention between the head and the tail,
the queue uses a dummy node at the head of the linked list.
Except initially, the dummy node is the last node that was dequeued.
Note that neither the \texttt{head} nor \texttt{tail} pointer are
ever \textbf{None}.

\begin{figure}
\begin{code}
\harmonysource{queuetest}
\end{code}
\caption{\harmonylink{code/queuetest.hny} Test program for a concurrent queue.}
\label{fig:queuetest}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{queue}
\end{code}
\caption{\harmonylink{code/queue.hny}A basic concurrent queue data structure.}
\label{fig:queue}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{queueMS}
\end{code}
\caption{\harmonylink{code/queueMS.hny} A queue with separate locks for enqueuing and dequeuing items.}
\label{fig:queueMS}
\end{figure}

\section*{Exercises}
\begin{problems}
\item \label{ex:qcontains} Add a method $\mathtt{contains}(q, v)$ to both \autoref{fig:queue} and \autoref{fig:queueMS}
that checks to see if $v$ is in queue $q$.
\item Add a method $\mathtt{remove}(q, v)$ to both \autoref{fig:queue} and \autoref{fig:queueMS}
that removes all occurrences of $v$, if any, from queue $q$.
\item The test program in \autoref{fig:queuetest} is not very thorough.  Design
and implement a better one.
\item Create a thread-safe sorted binary tree.  Implement a module
\texttt{bintree} with methods
$\mathtt{BinTree}()$ to create a new binary tree, $\mathtt{insert}(t, v)$ that inserts $v$ into
tree $t$, and $\mathtt{contains}(t, v)$ that checks if $v$ is in tree $t$.  Use a single
lock per binary tree.
\item Bonus problem: create a binary tree that uses, instead of a single lock per tree,
a lock for each node in the tree.
\end{problems}

\chapter{Conditional Waiting}
\label{ch:condwait}

Critical sections enable multiple threads
to easily share data structures whose modification
requires multiple steps.
A critical section only allows one thread to execute the code
of the critical section at a time.
Therefore, when a thread arrives at a critical section,
the thread blocks until there is no other thread in the critical section.

\glsadd{busy waiting}%
\index{busy waiting}%
Sometimes it is useful for a thread to block waiting for additional
conditions.
For example, when dequeuing from an empty shared queue,
it may be useful for the thread to block until the queue is non-empty
instead of returning an error.
The alternative would be \emph{busy waiting} (aka \emph{spin-waiting}),
where the thread repeatedly tries to dequeue an item until it is successful.
Doing so wastes CPU cycles and adds contention to queue access.
%
A thread that is busy waiting until the queue is non-empty cannot
make progress until another thread enqueues an item.
However, the thread is not considered \emph{blocked} because it is
changing the shared state by repeatedly acquiring and releasing the
lock.
We would like to find a solution to \emph{conditional waiting}
so that a thread blocks until the condition holds---or at least most
of the time.

Before we do so, we will give two classic examples of synchronization
problems that involve conditional waiting: \emph{reader/writer locks}
and \emph{bounded buffers}.

\begin{figure}
\begin{code}
\harmonysource{RWtest}
\end{code}
\caption{\harmonylink{code/RWtest.hny} Test code for reader/writer locks.}
\label{fig:rwtest}
\end{figure}

\section{Reader/Writer Locks}
\index{reader/writer lock}%
\glsadd{reader/writer lock}%

\begin{comment}
\begin{figure}
\begin{code}
\harmonysource{RW}
\end{code}
\caption{\harmonylink{code/RW.hny} Busy-Waiting Reader/Writer Lock implementation.}
\label{fig:rwbusy}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=2.3in]{figures/rdwr.png}
\end{center}
\caption{High-level state diagram specification of reader/writer locks with
up to two threads.
The first number in a state gives the number of threads; the second number is the
number of threads reading in the critical section; the third is the number of
threads writing in the critical section.}
\label{fig:rdwr}
\end{figure}
\end{comment}

Locks are useful when accessing a shared data structure.  By preventing
more than one thread from accessing the data structure at the same
time, conflicting accesses are avoided.  However, not all concurrent
accesses conflict, and opportunities for concurrency may be lost,
hurting performance.  One important case is when multiple threads
are simply reading the data structure.
In many applications, reads are the majority of all accesses.
Allowing reads to proceed concurrently can significantly improve performance.

What we want is a special kind of lock that allows either (i) one writer
or (ii) one or more readers to acquire the lock.  This is called
a \emph{reader/writer lock}~\cite{CHP71}.
%
A reader/writer lock is an object whose abstract (and opaque)
state contains two integer counters:
\begin{enumerate}
\item \textit{nreaders}: the number of readers
\item \textit{nwriters}: the number of writers
\end{enumerate}
satisfying the following invariant:
\begin{itemize}
\item [] $(\mathit{nreaders} \ge 0 \land \mathit{nwriters} = 0) \lor
    (\mathit{nreaders} = 0 \land 0 \le \mathit{nwriters} \le 1)$
\end{itemize}

There are four operations on a reader/writer lock \textit{rw}:
\begin{itemize}
\item \texttt{read\_acquire}(\textit{rw}): waits until $\mathit{nwriters} = 0$
and then increments \textit{nreaders};
\item \texttt{read\_release}(\textit{rw}): decrements $\mathit{nreaders}$;
\item \texttt{write\_acquire}(\textit{rw}): waits until
$\mathit{nreaders} = \mathit{nwriters} = 0$
and then sets \textit{nwriters} to 1;
\item \texttt{write\_release}(\textit{rw}): sets $\mathit{nwriters}$ to 0.
\end{itemize}

\autoref{fig:rwtest} shows how reader/writer locks operations
may be tested.
Similar to ordinary locks, a thread is restricted in how it is allowed to
invoke these operations.
In particular, a thread can only release a reader/writer lock for reading
if it acquired it for reading and the same for writing.
Moreover, a thread is only allowed the acquire a reader/writer lock once.

\begin{comment}
We will explore various ways of implementing reader/writer locks in
this chapter and future ones.

\autoref{fig:rwbusy} presents a solution that uses a
single (ordinary) lock and two counters: one that maintains the number
of readers and one that maintains the number of writers.
\autoref{fig:rwtest} shows how the code may be used.
We will call the lock the \emph{mutex} to distinguish it clearly from
the reader/writer lock that we are implementing.
The mutex is used to protect shared access to the counters.
The program shows a thread that executes in a loop.
Each time, it decides whether to read or write.
The critical section is spread between two labels:
readers access \texttt{@rcs} and writers access \texttt{@wcs}.
The specification is that if a reader is at label \texttt{@rcs},
no writer is allowed to be at label \texttt{@wcs}.  Vice versa, if
a writer is at label \texttt{@wcs}, no reader is allowed to be at
label \texttt{@rcs} \emph{and} there cannot be another writer at
label \texttt{@wcs}.

\autoref{fig:rdwr} shows a high-level
specification for two threads.

A thread that wants to read first waits until there are no writers:
\textit{nwriters} == 0.  Then it increments the number of readers.
Similarly, a
thread that wants to write waits until there are no readers \emph{or} writers.
Then it increments the number of writers.
The important invariants in this code are:
\begin{itemize}
\item $n$ readers at \texttt{@rcs} $\Rightarrow \mathit{nreaders} \ge n$,
\item $n$ writers at \texttt{@wcs} $\Rightarrow \mathit{nwriters} \ge n$,
\item $(\mathit{nreaders} \ge 0 \land \mathit{nwriters} = 0) \lor
    (\mathit{nreaders} = 0 \land 0 \le \mathit{nwriters} \le 1)$.
\end{itemize}
It is easy to see that the invariants hold and imply the reader/writer
specification.
The solution also supports progress: if no thread is in the critical
section then any thread can enter.  Better still: if any reader is in the
critical section, any other reader is also able to enter.

\begin{figure}
\begin{code}
\harmonysource{RWbusychk}
\end{code}
\caption{\harmonylink{code/RWbusychk.hny} Checking for Busy Waiting.}
\label{fig:rwblock}
\end{figure}

\glsadd{busy waiting}%

While correct, it is not considered a good solution.
The solution is an example of what is called \emph{busy-waiting}
\index{busy waiting}%
(aka \emph{spin-waiting}):
\index{spin waiting}%
threads spin in a loop until some desirable application-level condition is met,
wasting CPU cycles.
\autoref{fig:inc} also has an example of busy-waiting: the \texttt{main}
thread waits for the other two threads to finish by checking their \texttt{done}
flags.

The astute reader might wonder if obtaining the mutex itself is an
example of busy-waiting.  After all, the Harmony \texttt{synch} implementation
of \texttt{lock()} spins in a loop until the mutex is available
(see \autoref{fig:spinlocks}).
However, there is a big difference.  As we pointed out, the threads that
are waiting for a spinlock are \emph{blocked}.
In most operating systems and programming language runtimes,
when a thread acquires a mutex, the thread is placed on a scheduling
queue and stops using CPU cycles until the mutex becomes available.
Harmony supports this with the \texttt{synchS} module.
Busy waiting disables all this: even when using the \texttt{synchS}
module, readers and writers only get suspended temporarily in case
there is contention for the mutex.
A thread trying to obtain a read or write lock in
\autoref{fig:rwbusy} tries to obtain the mutex repeatedly.

Thus, it is considered ok to have threads be blocked while waiting
for application-specific conditions, but not for them to
busy-wait for application-specific conditions.

Harmony does not complain about \autoref{fig:rwbusy}, but, using
the right test, Harmony can be used to check for busy-waiting.
\autoref{fig:rwblock} presents such a test.
Note that the initialization code obtains a write lock, preventing
all readers and writers from entering the critical section.
In that case, those threads should ideally be blocked and Harmony
can test for that by running it with the \texttt{-b} flag.
This flag tells Harmony to check
that all states are non-terminating and lead to a state in which
all threads are blocked.

\section*{Exercises}
\begin{problems}
\item Draw additional states and steps in \autoref{fig:rdwr}
for three threads.
\end{problems}

\chapter{Reader/Writer Locks with Blocking}
\label{ch:rdwr}

\begin{figure}
\begin{code}
\harmonysource{RWlock}
\end{code}
\caption{\harmonylink{code/RWlock.hny} Reader/Writer with Two Locks.}
\label{fig:rw2lock}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{RWmulti}
\end{code}
\caption{\harmonylink{code/RWmulti.hny} Checking that multiple readers can acquire the read lock.}
\label{fig:rwmulti}
\end{figure}

\autoref{fig:rw2lock} presents a reader/writer lock implementation
that does not busy-wait.
You can test the code by running \texttt{harmony -m RW=RWlock RWtest.hny}.
It uses two ordinary locks: \textit{rwlock} is used by both readers and writers,
while \textit{rlock} is only used by readers.
\textit{rwlock} is held either when readers are at label \texttt{@rcs}
or when a writer is at label \texttt{@wcs}.
\textit{rlock} is used to protect the \textit{nreaders} variable that
counts the number of readers in the critical section. 

The invariants that imply the reader/writer specification
(which are again easy to verify) are as follows:

\begin{itemize}
\item $n$ readers at \texttt{@rcs} $\Rightarrow \mathit{nreaders} \ge n$
\item $\exists$ writer at \texttt{@wcs} $\Rightarrow \mathit{nreaders} = 0$ and writer holds \textit{rwlock}
\item at most one writer can hold \textit{rwlock}
\end{itemize}

A writer just acquires \textit{rwlock} to enter the critical section
and releases it to exit.  The \emph{first} reader to enter the critical
section acquires \textit{rwlock} and the \emph{last} reader to exit
the critical section releases \textit{rwlock}.
The implementation satisfies progress: if no thread is in the critical
section then any thread can enter.

It is instructive to see what happens when a writer is in the critical
section and two readers try to enter.  The first reader successfully
acquires \textit{rlock} but blocks when trying to acquire
\textit{rwlock}, which is held by the writer.  The second reader blocks
trying acquire \textit{rlock} because it is held by the first reader.
When the writer leaves the critical section, the first reader acquires
\textit{rwlock}, sets \textit{nreaders} to 1, and releases \textit{rlock}.
\textit{rwlock} is still held.
Then the second reader acquires \textit{rlock} and, assuming the first
reader is still in the critical section, increments \textit{nreaders} to 2
and enters the critical section \emph{without} acquiring \textit{rwlock}.
\textit{rwlock} is essentially jointly held by both readers.
It does not matter in which order they leave: the second will release
\textit{rwlock}.

While Harmony does not detect any issues, we must remember what it is checking
for: it checks to make sure that there are never a reader and a writer
in the critical section, and there are never two writers in the critical
section.  Moreover, it checks progress: all executions eventually terminate.
What it does \emph{not} check is if the code allows more than one reader
in the critical section.  Indeed, we could have implemented the reader/writer
lock using an ordinary lock, never allowing
more than one thread in the critical section, and Harmony would not have
complained.

\autoref{fig:rwmulti} contains a new test that ensures that indeed more
than one reader can enter the critical section at a time.  It does so by
having threads sometimes wait in the critical section until all other readers
are there.  If other readers cannot enter, then that thread enters in an
infinite loop that Harmony will readily detect.  Try it out!

An important lesson here is that one should not celebrate too early if Harmony
does not find any issues.  While a test may be successful, a test may not
explore all desirable executions.  While Harmony can help explore all cornercases
in a test, it cannot find problems that the test is not looking for.

\section*{Exercises}
\begin{problems}
\item Why do you suppose the code in \autoref{fig:rwmulti} uses a flag
per thread rather than a simple counter?
Can you write a version of this test that uses a counter instead
of a flag per thread?
% \item Write a ``library implementation'' of reader/writer locks so multiple
% reader/writer locks can be instantiated.
\end{problems}

\chapter{Semaphores}
\label{ch:semaphore}
\index{semaphore}%

\glsadd{semaphore}%

\begin{figure}
\begin{code}
\harmonysource{semaphore}
\end{code}
\caption{\harmonylink{modules/synch.hny} The \texttt{Semaphore} interface in the \texttt{synch} module.}
\label{fig:semaphore}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{DinersSema}
\end{code}
\caption{\harmonylink{code/DinersSema.hny} Italian Philosophers.}
\label{fig:dinerssema}
\end{figure}

So far we have looked at how to protect a single resource.
Some types of resources may have multiple units.
If there are only $n$ units, no more than $n$ units can be used at a time;
if all are in use and a thread comes along that needs one of the units,
it has to wait until another thread releases one of the units.
Note that we cannot solve this problem simply using a lock per unit---allocating
a unit requires access to the entire collection.

Consider the following synchronization problem that we call
\emph{Italian Philosophers} (a simplification of
the famous Dining Philosophers problem described in \autoref{ch:deadlock}).
There are five philosophers sitting around a table, each with a plate of
spaghetti in front of them.
There is a glass sitting in the middle of the table with only
three forks in it.  A philosopher needs one of those forks to eat, but clearly,
no more than three philosophers can eat at a time.
Unfortunately, a lock can only count to one.

Introduced by Dijkstra,
a \emph{semaphore} is a synchronization primitive that
can solve this kind of problem~\cite{EWD35}.
A semaphore is essentially
a counter that can be incremented and decremented but is not allowed to go
below zero.  The semaphore counter is typically initialized to the number of
units of a resource initially available.
When allocating a resource, a thread decrements the
counter using the \texttt{P}
\index{P}%
\index{procure}%
operation.  You can think of \texttt{P} standing
for \emph{Procure}, as in procuring the resource associated with the semaphore.
The \texttt{P} operation blocks the invoking thread if the counter is zero.
To release the resource, a thread increments the resource using the
\texttt{V}
\index{V}%
\index{vacate}%
operation.  You can think of \texttt{V} standing for \emph{vacate},
as in vacating the resource.
One can think of a semaphore as a generalization of a lock.  In fact, a
lock can be implemented by a semaphore initialized to 1.  Then \texttt{P}
procures the lock, and \texttt{V} vacates the lock.

\autoref{fig:semaphore} shows the \texttt{synch} module implementation of
semaphores.
It represents a semaphore as an integer.
\autoref{fig:dinerssema} shows a solution to the Italian Philosphers problem
using a semaphore.

You may find the free ``The Little Book of Semaphores'' by
Allen Downey a great resource for working with semaphores~\cite{Downey09}.

\section*{Exercises}
\begin{problems}
\item How might you check with Harmony that at most three
Italian philosophers can eat at the same time?
\item Implement a test program that checks to see that
the solution allows more than one Italian philosopher to eat at the same time?
\end{problems}
\end{comment}

\section{Bounded Buffer}
\index{bounded buffer}%
\index{producer/consumer problem}%
\glsadd{producer/consumer problem}%

\begin{comment}
\begin{figure}
\begin{center}
\includegraphics[width=2.5in]{figures/pc-crop.pdf}
\end{center}
\caption{High-level state diagram specification of producers/consumers.}
\label{fig:pc}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{BBsema}
\end{code}
\caption{\harmonylink{code/BBsema.hny} Bounded Buffer implementation using semaphores.}
\label{fig:boundedbuffer}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{BBsematest}
\end{code}
\caption{\harmonylink{code/BBsematest.hny} Test program for \autoref{fig:boundedbuffer}.}
\label{fig:bbtest}
\end{figure}

\begin{figure}
\begin{code}
\begin{verbatim}
harmony -b -c NSLOTS=0 -c NPRODS=1 -c NCONSS=1 BBsematest.hny
harmony -b -c NSLOTS=1 -c NPRODS=0 -c NCONSS=1 BBsematest.hny
harmony    -c NSLOTS=1 -c NPRODS=1 -c NCONSS=0 BBsematest.hny
harmony    -c NSLOTS=1 -c NPRODS=1 -c NCONSS=1 BBsematest.hny
harmony -b -c NSLOTS=1 -c NPRODS=1 -c NCONSS=2 BBsematest.hny
harmony -b -c NSLOTS=1 -c NPRODS=2 -c NCONSS=0 BBsematest.hny
harmony    -c NSLOTS=1 -c NPRODS=2 -c NCONSS=1 BBsematest.hny
harmony    -c NSLOTS=1 -c NPRODS=2 -c NCONSS=2 BBsematest.hny
harmony -b -c NSLOTS=1 -c NPRODS=2 -c NCONSS=3 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=1 -c NCONSS=0 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=1 -c NCONSS=1 BBsematest.hny
harmony -b -c NSLOTS=2 -c NPRODS=1 -c NCONSS=2 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=2 -c NCONSS=0 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=2 -c NCONSS=1 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=2 -c NCONSS=2 BBsematest.hny
harmony -b -c NSLOTS=2 -c NPRODS=2 -c NCONSS=3 BBsematest.hny
harmony -b -c NSLOTS=2 -c NPRODS=3 -c NCONSS=0 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=3 -c NCONSS=1 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=3 -c NCONSS=2 BBsematest.hny
harmony    -c NSLOTS=2 -c NPRODS=3 -c NCONSS=3 BBsematest.hny
\end{verbatim}
\end{code}
\caption{Script for testing producer/consumer synchronization.}
\label{fig:pcscript}
\end{figure}
\end{comment}

\index{ring buffer}%
A \emph{bounded buffer} (aka \emph{ring buffer}) is
a queue with the usual enqueue/dequeue interface,
but implemented using a circular buffer
\index{circular buffer}%
of a certain length and two pointers:
the \emph{tail} points where new items are enqueued and the \emph{head}
points where items are dequeued.
If the buffer is full, the enqueur must wait; if the buffer is empty, the
dequeuer must wait.
This problem is known as the ``Producer/Consumer Problem'' and was
proposed by Dijkstra~\cite{EWD329}.

The producer/consumer pattern is common.  Threads may be arranged
in \emph{pipelines},
\index{pipeline}%
where each upstream thread is a producer and each downstream
thread is a consumer.
Or threads may be arranged in a manager/worker pattern, with a manager
producing jobs and workers consuming and executing them in parallel.
Or, in the client/server model,
\index{client/server model}%
some thread may act as a \emph{server} that clients can send requests to
and receive responses from.  In that case, there is a bounded buffer
for each client/server pair. Clients produce requests and
consume responses, while the server consumes requests and produces responses.
%
Multiple producers and multiple consumers may all share
the same bounded buffer.

\begin{comment}
\autoref{fig:pc} gives a high-level description.  The
two numbers in a state specify
the number of items that the producers still can produce and
the number of items that the consumers have consumed.  In this
description there are three items to produce initially.

\begin{figure}
\begin{code}
\harmonysource{BBsemadata}
\end{code}
\caption{\harmonylink{code/BBsemadata.hny} Test whether the bounded buffer delivers the right data in the
right order.}
\label{fig:BBsemadata}
\end{figure}

\autoref{fig:boundedbuffer} presents the implementation of a bounded
buffer using semaphores.  It features a circular bounded buffer \textit{buf} with
slots numbered 1 through \texttt{NSLOTS}.  There are two indexes into
the buffer: \textit{b\_in} specifies where the next produced item is inserted,
while \textit{b\_out} specifies from where the next can be consumed.
As there may be multiple producers and multiple
consumers, updates to the indexes, which are shared variables, must be protected.
To this end, we use two semaphores as locks: \textit{l\_in} for \textit{b\_in}
and \textit{l\_out} for \textit{b\_out}.

Without additional synchronization, the indexes may overflow and point to invalid
entries in the circular buffer.
For this, there is a semaphore \textit{n\_full} that
keeps track of the number of filled entries in the buffer and a semaphore
\textit{n\_empty} that keeps track of the number of empty entries in the buffer.

To add an item to the bounded buffer (\texttt{produce}(\textit{item})), the producing
thread first has to wait until there is room in the bounded buffer.
To this end, it invokes \texttt{P}(\textit{n\_empty}), which waits until
$\mathit{n\_empty} > 0$ and atomically decrements \textit{n\_empty} once this
is the case.  In other words, the producer procures an empty slot.
Next, it adds the item to the next position in the buffer.
Since there may be more than one empty slot, multiple producers may attempt
to do the same thing concurrently.  Hence the use of the \textit{l\_in}
semaphore used as a lock.  Finally, once the item is added, the thread
increments the \textit{n\_full} semaphore to signal to consumers that
an item is available.
The consumer code is symmetric.

Note the quite different usage of the \textit{l\_} semaphores and \textit{n\_}
semaphores.
People often call semaphores that are used as locks \emph{binary semaphores},
\index{binary semaphore}%
as they only take on the values 0 and 1.
They protect critical sections.
The second type of semaphores are called \emph{counting semaphores}.
\index{counting semaphore}%
They can be used to send signals between threads.
However, binary and counting semaphores are implemented the same way.

\autoref{fig:bbtest} invokes the code of \autoref{fig:boundedbuffer}.
We can run this through Harmony, but what would it test?  There are
no assert statements in the code.  We can split the functionality
of this code into two and test each separately.  One is the
synchronization part: producers should never run more than
\texttt{NSLOTS} ahead of the consumers, but they should be able to
produce up to \texttt{NSLOTS} items even if there are no consumers.
The other part is the data: we want to make sure that the items are
sent through the buffer in order and without loss.  We will first
focus on the synchronization.

The first part can be tested with the code given in \autoref{fig:pc}, by
experimenting with different non-negative values for the three constants.
In particular, any run with a positive \texttt{NSLOTS} and
$\mathtt{NCONSS} \le \mathtt{NPRODS} \le \mathtt{NCONSS} + \mathtt{NSLOTS}$
should terminate, while other values should lead to threads blocking.
To this this, one can write a script like the one in \autoref{fig:pcscript}.

\autoref{fig:BBsemadata} tests whether the correct data is sent in
the correct order.  There are the same number of producers and consumers.
Each producer $i$ produces two tuples: $(i, 0)$ and $(i, 1)$ in that order.
Each consumer consumes two tuples and checks that the tuples are different
and that, if the two tuples come from the same producer, then they have to
be in the order sent.
Moreover, consumers add the tuples they received to the set
\textit{received} (atomically---a lock could have been used here as well).
The \texttt{main()} thread waits for all consumers to have received
exactly the set of tuples that the producers produce.

\section*{Exercises}
\begin{problems}
\end{problems}
\end{comment}

\chapter{Split Binary Semaphores}
\label{ch:sbs}
\index{split binary semaphore}%

\begin{figure}
\begin{code}
\harmonysource{RWsbs}
\end{code}
\caption{\harmonylink{code/RWsbs.hny} Reader/Writer Lock using Split Binary Semaphores.}
\label{fig:RWsplitsema}
\end{figure}

\glsadd{conditional critical section}%

Split Binary Semaphores (SBS) is a general technique to implement conditional
waiting.  It was originally proposed by
Tony Hoare and popularized by Edsger Dijkstra~\cite{EWD703}.
SBS is an extension of a critical section that is protected by a single
binary semaphore.
If there are $n$ \emph{waiting conditions},
then SBS uses $n+1$ binary semaphores to protect the critical section.
An ordinary critical section has no waiting conditions.
Reader/writer locks have two waiting conditions:
\begin{enumerate}
\item readers waiting for a writer to release the lock;
\item writers waiting for another writer or all readers to release the lock.
\end{enumerate}
Similarly, a bounded buffer has two waiting conditions:
\begin{enumerate}
\item consumers waiting for the buffer to be non-empty;
\item producers waiting for an empty slot in the buffer.
\end{enumerate}
So each will require 3 binary semaphores if the SBS technique is applied.

Think of each of these semaphores as a gate that a thread must go
through in order to enter the critical section.  A gate is either open
or closed.  Initially, exactly one gate, the main gate, is open.
Each of the other gates, the \emph{waiting gates}, is associated with a
waiting condition.
When a gate is open, one thread can enter the critical section,
closing the gate behind it.

When leaving the critical section, the thread must open exactly one
of the gates, but it does not have to be the gate that it used to enter
the critical section.
In particular, when a thread leaves the critical section it should
check for each waiting gate if its waiting condition hold and if there are
processes trying to get through the gate.  If there is such a gate,
it must select one and open that gate.  If there is no such gate,
it must open the main gate.

Finally, if a thread is executing in the critical section and needs to
wait for a particular condition, it leaves the critical section and waits
for the gate associated with that condition to open.

Note that the following invariants holds:
\begin{itemize}
\item At any time, at most one gate is open;
\item If some gate is open, then no thread is in the critical section;
\item If some thread is in the critical section, all gates are closed;
\item At any time, at most one thread is in the critical section.
\end{itemize}

The main gate is implemented by a binary semaphore, initialized in the
released state (signifying that the gate is open).
The waiting gates each consist of a pair: a counter that counts how many
threads are waiting behind the gate, and a binary semaphore initialized
in the acquired state (signifying that the gate is closed).

We will illustrate the technique using the reader/writer problem.
\autoref{fig:RWsplitsema} shows code.
The first step is to enumerate all waiting conditions.
In the case of the reader/writer
problem, there are two: a thread that wants to read may have to wait for a
writer to leave the critical section, while a thread that wants to write may
have to wait until all readers have left the critical section or until a writer has left.

The state of a reader/writer lock thus consists of the following:
\begin{itemize}
\item \textit{nreaders}: the number of readers in the critical section;
\item \textit{nwriters}: the number of writers in the critical section;
\item \textit{mutex}: the ``main gate'' binary semaphore;
\item \textit{r\_gate}: the ``waiting gate'' used by readers, consisting of a binary semaphore and the number of readers waiting to enter;
\item \textit{w\_gate}: the ``waiting gate'' used by writers, similar to the readers' gate.
\end{itemize}

Each of the
\texttt{read\_acquire}, \texttt{read\_release},
\texttt{write\_acquire}, and \texttt{write\_release} methods must maintain
this state.
Each of these methods first has to acquire the \textit{mutex}
(i.e., enter the main gate).
\texttt{read\_acquire} and \texttt{write\_acquire} must check to see
if it has to wait.
If so, it increments the count associated with its respective gate,
opens a gate (using \texttt{release\_one}, and then blocks until
its waiting gate opens up.

\texttt{release\_one()} is the function that a thread uses when leaving
the critical section.  It must check to see if there is a waiting gate
that has threads waiting behind it and whose condition is met.
If so, it selects one and opens that gate.  In the given code,
\texttt{release\_one()} first checks the readers' gate and then the
writers' gate, but the other way around would have worked as well.
If neither waiting gate qualifies, then \texttt{release\_one()} 
has to open the main gate (i.e., release \textit{mutex}).

Let us examine \texttt{read\_acquire} more carefully.
First the method acquires the \textit{mutex}.
Then, in the case that the thread, say $p$,
finds that there is a writer in the critical section
($\mathit{nwriters > 0}$), it increments the counter associated with
the readers' gate, leaves the critical section (\texttt{release\_one}),
and then tries to acquire the semaphore associated with the gate.
This will cause it to block until some thread opens that gate.

Now consider the case where there is a writer in the critical
section and there are two readers waiting.  Let us see what happens when
the writer calls \texttt{write\_release}:
\begin{enumerate}
\item After acquiring \textit{mutex}, the writer decrements
\textit{nwriters}, which must be 1 at this time, and thus becomes 0.
\item It then calls \texttt{release\_one()}.
\texttt{release\_one()} finds that there are no writers in the critical section
and there are two readers waiting.  It therefore releases not
\textit{mutex} but the readers' gate's binary semaphore.
\item One of the waiting reader can now re-enter the critical section.
When it does, the reader decrements the gate's counter (from 2 to 1),
and increments \textit{nreaders} (from~0 to~1).
The reader finally calls \texttt{release\_one()}.
\item Again, \texttt{release\_one()} finds that there are no writers and
that there are readers waiting, so again it releases the readers' semaphore.
\item The second reader can now enter the critical section.
It decrements the gate's count from 1 to 0 and increments \textit{nreaders}
from~1 to~2.
\item Finally, the second reader  calls \texttt{release\_one()}.
This time \texttt{release\_one()} does not find any threads waiting,
and so it releases \textit{mutex}.
There are now two threads that are holding the reader/writer lock.
\end{enumerate}

\section*{Exercises}
\begin{problems}
% \item Assume that you have only binary semaphores.
% Implement counting semaphores using the split binary semaphores technique.
\item Several of the instantiations of \texttt{release\_one()} in
\autoref{fig:RWsplitsema} can be replaced by simply releasing \textit{mutex}.
Which ones?
\item Optimize your solutions to \autoref{ex:qcontains} to use reader/writer locks.
\item \label{ex:bbsbs} Implement a solution to the producer/consumer problem
using split binary semaphores.
\item \label{ex:boundlock} Using busy waiting, implement a ``bound lock'' that allows
up to \texttt{M} threads to acquire it at the same time.\footnote{A bound lock is a restricted version of a \emph{counting} semaphore.}
A bound lock
with \texttt{M == 1} is an ordinary lock.
You should define a constant \texttt{M} and two methods:
\texttt{acquire\_bound\_lock()}
and \texttt{release\_bound\_lock()}.
(Bound locks are useful for situations where too many threads working
at the same time might exhaust some resource such as a cache.)
\item Write a test program for your bound lock
that checks that no more than \texttt{M} threads can acquire the
bound lock.
\item Write a test program for bound locks
that checks that up to \texttt{M} threads
can acquire the bound lock at the same time.
\item Implement a \emph{bounded stack}.  Method \texttt{push}(\textit{item})
should block if the stack is full.  Method \texttt{pop()} should block
until the stack is non-empty and then return an item.
Also implement test programs for your solution.
\item \label{ex:gpu} Implement a thread-safe \emph{GPU allocator} by modifying
\autoref{fig:gpu}.
There are \texttt{N} GPUs identified by the numbers
1 through \texttt{N}.  Method \texttt{gpuAlloc()} returns the identifier
of an available GPU, blocking if there is currently no available GPU.
Method \texttt{gpuRelease}(\textit{gpu}) releases the given GPU.  It never needs
to block.
\item With reader/writer locks,
concurrency can be improved if a thread \emph{downgrades} its write lock
to a read lock when its done writing but not done reading.  Add
a \texttt{downgrade} method to the code in \autoref{fig:RWsplitsema}.
% How easy or hard would it be to do the same for the code in
% \autoref{fig:rw2lock}?
% Note, you cannot simply release the
% write lock and then obtain the read lock, because the thread needs
% to remain in the critical section.
\item \label{ex:onelane} Cornell's campus features some one-lane bridges.
On a one-lane bridge,
cars can only go in one direction at a time. Consider northbound
and southbound cars wanting to cross a one-lane bridge.
The bridge allows arbitrary many cars, as long as they're going in the
same direction.
Implement a lock that observes this requirement using SBS.
Write methods \texttt{OLBlock()} to create a new ``one lane bridge'' lock,
\texttt{nb\_enter()} that a car must invoke before going northbound on
the bridge and \texttt{nb\_leave()} that the car must invoke after leaving
the bridge.  Similarly write \texttt{sb\_enter()} and \texttt{sb\_leave()}
for southbound cars.
\item Extend the solution to \autoref{ex:onelane} by implementing the
requirement that at most $n$ cars are allowed on the bridge.  Add $n$
as an argument to \texttt{OLBlock}.

\end{problems}

\begin{figure}
\begin{code}
\harmonysource{gpu}
\end{code}
\caption{\harmonylink{code/gpu.hny} A thread-unsafe GPU allocator.}
\label{fig:gpu}
\end{figure}

\chapter{Starvation}
\label{ch:starvation}
\index{starvation}%

\glsadd{starvation}%

\begin{figure}
\begin{code}
{\small
\harmonysource{RWfair}
}
\end{code}
\caption{\harmonylink{code/RWfair.hny} Reader/Writer Lock SBS implementation addressing fairness.}
\label{fig:RWfair}
\end{figure}

\glsadd{property}%

A \emph{property}
\index{property}%
is a set of traces.
If a program has a certain property, that means that the traces that that
program allows are a subset of the traces in the property.
So far we have pursued two properties: \emph{mutual exclusion}
and \emph{progress}.  The former is an example of a
\emph{safety property}---it prevents something ``bad'' from
happening, like a reader and writer thread both entering the
critical section.  The \emph{progress} property is an example
of a \emph{liveness property}---guaranteeing that something good
eventually happens.
Informally (and inexactly), progress states that if no threads
are in the critical section, then some thread that wants to enter
can.

Progress is a weak form of liveness.  It says that \emph{some}
thread can enter, but it does not prevent a scenario such as
the following.  There are three threads repeatedly trying to
enter a critical section using a spinlock.  Two of
the threads successfully keep entering, alternating, but the third
thread never gets a turn.  This is an example of
\texttt{starvation}.  With a spinlock, this scenario could
even happen with two threads.  Initially both threads
try to acquire the spinlock.  One of the threads is
successful and enters.  After the thread leaves, it immediately
tries to re-enter.  This state is identical to the initial
state, and there is nothing that prevents the same thread
from acquiring the lock yet again.

It is worth noting that Peterson's Algorithm (\autoref{ch:peterson})
does not suffer from starvation, thanks to the \texttt{turn} variable
that alternates between 0 and 1 when two threads are contending for
the critical section.

While spinlocks suffer from starvation, it is a uniform random
process and each thread has an equal chance of entering the critical
section.  Thus the probability of starvation is exponentially vanishing.
We shall call such a solution \emph{fair}
\index{fairness}%
(although it does not quite
match the usual formal nor vernacular concepts of fairness).

\glsadd{fairness}%

Unfortunately, such is not the case for the
reader/writer solution that we presented in~\autoref{ch:sbs}.
Consider this scenario: there are two readers and one writer.  One reader
is in the critical section while the writer is waiting.  Now the
second reader tries to enter and is able to.  The first reader leaves.
We are now in a similar situation as the initial state with one reader
in the critical section and the writer waiting, but it is not the same
reader.  Unfortunately for the writer, this scenario can repeat itself
indefinitely.  So even if neither reader was in the critical section
all of the time, and the second reader arrived well after the writer,
the writer never had a chance.

SBSs allow much control over which type of thread runs next and is therefore
a good starting point for developing fair synchronization algorithms.
\autoref{fig:RWfair} is based on \autoref{fig:RWsplitsema}, but there
are two important differences:

\begin{enumerate}
\item When a reader tries to enter the critical section, it yields not only
if there are writers in the critical section, but also if there are writers
waiting to enter the critical section;
\item Instead of a one-size-fits-all \texttt{release\_one} method, each
method has a custom way of selecting which gate to open.  In particular,
\texttt{read\_release} prefers the write gate, while \texttt{write\_release}
prefers the read gate.
\end{enumerate}

The net effect of this is that if there is contention between readers and
writers, then readers and writers end up alternating entering the critical
section.  While readers can still starve other readers and writers can still
starve other writers, readers can no longer starve writers nor vice versa.
Other fairness is based on the fairness of semaphores themselves.

\begin{comment}
\autoref{fig:RWqueue} and \autoref{fig:RWqtest} present a different approach
in which threads are allowed into the critical section in a purely First
Come First Served manner.  When a reader arrives, it is allowed in only if
there is no writer in the critical section or waiting to enter.  When a writer
arrives, it is allowed in only if there is no reader or writer in the
critical section (and therefore also not waiting to enter).  To maintain the
arrival order, the solution uses a semaphore per thread and a queue.
The \texttt{acquire\_} methods take a pointer to
the semaphore of the thread as argument.
Each entry in the queue is a pair consisting of a type of access
(\texttt{.read} or \texttt{.write}) and a pointer to the semaphore of the
thread that is waiting.  Method \texttt{release\_one} considers semaphores
to release in the order in which they appear in the queue.

\begin{figure}
\begin{code}
{\small
\harmonysource{RWqueue}
}
\end{code}
\caption{\harmonylink{code/RWqueue.hny} Reader/Writer Lock SBS implementation using a queue.}
\label{fig:RWqueue}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{RWqtest}
\end{code}
\caption{\harmonylink{code/RWqtest.hny} Test program for \autoref{fig:RWqueue}.}
\label{fig:RWqtest}
\end{figure}
\end{comment}

\section*{Exercises}
\begin{problems}
\item Write a fair solution to the one-lane bridge problem of
\autoref{ex:onelane}.
If you want to use the queue method, you can change the signature of the
methods to enter the bridge, adding an argument that contains a pointer to a
semaphore.
\end{problems}

\chapter{Monitors}
\label{ch:monitors}

\glsadd{monitor}%
\glsadd{condition variable}%

\begin{figure}
\begin{code}
\harmonysource{hoare}
\end{code}
\caption{\harmonylink{modules/hoare.hny} Implementation of Hoare monitors.}
\label{fig:hoare}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{BBhoare}
\end{code}
\caption{\harmonylink{code/BBhoare.hny} Bounded Buffer implemented using
a Hoare monitor.}
\label{fig:hoaretest}
\end{figure}

Tony Hoare, who came up with the concept of split binary semaphores, devised
an abstraction of the concept in a programming language paradigm called
\emph{monitors}~\cite{Hoare74}.
\index{monitor}%
(A similar construct was independently invented by Per Brinch Hansen~\cite{BH73}.)
A monitor is a special version of an object-oriented \emph{class}, comprising
a set of variables and methods that operate on those variables.
There is a split binary semaphore associated with each such class.
The \textit{mutex} is hidden: it is automatically acquired when invoking a
method and released upon exit.
The other semaphores are encapsulated in \emph{condition variables}.
\index{condition variable}%
Each condition variable also keeps track of the number of threads waiting
on the condition variable.

There are two operations on condition variables: \texttt{wait}
\index{wait}%
and
\texttt{signal}.
\index{signal}%
\autoref{fig:hoare} presents the semantics by giving an implementation of
\texttt{wait} and \texttt{signal} as well as the code that is used to enter
and exit monitor methods.
\texttt{wait} increments the condition's counter, releases the monitor mutex,
blocks while trying to acquire the condition's semaphore, and after resuming
decrements the counter---in much the same way as we have seen for split binary
semaphores (SBS).
\texttt{signal} checks to see if the condition's count is non-zero, and if so
releases the condition's semaphore,
and then blocks by trying to acquire the mutex again.

\autoref{fig:hoaretest} presents a bounded buffer implemented using
Hoare monitors.  It is written in much the same way you would if using
the SBS technique (see \autoref{ex:bbsbs}).
There is no \texttt{release\_one} method.  Instead, one can conclude
that \texttt{enqueue} causes the queue to be non-empty, and
\texttt{signal} will check if there are any threads waiting for
this event.  If so, \texttt{signal} will pass control to one such thread
and, unlike \texttt{release\_one}, re-enter the critical
section afterwards by acquiring the \textit{mutex}.

Implementing a reader/writer lock with Hoare monitors is not quite so
straightforward, unfortunately.  When a writer releases the lock, it
has to choose whether to signal a reader or another writer.  For that
it needs to know if there is a reader or writer waiting.  The simplest
solution would be to peek at the counters inside the respective condition
variable, but that breaks the abstraction.  The alternative is for the
reader/writer implementation to keep track of that state explicitly,
which complicates the code.  Also, it requires a deep understanding of
the SBS method to remember to place a call \texttt{signal} in the
\texttt{read\_acquire} method that releases additional readers that
may be waiting to acquire the lock.

In the late 70s, researchers at Xerox PARC,
where among others the desktop and Ethernet
were invented, developed a new programming language called
Mesa~\cite{LR80}.
\index{Mesa}%
Mesa introduced various important concepts to programming languages,
including software exceptions and incremental compilation.  Mesa also
incorporated a version of monitors.
However, there are some subtle but important differences with Hoare
monitors that make Mesa monitors quite unlike split binary semaphores
and mostly easier to use in practice.

As in Hoare monitors, there is a hidden mutex associated with each monitor,
and the mutex is automatically acquired upon entry to a method and released
upon exit.
Mesa monitors also have condition variables that a thread can wait on.
Like in Hoare monitors, the \texttt{wait} operation releases the mutex.
The most important difference is in what \texttt{signal} does.
To make the distinction more clear, we shall call the corresponding Mesa
operation \texttt{notify} rather than \texttt{signal}.
\index{notify}%
When a thread $p$ invokes \texttt{notify}, it does not immediately pass 
control to a thread that is waiting on the corresponding condition (if there
is such a thread).  Instead, $p$ remains in the critical section
until it leaves the monitor explicitly.  At that point, any thread that
was notified will have a chance to enter the critical section, but they compete
with other threads trying to enter the critical section.
Basically, there is just one gate to enter the critical section, instead
of a main gate and a gate per waiting condition.

\begin{comment}
\begin{figure}
\begin{center}
\includegraphics[width=6in]{figures/monitor-crop.pdf}
\end{center}
\caption{High-level depictions of Hoare and Mesa monitors.  The hall is
where threads wait to enter the bathroom (the critical section).  The
bedrooms illustrate two condition variables.  The circles are threads.}
\label{fig:monitors}
\end{figure}

\autoref{fig:monitors} illustrates the difference with a drawing.
Here a bathroom represents the critical section, allowing only one
thread at a time.  The bedrooms represent condition variables.
There is some condition associated with each bedroom.
In the hall are threads waiting to enter the critical section.

On the left is a Hoare monitor.
When some thread $p$ is in the bathroom, it can invoke \textbf{signal}
on some non-empty bedroom associated with a condition that holds.
This select a thread $q$ in that bedroom.
As a result of invoking \textbf{signal}, $p$ goes back into the hall
and $q$ enters the bathroom.\footnote{Per Brinch Hansen' monitors have
a \textbf{continue} operation instead of \textbf{signal}.  The difference
is that threads that invoke \textbf{continue} leave the monitor rather
than returning to the hall.}
Importantly, when $q$ enters the bathroom, the condition that $q$ was
waiting for is guaranteed to hold.

On the right is a Mesa monitor.  The only way into the bathroom is through
the hall.  When thread $p$ invokes \textbf{notify} on a bedroom, it
selects a thread $q$ in that bedroom, if any.
Thread $q$ goes into the hall,
joining any other threads that are also waiting to
enter the bathroom.
If there are no threads in that bedroom, \textbf{notify} is a no-op.
Importantly, in either case $p$ remains in the bathroom.
Finally, when $p$ leaves the bathroom, one of the
threads in the hall can enter the bathroom.  But that may not be $q$.
Assuming every thread eventually leaves the bathroom and the system is
fair, eventually $q$ will enter the bathroom.  However, it is no longer
guaranteed that the condition it was waiting for still holds.
Therefore, the first thing $q$ must
do is check, again, to see if the condition holds.  If not, it should go back
into the bedroom corresponding to the condition.
\end{comment}

This is a very important difference.  In Hoare monitors, when a thread
enters through a waiting gate, it can assume that the condition associated
with the waiting gate still holds because no other thread can run in between.
Not so with Mesa monitors: by the time a thread that was notified enters
through the main gate, other threads may have entered first and falsified
the condition.  So in Mesa, threads always have to check the condition
again after resuming from the \texttt{wait} operation.  This is accomplished
by wrapping each \texttt{wait} operation in a \textbf{while} statement that
loops until the condition of interest becomes valid.
A Mesa monitor therefore is more closely related to \emph{busy waiting}
than to split binary semaphores.

Mesa monitors provide another important and convenient option:
operation \texttt{notifyAll}
notifies \emph{all} threads that are waiting on a condition
instead of just one.
Having such an operation is not possible with Hoare monitors because using
Hoare monitors control
must be passed immediately to a thread that has been signaled, and that
can only be done if there is just one such thread.
\index{notifyAll}%
(aka \texttt{broadcast)})
\index{broadcast}%
lets all threads in that bedroom into the hall.

The so-called
``Mesa monitor semantics'' or ``Mesa condition variable'' semantics
have become more popular than Hoare monitor semantics, and have been
adopted by all major programming languages.
That said, few programming languages provide full support for monitors.
In Java, each object has a hidden lock \emph{and} a hidden condition variable
associated with it.
Methods declared with the \texttt{synchronized} keyword automatically
obtain the lock.  Java objects also support \texttt{wait}, \texttt{notify},
and \texttt{notifyAll}.
In addition, Java supports explicit allocations of locks
and condition variables.
In Python, locks and condition variables must be explicitly declared.
The \texttt{with} statement makes it easy to acquire and release a lock
for a section of code.
In C and C++, support for locks and condition variables is through libraries.

% \chapter{Mesa Condition Variables in Harmony}
% \label{ch:mesa}

\begin{figure}
\begin{code}
\harmonysource{mesa}
\end{code}
\caption{\harmonylink{modules/synch.hny}
Implementation of condition variables in the \texttt{synch} module.}
\label{fig:cv}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{RWcv}
\end{code}
\caption{\harmonylink{code/RWcv.hny} Reader/Writer Lock using Mesa-style condition variables.}
\label{fig:RWcv}
\end{figure}

Like C, Java, and Python, Harmony does not have built-in language
support for monitors, but provides library support through the
Harmony \texttt{synch} module.
\autoref{fig:cv} shows the implementation of
condition variables in the \texttt{synch} module.
\texttt{Condition}() creates a new condition variable.
It is represented by a dictionary containing
a bag of name tags of threads waiting on the condition variable.
(The \texttt{synchS} library instead uses a list of contexts.)

\texttt{wait} adds the nametag of the thread to the bag.
This increments the number of threads in the bag with the same context.
\texttt{wait} then waits until that count is restored to the
value that it had upon entry to \texttt{wait}.
\texttt{notify} removes an arbitrary context from
the bag, allowing one of the threads with that context to
resume and re-acquire the lock associated with the monitor.
\texttt{notifyAll} empties out the entire bag, allowing all threads
in the bag to resume.

To illustrate how Mesa condition variables are used in practice, we demonstrate using an implementation of reader/writer locks.
\autoref{fig:RWcv} shows the code.  \textit{mutex} is the shared lock
that protects the critical region.
(Again, there is only one ``gate.'')
There are two condition variables: readers wait on \textit{r\_cond} and
writers wait on \textit{w\_cond}.
The implementation also keeps track of the number of
readers and writers in the critical section.

Note that \texttt{wait} is always invoked within a \textbf{while}
loop that checks for the condition that the thread is waiting for.
It is \emph{imperative} that there is always a \textbf{while} loop
around any invocation of \texttt{wait} containing the negation of
the condition that the thread is waiting for.  Many implementation
of condition variables depend on this, and optimized implementations
of condition variables often allow so-called ``spurious wakeups,''
where \texttt{wait} may sometimes return even if the conditon
variable has not been notified.
In particular, one should always be able to replace \texttt{wait}
by \texttt{unlock} followed by \texttt{lock}.
This turns the solution into a busy-waiting one, inefficient but still
correct.

In \texttt{release\_rlock}, notice that \texttt{notify}(?\textit{w\_cond})
is invoked when there are no readers left, \emph{without} checking
if there are writers waiting to enter.  This is
ok, because calling \texttt{notify} is a no-op if no thread is
waiting.

\texttt{release\_wlock} executes \texttt{notifyAll}(?\textit{r\_cond})
as well as \texttt{notify}(?\textit{w\_cond)}.  Again, because we
do not keep track of the number of waiting readers or writers, we
have to conservatively assume that all waiting readers can enter,
or, alternatively, up to one waiting writer can enter.  So
\texttt{release\_wlock} wakes up all potential candidates.  There
are two things to note here.  First, unlike split binary semaphores
or Hoare monitors, where multiple waiting readers would have to be
signaled one at a time in a baton-passing fashion (see
\autoref{fig:RWsplitsema}), with Mesa monitors all readers are
awakened in one fell swoop using \texttt{notifyAll}.  Second, both
readers and writers are awakened---this is ok because both execute
\texttt{wait} within a \textbf{while} loop, re-checking the condition
that they are waiting for.  So if both type of threads are waiting,
either all the readers get to enter next or one of the writers gets
to enter next.

While Hoare moniters embody the split binary semaphore approach,
Mesa monitors are much closer to the busy-waiting approach.
The Mesa code then has to be careful to invoke \textbf{notify} or
\textbf{notifyAll} in the right places.
Much of the complexity of programming with Mesa condition variables
is in figuring out when to invoke \texttt{notify} and when to invoke
\texttt{notifyAll}.  As a rule of thumb: be conservative---it is
better to wake up too many threads than too few.  Waking up too
many threads may lead to some inefficiency, but waking up too few
may cause the application to get stuck.  Harmony can be particularly
helpful here, as it examines every corner case.  You can safely try
to replace each \texttt{notifyAll} with \texttt{notify} and see if
every possible execution of the the application still terminates.

Andrew Birrell's paper on Programming with Threads gives an excellent
introduction to working with Mesa-style condition variables~\cite{Birrell89}.

\section*{Exercises}
\begin{problems}
\item \label{ex:bbmesa} Implement a solution to the bounded buffer problem using Mesa condition
variables.
\item Implement a ``try lock'' module using Mesa condition variables
(see also Exercise~\ref{ex:trylock}).  It should
have the following API:
\begin{enumerate}
\item \textit{tl} = \texttt{TryLock}() \# \emph{create a try lock}
\item \texttt{acquire}(?\textit{tl}) \# \emph{acquire a try lock}
\item \texttt{tryAcquire}(?\textit{tl}) \# \emph{attempt to acquire a try lock}
\item \texttt{release}(?\textit{tl}) \# \emph{release a try lock}
\end{enumerate}
\noindent
\texttt{tryAcquire} should not wait.
Instead it should return \texttt{True} if the lock was successfully
acquired and \texttt{False} if the lock was not available.
% \item Implement semaphores using Mesa condition variables.
\item Write a new version of the GPU allocator in \autoref{ex:gpu}
using Mesa condition variables.
In this version,
a thread is allowed to allocate a set of GPUs and release a set of GPUs that it
has allocated.  Method \texttt{gpuAllocSet(n)} should block until $n$ GPUs are
available, but it should grant them as soon as they are available.
Method \texttt{gpuReleaseSet(s)} takes a set of GPU identifiers as argument.
A thread does not have to return all the GPUs it allocated at once.
\item The specification in the previous question makes the solution unfair.
Explain why this is so.  Then change the specification and the solution so that
it is fair.
\item \label{ex:qsort}
Bonus problem: \autoref{fig:qsort} shows an iterative implementation of the Qsort
algorithm, and \autoref{fig:qsorttest} an accompanying test program.
The array to be sorted is stored in shared variable \textit{textqs.arr}.
Another shared variable, \textit{testqs.todo}, contains the ranges of the
array that need to be sorted (initially the entire array).
Re-using as much of this code as you can, implement a parallel version of
this.  You should not have to change the methods \texttt{swap}, \texttt{partition},
or \texttt{sortrange} for this.  Create \texttt{NWORKERS} ``worker threads''
that should replace the \texttt{qsort} code.
Each worker loops until \textit{todo}
is empty and sorts the ranges that it finds until then.  The \texttt{main}
thread needs to wait until all workers are done.
\end{problems}

\begin{figure}
\begin{code}
\harmonysource{qsort}
\end{code}
\caption{\harmonylink{code/qsort.hny} Iterative qsort() implementation.}
\label{fig:qsort}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{qsorttest}
\end{code}
\caption{\harmonylink{code/qsorttest.hny} Test program for \autoref{fig:qsort}.}
\label{fig:qsorttest}
\end{figure}

\chapter{Deadlock}
\label{ch:deadlock}
\index{deadlock}%

\glsadd{deadlock}%

\begin{figure}
\begin{code}
\harmonysource{Diners}
\end{code}
\caption{\harmonylink{code/Diners.hny} Dining Philosophers.}
\label{fig:diners}
\end{figure}

When multiple threads are synchronizing access to shared resources, they
may end up in a \emph{deadlock} situation where one or more of the threads
end up being blocked indefinitely because each is waiting for another to give
up a resource.
The famous Dutch computer scientist Edsger W.~Dijkstra illustrated this using
a scenario he called ``Dining Philosophers.''
\index{dining philosopher}%

Imagine five philosopers sitting around a table, each with a plate of food in
front of them and a fork between each two plates.  Each philosopher requires
two forks to eat.  To start eating, a philosopher first picks up the fork on
the left, then the fork on the right.  Each philosopher likes to take breaks
from eating to think for a while.  To do so, the philosopher puts down both
forks.  Each philosopher repeats this procedure.  Dijkstra had them repeating
this for ever, but for the purposes of this book, philosophers can leave
the table when they're not eating.

\autoref{fig:diners} implements the dining philosophers in Harmony, using a
thread for each philosopher and a lock for each fork.  If you
run it, Harmony complains that the execution may not terminate, with all five
threads being blocked trying to acquire the lock.

\begin{quote}
\begin{itemize}
\item Do you see what the problem is?
\item Does it depend on \texttt{N}, the number of philosophers?
\item Does it matter in what order the philosophers lay down their forks?
\end{itemize}
\end{quote}

% \chapter{Deadlock Prevention}
% \label{ch:deadlockprevention}
% \index{deadlock prevention}

\begin{figure}
\begin{code}
\harmonysource{DinersCV}
\end{code}
\caption{\harmonylink{code/DinersCV.hny} Dining Philosophers that grab both forks at the same time.}
\label{fig:dinerscv}
\end{figure}

\noindent
There are four conditions that must hold for deadlock to occur~\cite{CES71}:
\begin{enumerate}
\item \emph{Mutual Exclusion}: each resource can only be used by one thread at a time:
\item \emph{Hold and Wait}: each thread holds resources it already allocated while it
waits for other resources that it needs;
\item \emph{No Preemption}: Resources cannot be forcibly taken away from threads that
allocated them;
\item \emph{Circular Wait}: There exists a directed circular chain of threads, each waiting
to allocate a resource held by the next.
\end{enumerate}

Preventing deadlock thus means preventing that one of these conditions occurs.
However, mutual exclusion is not easily prevented (although, for some resources it is
possible, as demonstrated in \autoref{ch:nonblocking}).
Havender proposed the following techniques that avoid the remaining
three conditions~\cite{Havender68}:

\begin{itemize}
\item \emph{No Hold and Wait}: a thread must request all resources it is going to
need at the same time;
\item \emph{Preemption}: if a thread is denied a request for a resource, it must
release all resources that it has already acquired and start over;
\item \emph{No Circular Wait}: define an ordering on all resources and allocate
resources in a particular order.
\end{itemize}

To implement a \emph{No Hold and Wait} solution, a philosopher would need a
way to lock both the left and right forks at the same time.  Locks do not
have such an ability, and neither do semaphores. so we re-implement the
Dining Philosophers using condition variables that allow one to wait for
arbitrary application-specific conditions.
%
\autoref{fig:dinerscv} demonstrates how this might be done.
We use a single mutex for the diners, and, for each fork, a boolean
and a condition variable.  The boolean indicates if the fork has been
taken.
Each diner waits if either the left or right fork is already taken.
But which condition variable to wait on?
The code demonstrates an important technique to use when waiting for
multiple conditions.
\index{multiple conditions, waiting on}%
The condition in the \textbf{while} statement is the negation of the
condition that the diner is waiting and consists of two disjuncts.
Within the \textbf{while} statement,
there is an \textbf{if} statement for each disjunct.
The code waits for either or both forks if necessary.  After that, it goes
back to the top of the \textbf{while} loop.

A common mistake is to write the following code instead:

\begin{code}
\harmonysource{baddblwait}
\end{code}

\begin{quote}
\begin{itemize}
\item Can you see why this does not work?  What can go wrong?
\item Run it through Harmony in case you are not sure!
\end{itemize}
\end{quote}

The \emph{Preemption} approach suggested by Havender is to allow threads to back out.
While this could be done, this invariably leads to a busy waiting solution
where a thread keeps obtaining locks and releasing them again until it
finally is able to get all of them.

The \emph{No Circular Waiting} approach
is to prevent a cycle from forming, with each
thread waiting for the next thread on the cycle.
We can do this by establishing an ordering among the
resources (in this case the forks) and, when needing more than one
resource, always acquiring them in order.  In the case of the philosopers,
they could prevent deadlock by always picking up the lower numbered fork
before the higher numbered fork, like so:

\vspace{1em}
\begin{code}
\harmonysource{dinersfix}
\end{code}
\vspace{1em}

This completes all the Havender methods.
There is, however, another approach, which is sometimes called deadlock
\emph{avoidance}
\index{deadlock avoidance}%
instead of deadlock \emph{prevention}.
In the case of the Dining Philosophers, we want to avoid the situation where each
diner picks up a fork.  So if we can prevent more than four diners from starting to
eat at the same time, then we can avoid the conditions for deadlock from ever
happening.
\autoref{fig:dinersavoid} demonstrates this concept.  It uses a
\emph{counting semaphore} to restrict the number of diners at any time to
four.  A counting semaphore is like a binary semaphore, but can be
acquired a given number of times.  It is supported by the \texttt{synch}
module.

This concept can be generalized using something called the
Banker's Algorithm~\cite{EWD108}, but it is outside the scope of this book.
The problem with these kinds of schemes is that one needs to know ahead of time
the set of threads and what the maximum number of resources is that each thread
wants to allocate, making them generally quite impractical.

\begin{figure}
\begin{code}
\harmonysource{DinersAvoid}
\end{code}
\caption{\harmonylink{code/DinersAvoid.hny} Dining Philosophers that carefully avoid getting into a deadlock
scenario.}
\label{fig:dinersavoid}
\end{figure}

\section*{Exercises}
\begin{problems}
\item \label{ex:bank} \autoref{fig:bank} shows an implementation of a bank with various
accounts and transfers between those accounts.
Unfortunately, running the test reveals that it sometimes leaves unterminated
threads.  Can you fix the problem?
\item Add a method \texttt{total()} to the solution of the previous question
that computes the total over all balances.
It needs to obtain a lock on all accounts.  Make sure that
it cannot cause deadlock.
\item Implement an extra thread that checks, once, if the total of
the balances is the same as that at the beginning (using an assertion).
Is once enough to determine that no money gets lost or created, or should it do
so continually?
\end{problems}

\begin{figure}
\begin{code}
\harmonysource{bank}
\end{code}
\caption{\harmonylink{code/bank.hny} Bank accounts.}
\label{fig:bank}
\end{figure}

\chapter{Actors and Message Passing}
\label{ch:actor}
\index{actor model}%
\index{message passing}%

\glsadd{actor model}%

\begin{figure}
\begin{center}
\includegraphics[width=4in]{figures/actor-crop.pdf}
\end{center}
\caption{Depiction of three actors.  The producer does not receive messages.}
\label{fig:actorpic}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{actor}
\end{code}
\caption{\harmonylink{code/actor.hny} A producer/consumer actor.}
\label{fig:actor}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{actortest}
\end{code}
\caption{\harmonylink{code/actortest.hny} Test code for producer/consumer actor.}
\label{fig:actortest}
\end{figure}

Some programming languages favor a different way of implementing
synchronization using so-called \emph{actors}~\cite{HBS73}.  Actors are
threads that have only private memory and communicate through
\emph{message passing}.
See \autoref{fig:actorpic} for an illustration.
Given that there is no shared memory in the actor model (other than the message
queues, which have built-in synchronization), there is no need
for critical sections.  Instead, some sequential thread owns a particular
piece of data and other threads access it by sending request messages
to the thread and optionally waiting for response messages.  Each thread
handles one message at a time, serializing all access to the data it owns.
As message queues are FCFS (First-Come-First-Served), starvation is prevented.

The actor synchronization model is popular in a variety of programming
languages, including Erlang and Scala.  Actor support is also available
through popular libraries such as Akka, which is available for various
programming languages.  In Python, Java, and C/C++,
actors can be easily emulated using threads and \emph{synchronized queues}
\index{synchronized queue}%
(aka \emph{blocking queues})
\index{blocking queue}%
for messaging.
Each thread would have one such queue for receiving messages.
Dequeuing from an empty synchronized queue blocks the thread until
another thread enqueues a message on the queue.

The \texttt{synch} library supports a synchronized message queue,
similar to the \texttt{Queue} object in Python.
Its interface is as follows:
\begin{itemize}
\item \texttt{Queue()} returns a new message queue;
\item \texttt{enqueue}($q$, \textit{item}) adds \textit{item} to the queue pointed to by $q$;
\item \texttt{dequeue}($q$) waits for and returns an item on the queue pointed to by $q$.
\end{itemize}

Note that a \texttt{Queue} behaves much like a zero-initialized semaphore.
\texttt{enqueue} is much like \texttt{V}, except that it is accompanied by data.
\texttt{dequeue} is much like \texttt{P}, except that it also returns data.
Thus, synchronized queues can be considered a generalization of counting semaphores.

\autoref{fig:actor} shows a Harmony solution to the
``multiple producer / multiple consumer problem'' (with an unbounded buffer)
using an actor (as illustrated in \autoref{fig:actorpic}).
\texttt{pc\_actor} is essentially a matchmaker: it matches
\texttt{.produce} requests with \texttt{.consume} requests.
Sometimes it may have buffered \texttt{.consume} requests and is waiting for
\texttt{.produce} requests, and sometimes it is vice versa.
The variable \textit{balance} specifies what the situation is: if it is positive
then it has buffered \texttt{.produce} requests;
if it is negative
then it has buffered \texttt{.consume} requests.
Note that both \textit{balance} and \textit{requests} are variables that are
local to the \texttt{pc\_actor} thread.

Each request message is a dictionary with two fields.  One of the fields is
\texttt{.type}, which either is \texttt{.produce} or \texttt{.consume}.
In case of \texttt{.produce}, the second field is \texttt{.item} and holds
the data that is produced.
In case of \texttt{.consume}, the second field is \texttt{.queue} and holds
a pointer to the queue where the response message must be enqueued.
\texttt{.produce} messages do not get a response.

The \texttt{pc\_actor} thread has two arguments.  $q$ is a pointer
to the queue on which it receives messages.  \textit{nrequests} is the total
number of requests that it will handle.  We need the latter because in Harmony
all threads are required to terminate.  In a more realistic implementation,
the \texttt{pc\_actor} would be in an infinite loop awaiting requests.

The shared memory consists entirely of message queues.
The \texttt{produce} method takes two arguments: a pointer to the queue
of the \texttt{pc\_actor} and the item.  The \texttt{consume} method
takes two queue pointer arguments.  The first is a pointer to the queue
of the \texttt{pc\_actor}.  The second is a pointer to the queue of the
invoker: it is the queue to which the response must be posted.

\autoref{fig:actortest} shows how this code may be used.  It spawns
\texttt{NITEMS} producer and consumer threads.  Note that in that
case the \texttt{pc\_actor} is expected to receive \texttt{2 * NITEMS}
requests.

\section*{Exercises}
\begin{problems}
\item \label{ex:cltsvr} A popular model is the \emph{client/server} model.
Here a single actor implements some service, like computing the square of a
number.
A client can send a message containing an integer to the server, and the
server returns a response message containing the square of that integer.
Implement this.
\item Actors and message queues are good for building pipelines.
Develop a pipeline that computes Mersenne primes (primes that are one less
than a power of two).  Write four actors:
\begin{enumerate}
\item an actor that generates a sequence of integers 1 through \texttt{N};
\item an actor that receives integers and forwards only those that are prime;
\item an actor that receives integers and forwards only those that are one
less than a power of two;
\item an actor that receives integers but otherwise ignores them.
\end{enumerate}
Configure two versions of the pipeline, one that first checks if a number
is prime and then if it is one less than a power of two, the other
in the opposite order.  Which do you think is better?
\end{problems}

\chapter{Interrupts}
\label{ch:interrupts}
\index{interrupt}

\begin{figure}
\begin{code}
\harmonysource{trap}
\end{code}
\caption{\harmonylink{code/trap.hny} How to use \textbf{trap}.}
\label{fig:trap}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{trap2}
\end{code}
\caption{\harmonylink{code/trap2.hny} A race condition with interrupts.}
\label{fig:trap2}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{trap3}
\end{code}
\caption{\harmonylink{code/trap3.hny} Locks do not work with interrupts.}
\label{fig:trap3}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{trap4}
\end{code}
\caption{\harmonylink{code/trap4.hny} Disabling and enabling interrupts.}
\label{fig:trap4}
\end{figure}

Threads can be \emph{interrupted}.  An interrupt is a notification of some event
such as a keystroke, a timer expiring, the reception of a network packet, the completion
of a disk operation, and so on.  We distinguish \emph{interrupts} and \emph{exceptions}.
An exception is caused by the thread executing an invalid machine instruction such as
divide-by-zero.  An interrupt is caused by some peripheral device and can be handled in
Harmony.
In other words: an interrupt is a notification, while an exception is an error.
\index{exception}

Harmony allows modeling interrupts using the \texttt{trap} statement:
\begin{code}
\textbf{trap} \textit{handler} \textit{argument}
\end{code}
invokes \textit{handler argument} at some later, unspecified time.
Only one of these asynchronous events can be outstanding at a time; a new call to
\textbf{trap} overwrites any outstanding one.
\autoref{fig:trap} gives an example of how \textbf{trap} might be used.
Here, the \textit{main}() thread loops until the interrupt has occurred and
the \textit{done} flag has been set.

But now consider \autoref{fig:trap2}.  The difference with \autoref{fig:trap} is
that both the \textit{main}() and \textit{handler}() methods increment \textit{count}.
This is not unlike the example we gave in \autoref{fig:inc}, except that only a single
thread is involved now.  And, indeed, it suffers from a similar race condition; run
it through Harmony to see for yourself.  If the interrupt occurs after \textit{main}()
reads \textit{count} (and thus still has value 0) but before \textit{main()} writes the
updated value~1, then the interrupt handler will also read value~0 and write value~1.
We say that the code in \autoref{fig:trap2} is not \textit{interrupt-safe} (as opposed
to not being \textit{thread-safe}).
\index{interrupt-safety}

You would be excused if you wanted to solve the problem using locks, similar to
\autoref{fig:incfixed}.  \autoref{fig:trap3} shows how one might go about this.
But locks are intended to solve synchronization issues between multiple threads.
If you run the code through Harmony, you will find that the code may not terminate.
The issue is that a thread can only acquire a lock once.  If the interrupt happens
after \textit{main}() acquires the lock but before \textit{main}() releases it, the
\textit{handler}() method will block trying to acquire the lock, even though
it is being acquired by the same thread that already holds the lock.

Instead, the way one fixes interrupt-safety issues is through disabling interrupts
temporarily.  In Harmony, this can be done by setting the \textit{interrupt level}
of a thread to \texttt{True} using the \textbf{setintlevel} interface.
\autoref{fig:trap4} illustrates how this is done.
Note that it is not necessary to change the interrupt level during servicing an
interrupt, because it is automatically set to \texttt{True} upon entry to the interrupt
handler and restored to \texttt{False} upon exit.
It is important that the \textit{main}() code re-enables interrupts after incrementing
\textit{count}.  What would happen if \textit{main}() left interrupts disabled?

\textbf{setintlevel}(\textit{il}) sets the interrupt level to \textit{il} and returns
the prior interrupt level.  Returning the old level is handy when writing interrupt-safe
methods that can be called from ordinary code as well as from an interrupt handler.
\autoref{fig:trap5} shows how one might write a interrupt-safe method
to increment the counter.

\begin{figure}
\begin{code}
\harmonysource{trap5}
\end{code}
\caption{\harmonylink{code/trap5.hny} Example of an interrupt-safe method.}
\label{fig:trap5}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{trap6}
\end{code}
\caption{\harmonylink{code/trap6.hny} Code that is both interrupt-safe and thread-safe.}
\label{fig:trap6}
\end{figure}

It will often be necessary to write code that is both interrupt-safe \emph{and}
thread-safe.  As you might expect, this involves both managing locks and
interrupt levels.
To increment \textit{count}, the interrupt level must be \textit{True} and
\textit{countlock} must be held.
\autoref{fig:trap6} gives an example of how this might be done.
One important rule to remember is that a thread should disable interrupts \emph{before}
attempting to acquire a lock.

Try moving \texttt{lock}() to the beginning of the \texttt{increment} method
and \texttt{unlock}() to the end of \texttt{increment} and see what happens.
While Harmony will only report one faulty run, this incorrect code can lead to
the assertion failing as well as threads getting blocked indefinitely.

(Another option is to use synchronization techniques that do not use locks.
See \autoref{ch:nonblocking} for more information.)

There is another important rule to keep in mind.  Just like locks should never be held
for long, interrupts should never be disabled for long.  With locks the issue is to
maximize concurrent performance.  For interrupts the issue is fast response to
asynchronous events.  Because interrupts may be disabled only briefly, interrupt
handlers must run quickly and cannot wait for other events.  So it is ok to invoke
synchronization calls such as \texttt{V} and \texttt{notify}, but calls such as
\texttt{P} and \texttt{wait} should only be used if it is certain that they will not
block for long.  Informally, interrupt handlers can be
\emph{producers} but not \emph{consumers} of synchronization events.

\section*{Exercises}
\begin{problems}
\item The \texttt{enqueue} method you implemented in \autoref{ex:bbmesa} cannot be used
in interrupt handlers for two reasons: (1) it is not interrupt-safe, and (2)
it may block for a long time if the buffer is full.  Yet, it would be useful if,
say, a keyboard interrupt handler could place an event on a shared queue.
Implement a new method \texttt{i\_enqueue}(\textit{item}) that does not
block.  Instead, it should return \texttt{False} if the buffer is full and \texttt{True}
if the item was successfully enqueued.
The method also needs to be interrupt-safe.
\end{problems}

\chapter{Alternating Bit Protocol}
\label{ch:abp}
\index{alternating bit protocol}%
\index{protocol}%
\index{distributed system}%

\begin{figure}
\begin{code}
\harmonysource{abp}
\end{code}
\caption{\harmonylink{code/abp.hny} Alternating Bit Protocol.}
\label{fig:abp}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{abptest}
\end{code}
\caption{\harmonylink{code/abptest.hny} Test code for alternating bit protocol.}
\label{fig:abptest}
\end{figure}

A
\emph{distributed system}
\index{distributed system}%
is a concurrent system in which a collection
of threads communicate by message passing, much the same as
in the actor model.
The most important difference between distributed and concurrent
systems is that the former takes \emph{failures}
\index{failure}%
into account,
including failures of threads and failures of shared memory.
In this chapter, we will consider two actors, Alice and Bob.
Alice wants to send a sequence of application messages to Bob,
but the underlying network may lose messages.
\index{network}%
The network does not re-order messages: when sending messages
$m_1$ and $m_2$ in that order, then if both messages are received,
$m_1$ is received before $m_2$.
Also, the network does not create messages out of nothing: if
message $m$ is received, then message $m$ was sent.

It is useful to create an abstract network that reliably sends messages
between threads, much like the FIFO queue in the \texttt{synch} module.
For this, we need a network protocol that Alice and Bob can run.
In particular, it has to be the case that if Alice sends application
messages $m_1, ..., m_n$ in that order, then if Bob receives an
application message
$m$, then $m = m_i$ for some $i$ and Bob will already have received
application messages $m_1, ..., m_i$ (safety).
Also, if the network is fair and Alice sends application message
$m$, then eventually Bob should deliver $m$ (liveness).

The \emph{Alternating Bit Protocol} is suitable for our purposes.
We assume that there are two unreliable network channels: one from Alice
to Bob and one from Bob to Alice.
Alice and Bob each maintain a zero-initialized
\emph{sequence number},
\index{sequence number}%
\textit{s\_seq} and \textit{r\_seq} resp.
Alice sends a network message to Bob containing an
application message as \emph{payload} and Alice's sequence number
as \emph{header}.
When Bob receives such a network message, Bob returns an
\emph{acknowledgment}
\index{acknowledgment}%
to Alice, which is a network message
containing the same sequence number as in the message that Bob received.

In the protocol, Alice keeps sending the same network message until she
receives an acknowledgment with the same sequence number.
This is called \emph{retransmission}.
\index{retransmission}%
When she receives the desired sequence number, Alice increments her sequence number.
She is now ready to send the next message she wants to send to Bob.
Bob, on the other hand, waits until he receives a message matching
Bob's sequence number.  If so, Bob \emph{delivers} the payload in the
message and increments his sequence number.
Because of the network properties,  a one-bit sequence number suffices.

We can model each
channel as a variable that either contains a network message or nothing
(we use \texttt{()} in the model).  Let \textit{s\_chan} be the channel
from Alice to Bob and \textit{r\_chan} the channel from Bob to Alice.
\texttt{net\_send}(\textit{pchan}, $m$, \textit{reliable}) models sending
a message $m$ to
\texttt{!}\textit{pchan}, where \textit{pchan} is either ?\textit{s\_chan}
or ?\textit{r\_chan}.
The method places either $m$ (to model a successful send)
or \texttt{()} (to model loss) in \texttt{!}\textit{pchan}.
The use of the \textit{reliable} flag will be explained later.
\texttt{net\_recv}(\textit{pchan}) models checking \texttt{!pchan} for
the next message.

Method \texttt{app\_send}($m$) retransmits $m$ until
an acknowledgment is received.
Method \texttt{app\_recv}(\textit{reliable}) returns the next successfully received
message.
\autoref{fig:abptest} shows how the methods may be used to send and receive
a stream of \texttt{NMSGS} messages reliably.
It has to be bounded, because model checking requires a finite model.

Only the last invocation of \texttt{app\_recv}(\textit{reliable}) is invoked with
\textit{reliable} == \texttt{True}.  It causes the last acknowledgment to be sent
reliably.  It allows the receiver (Bob) to stop, as well as the sender (Alice)
once the last acknowledgment has been received.
Without something like this, either the sender may be left hanging waiting
for the last acknowledgment, or the receiver waiting for the last message.

\section*{Exercises}
\begin{problems}
\item \autoref{ex:cltsvr} explored the \emph{client/server model}.  It is popular
in distributed systems as well.
Develop a protocol for a single client and server using the same network
model as for the ABP protocol.
Hint: the response to a request can contain the same sequence number as the
request.
\item Generalize the solution in the previous exercise to multiple clients.
Each client is uniquely identified.  You may either use separate channel pairs
for each client, or solve the problem using a single pair of channels.
\end{problems}

\chapter{Non-Blocking Synchronization}
\label{ch:nonblocking}
\index{non-blocking synchronization}%

\glsadd{non-blocking synchronization}%

\begin{figure}
\begin{code}
\harmonysource{hw}
\end{code}
\caption{\harmonylink{code/hw.hny} Non-blocking queue.}
\label{fig:hw}
\end{figure}

So far we have concentrated on critical sections to synchronize multiple
threads.  Certainly, preventing multiple threads from accessing
certain code at the same time simplifies how to think about synchronization.
However, it can lead to starvation.  Even in the absence of starvation,
if some thread is slow for some reason while being in the critical section,
the other threads have to wait for it to finish executing the critical section.
Also, using synchronization primitives in interrupt handlers is tricky
to get right (\autoref{ch:interrupts}) and might run too long.
In this chapter, we will have a look at how one can develop concurrent
code in which threads do not have to wait for other threads to complete their
ongoing operations.

As a first example, we will revisit the producer/consumer problem.
The code in \autoref{fig:hw} is based on code developed by Herlihy and
Wing~\cite{HW87}.
The code is a ``proof of existence'' for non-blocking synchronization; it
is not necessarily practical.
There are two variables.  \textit{items} is an unbounded array with each
entry initialized to \texttt{()}.  \textit{back} is an index into the
array and points to the next slot where a new value is inserted.
The code uses two interlock instructions:
\begin{itemize}
\item \texttt{inc}($p$): atomically increments \texttt{!}$p$ and returns
the old value;
\item \texttt{exch}($p$): sets \texttt{!}$p$ to \texttt{()} and returns
the old value.
\end{itemize}

Method \texttt{produce}(\textit{item}) uses \texttt{inc}(?\textit{back})
to allocate
the next available slot in the \textit{items} array.
It stores the item as a singleton tuple.
Method \texttt{consume()} repeatedly scans the array, up to the
\textit{back} index, trying to find an item to return.
To check an entry, it uses \texttt{exch()}
to atomically remove an item from a slot if there is one.
This way, if two or more threads attempt to extract an item from
the same slot, at most one will succeed.

There is no critical section.  If one thread is executing instructions
very slowly, this does not negatively impact the other threads, as it
would with solutions based on critical sections.
On the contrary, it helps them because it creates less contention.
Unfortunately, the solution is not practical for the following reasons:
\begin{itemize}
\item The \textit{items} array must be of infinite size if an unbounded number
of items may be produced;
\item Each slot in the array is only used once, which is inefficient;
\item the \texttt{inc} and \texttt{exch} interlock instructions are not
universally available on existing processors.
\end{itemize}
However, in the literature there are many examples of practical
non-blocking (aka \emph{wait-free})
\index{wait-free synchronization}%
synchronization algorithms.
% TODO~\cite{}.

There are also various algorithms where read-only access is wait-free
but updates do require a lock---these are useful in situations where
most accesses are read-only and the performance of updates is less
critical. %~\cite{}.
\autoref{fig:lst1} and \autoref{fig:lst2} implements an ordered linked
list of integers without duplicates.
There are two update operations:
values can be added using \texttt{lst\_insert} or deleted using
\texttt{lst\_remove}.
There is also a read-only operation: \texttt{lst\_contains} checks if
a particular value is in the list.
\glsadd{linearizable}%

The read-only operation \texttt{lst\_contains} is wait\_free: it scans
through the list without having to obtain a lock.
Nonetheless, the implementation of the list is
\emph{linearizable}~\cite{HW90},
\index{linearizable}%
a strong notion of consistency that
makes it appear as if each of the operations executes atomically at
some point between their invocation and return.

The list has two ``book-end'' nodes with values \texttt{-inf} and
\texttt{inf} (similar to the Python \texttt{math.inf} constant).
Each node has a lock, a value, and \textit{next}, a pointer to the next node
(which is \texttt{None} for the final \texttt{inf} node).
The \texttt{lst\_find}(\textit{lst}, $v$) method is a helper function for update
methods that
finds and locks two consecutive nodes \textit{before} and \textit{after}
such that $\mathit{before}.\mathtt{data.value} < v \le \mathit{after}.\mathtt{data.value}$.
It does so by performing something called \emph{hand-over-hand locking}.
\index{hand-over-hand locking}%
It first locks the first node, which is the \texttt{-inf} node.
Then, iteratively, it obtains a lock on the next node and release the
lock on the last one, and so on, similar to climbing a tree
hand-over-hand.

Having the two adjoining nodes locked, implementing \texttt{lst\_insert}
and \texttt{lst\_remove} is straightforward.
The \texttt{lst\_contains} method can simply scan through the list
because the \textit{next} pointers are updated atomically in such a
way that they always point to a legal suffix of the list.
An invariant of the algorithm is that at any point in time the
list is ``valid,'' starting with a \texttt{-inf} node and ending
with a \texttt{inf} node.

Determining if an implementation of a concurrent data structure
is linearizable involves finding what are known as the
\emph{linearization points}
\index{linearization point}%
of the operations in an execution.  These are the unique
points in time at which an operation appears to execute atomically.
The linearization points for the \texttt{lst\_insert} and
\texttt{lst\_remove} operations coincide exactly with the update
of the \textit{before}.\texttt{next} pointer.
The linearization point of a \texttt{lst\_contains} method
execution depends on whether the value is found or not.
If found, it coincides with retrieving the pointer to the node
that has the value.
If not found, it coincides with retrieving the pointer to the
\texttt{inf} node.

\begin{figure}
\begin{code}
\harmonysource{lst1}
\end{code}
\caption{\harmonylink{code/lst1.hny} List with non-blocking read operation, part 1.}
\label{fig:lst1}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{lst2}
\end{code}
\caption{\harmonylink{code/lst2.hny} List with non-blocking read operation, part 2.}
\label{fig:lst2}
\end{figure}

\section*{Exercises}
\begin{problems}
\item Add read-only methods to the data structure in \autoref{fig:lst1}
and \autoref{fig:lst2} that report the size of the list, the minimum value in the
list, the maximum value in the list, and the sum of the values in the list.
Are they linearizable?  If so, what are their linearization points?
\item A \emph{seqlock}
\index{seqlock}%
consists of a lock and a version number.
An update operation acquires the lock, increments the version number, makes the
changes to the data structure, and then releases the lock.  A read-only operation
does not use the lock.  Instead, it retrieves the version number,
reads the data structure, and then checks if the
version number has changed.  If so, the read-only operation is retried.
Use a seqlock to implement a bank much like \autoref{ex:bank}, with
one seqlock for the entire bank (i.e., no locks on individual accounts).
Method \texttt{transfer} is an update operation; method \texttt{total} is a
read-only operation.  Explain how a seqlock can lead to starvation.
\end{problems}

\chapter{Barrier Synchronization}
\label{ch:barrier}
\index{barrier synchronization}%

\glsadd{barrier synchronization}%

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{figures/barrier-crop.pdf}
\end{center}
\caption{High-level state diagram specification of barrier synchronization
with three threads and two rounds.  The three numbers specify how many
rounds each thread has completed.}
\label{fig:barrierdiagram}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{barrier}
\end{code}
\caption{\harmonylink{code/barrier.hny} Barrier synchronization with queues.}
\label{fig:barrier}
\end{figure}

Barrier synchronization is a problem that comes up in high-performance
parallel computing, used, among others, for scalable simulation.
A barrier is almost the opposite of a critical section:
the intention is to get a group of threads to run some code at the
same time, instead of having them execute it one at a time.
More precisely, with barrier synchronization the threads execute in rounds.
Between each round there is a so-called \emph{barrier} where threads wait
until all threads have completed the previous round, before they start the
next one.
For example, in an iterative matrix algorithm, the matrix may be
cut up into fragments.  During a round, the threads run concurrently,
one for each fragment.  The next round is not allowed to start
until all threads have completed processing their fragment.

\autoref{fig:barrierdiagram} shows a high-level depiction of barrier synchronization
with three and threads.  Initially all threads are in round~0.  Then each thread
can start and finish the round.  However, none of the threads can
progress to the next round until all threads have finished the
current round.

Blocking queues work well for implementing barrier synchronization.
\autoref{fig:barrier} shows an example.  There is a queue
for each of the \texttt{N} threads.
Before thread $i$ enters a round, it first sends enqueues a message to every
other thread and then waits until it receives a message from
every other thread.
In this simple case, each message contains \texttt{None}, but in practice useful
information may be exchanged between the threads.

The \textit{round} array is kept to check the correctness of this
approach.  Each thread increments its entry every time it enters
a round.  If the algorithm is correct, it can never be that two threads
are more than one round apart.

\section*{Exercises}
\begin{problems}
\item See if you can implement barrier synchronization for \texttt{N} threads
with just three binary semaphores.  Busy waiting is not allowed.  How about just two?
(As always, the Little Book of Semaphores~\cite{Downey09} is a good resource
for solving synchronization problems with semaphores.
Look for the \emph{double turnstile}
\index{double turnstile}%
solution.)
\item Implement barrier synchronization with Mesa condition variables.
(You may want to use a double turnstile approach here as well.)
\end{problems}


\bibliographystyle{alpha}
\bibliography{paper}

\appendix

\chapter{List of Values}
\label{ap:values}

\autoref{ch:harmonymachine} provides an introduction to Harmony values.
Below is a complete list of Harmony value types with examples:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
Boolean & \texttt{False}, \texttt{True} \\
\hline
Integer & \texttt{-inf, ..., -2, -1, 0, 1, 2, ..., inf} \\
\hline
Atom & \texttt{.example}, \texttt{.test1}, \texttt{.0x4A} \\
\hline
Program Counter & (method names are program counter constants) \\
\hline
Dictionary & \texttt{dict\{ .account: 12345, .valid: False \} } \\
\hline
Set & \texttt{\{ 1, 2, 3 \}, \{ False, .id, 3 \} } \\
\hline
Address & \texttt{?lock, ?flags[2], None} \\
\hline
Context & (generated by \textbf{stop} expression) \\
\hline
\end{tabular}
\vspace{1em}

Tuples, lists, strings, and bags are all special cases of dictionaries.
Both tuples and lists map indexes (starting at 0) to Harmony values.
Their format is either \texttt{($e$, $e$, ..., $e$,)} or
\texttt{[$e$, $e$, ..., $e$,]}.
If the tuple or list has two or more elements, then the final comma
is optional.
A string is represented as a tuple of its characters.
Characters are one-character atoms, which can be expressed
in hexadecimal unicode using the syntax \texttt{.0xXX}.
A bag or multiset is a dictionary that maps a value to how many
times it occurs in the bag.

All Harmony values are ordered with respect to one another.  First they
are ordered by type according to the table above.
So, for example, \texttt{\texttt{True} < 0 < .xyz < \{ 0 \})}.
Within types, the following rules apply:

\begin{itemize}
\item \texttt{False < True};
\item integers are ordered in the natural way;
\item atoms are lexicographically ordered;
\item program counters are ordered by their integer values;
\item dictionaries are first converted into a list of ordered (key, value)
pairs.  Then two dictionaries are lexicographically ordered by this
representation;
\item a set is first converted into an ordered list, then lexicographically
ordered;
\item an address is a list of atoms.  \texttt{None} is the empty list of atoms.
Addresses are lexicographically ordered accordingly;
\item contexts (Appendix~\ref{app:context}) are ordered first by name tag, then by program counter, then by hash.
\end{itemize}

\chapter{List of Operators}

Harmony currently supports the following operators:
\vspace{1em}

{\small
\begin{tabular}{|l|l|}
\hline
\texttt{$e$ == $e$}, \texttt{$e$ != $e$} & two Harmony values of the same type \\
\hline
\texttt{$e$ < $e$, $e$ <= $e$, $e$ > $e$, $e$ >= $e$} & any two Harmony values \\
\hline
\texttt{$e$ and $e$ and ...} & two or more booleans \\
\hline
\texttt{$e$ or $e$ or ...} & two or more booleans \\
\hline
\texttt{not $e$} & a boolean \\
\hline
\texttt{-$e$, \string~$e$} & an integer \\
\hline
\texttt{$e$ + $e$ + ...} & two or more integers, tuples, or lists \\
\hline
\texttt{$e$ - $e$} & two integers or sets \\
\hline
\texttt{$e$ \& $e$ \& ...} & two or more integers or sets \\
\hline
\texttt{$e$ | $e$ | ...} & two or more integers or sets \\
\hline
\texttt{$e$ \string^ $e$ \string^ ...} & two or more integers or sets \\
\hline
\texttt{$e$ * $e$} & two integers or an integer and a list \\
\hline
\texttt{$e$ / $e$, $e$ // $e$} & two integers (integer division) \\
\hline
\texttt{$e$ \% $e$, $e$ mod $e$} & two integers (division remainder) \\
\hline
\texttt{$e$ ** $e$, $e << e$, $e >> e$} & two integers \\
\hline
\texttt{$\{ e..e \}$} & two integers \\
\hline
\texttt{$e$ [not] in $e$} & first is any Harmony value, second is a set or dict \\
\hline
\texttt{$e$ if $e$ else $e$} & middle $e$ must be a boolean \\
\hline
\texttt{!$e$} & an address \\
\hline
\texttt{choose $e$} & a set \\
\hline
\texttt{min $e$, max $e$, any $e$, all $e$, len $e$} & a set or a dict \\
\hline
\texttt{keys $e$} & a dictionary (includes tuple and list) \\
\hline
\texttt{bagsize $e$} & a bag \\
\hline
\texttt{atLabel $e$} & atom (corresponding to a label) \\
\hline
\texttt{nametag $e$} & $e$ must be \texttt{()} \\
\hline
\texttt{threads $e$} & $e$ must be \texttt{()} \\
\hline
\texttt{setintlevel} \textit{e} & \textit{e} is a boolean \\
\hline
\texttt{stop} \textit{lv} & \textit{lv} is an lvalue \\
\hline
\texttt{?}\textit{lv} & \textit{lv} is an lvalue \\
\hline
\texttt{lambda $a$: $e$ end} & $a$ is a bounded variable, $e$ an expression\\
\hline
\end{tabular}
}
\vspace{1em}

\texttt{+} concatenates when applied to lists or tuples.
\texttt{-} computes set difference when applied to two sets.
\texttt{\&} computes the intersection when applied to sets.
\texttt{|} computes the union when applied to sets.
\texttt{\^} computes $(s \cup t) \backslash (s \cap t)$ when applied to sets $s$ and $t$.
\texttt{$\{ x .. y \}$} computes the set consisting of the integers $x$
through $y$ inclusive.  It returns the empty set if $x > y$.
If $d$ is a dictionary, $e$ \texttt{in} $d$ tests if $e$ is in the set of
\emph{values} of $d$, not the set of \emph{keys} (this is different from Python).

An \emph{lvalue}
\index{lvalue}%
(short for left hand value of an assignment statement)
is something that can be assigned.  This can be a
shared variable, a thread variable, or a dereferenced pointer
variable.  It can also be indexed.  Examples include
\textit{var}, \textit{var}[$e$], and \texttt{!}\textit{p}.

Harmony also supports the following comprehensions:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
Set comprehension & \texttt{\{ f($v$) for $v$ in $s$ \}} \\
\hline
List comprehension & \texttt{[ f($v$) for $v$ in $s$ ]} \\
\hline
Dict comprehension & \texttt{dict\{ f($v$) for $v$ in $s$ \}} \\
\hline
\end{tabular}
\vspace{1em}

In each case, $s$ must be a set or a dictionary.
\autoref{ap:details} provides more information on comprehensions.

\chapter{List of Statements}

Harmony currently supports the following statements:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
\textit{lv} = [lv =]... $e$ & \textit{lv} is an lvalue and $e$ is an expression\\
\hline
\textit{lv} \texttt{[op]=} $e$ & \texttt{op} is one of \texttt{+}, \texttt{-},
\texttt{*}, \texttt{/}, \texttt{//}, \texttt{\%},
\texttt{\string\&}, \texttt{|}, \texttt{\string\^},
\texttt{and}, and \texttt{or}\\
\hline
\texttt{pass} & do nothing\\
\hline
\texttt{del} \textit{lv} & delete\\
\hline
\texttt{assert $b$ [, $e$]} & $b$ is a boolean.  Optionally report value of expression $e$\\
\hline
\texttt{const a = $e$} & \texttt{a} is a bounded variable, $e$ is a constant expression\\
\hline
\texttt{def m $a$: S} & \texttt{m} is an identifier, $a$ a bounded variable, \texttt{S} a list of statements\\
\hline
\texttt{let $a$ = $e$ [let ...]: S} & $a$ is a bounded variable, $e$ is an expression, \texttt{S} a list of statements\\
\hline
\texttt{if $b$: S else: S} & $b$ is a boolean, \texttt{S} a list of statements\\
\hline
\texttt{while $b$: S} & $b$ is a boolean, \texttt{S} a list of statements\\
\hline
\texttt{await $b$} & $b$ is a boolean\\
\hline
\texttt{for $a$ in $e$: S} & $a$ is a bounded variable, $e$ is a set,
                            \texttt{S} a list of statements\\
\hline
\texttt{atomic: S} & \texttt{S} a list of statements\\
\hline
\texttt{spawn m $e$ [, $t$]} & \texttt{m} is a method,
$e$ is an expression, $t$ is a tag (an expression) \\
\hline
\texttt{trap m $e$} & \texttt{m} is a method and $e$ is an expression \\
\hline
\texttt{go $c$ $e$} & $c$ is a context, $e$ is an expression \\
\hline
\texttt{import m, ...} & \texttt{m} identifies a module \\
\hline
\texttt{from m import ...} & \texttt{m} identifies a module \\
\hline
\end{tabular}

\chapter{List of Modules}
\label{ap:module}

\section{The \texttt{alloc} module}

The \texttt{alloc} module
\index{alloc module}%
supports thread-safe (but not interrupt-safe) dynamic allocation of
shared memory locations.  There are just two methods:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
\texttt{malloc($v$)} & return a pointer to a memory location initialized to $v$ \\
\hline
\texttt{free($p$)} & free an allocated memory location $p$ \\
\hline
\end{tabular}
\vspace{1em}

The usage is similar to \texttt{malloc} and \texttt{free} in C.
\texttt{malloc}() is specified to return \texttt{None} when running out of
memory, although this is an impossible outcome in the current
implementation of the module.

\section{The \texttt{bag} module}

The \texttt{bag} module
\index{bag module}%
has various useful methods that operate on bags or
multisets:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
\texttt{empty()} & returns an empty bag\\
\hline
\texttt{fromSet($s$)} & create a bag from set $s$\\
\hline
\texttt{fromList($t$)} & convert list $t$ into a bag \\
\hline
\texttt{count($b$, $e$)} & count how many times $e$ occurs in bag $b$\\
\hline
\texttt{bchoose($b$)} & like \texttt{choose(s)}, but applied to a bag\\
\hline
\texttt{add}(\textit{pb}, $e$) & add one copy of $e$ to bag \texttt{!}\textit{pb}\\
\hline
\texttt{remove}(\textit{pb}, $e$) & remove one copy of $e$ from bag \texttt{!}\textit{pb}\\
\hline
\end{tabular}

\section{The \texttt{Hoare} module}
\label{ap:hoare}

\index{list module}%
The \texttt{hoare} module implements support for Hoare-style monitors
and condition variables.

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
\texttt{Monitor()} & return a monitor mutex\\
\hline
\texttt{enter($m$)} & enter a monitor.  $m$ points to a monitor mutex\\
\hline
\texttt{exit($m$)} & exit a monitor.\\
\hline
\texttt{Condition()} & return a condition variable\\
\hline
\texttt{wait($c$, $m$)} & wait on condition variable pointed to be $c$ in monitor pointed to by $m$\\
\hline
\texttt{signal($,$. $m$)} & signal a condition variable\\
\hline
\end{tabular}

\section{The \texttt{list} module}
\label{ap:list}

\index{list module}%
The \texttt{list} module has various useful methods that operate on lists
or tuples:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
\texttt{subseq($t$, $b$, $f$)} & return a \emph{slice} of list $t$ starting
at index $b$ and ending just before $f$\\
\hline
\texttt{append($t$, $e$)} & append $e$ to list $t$\\
\hline
\texttt{head($t$)} & return the first element of list $t$\\
\hline
\texttt{tail($t$)} & return all but the first element of list $t$\\
\hline
\texttt{reversed($t$)} & reverse a list \\
\hline
\texttt{sorted($t$)} & sorted set or list \\
\hline
\texttt{set($t$)} & convert values of a dict or list into a set \\
\hline
\texttt{list($t$)} & convert set into a list \\
\hline
\texttt{values($t$)} & convert values of a dict into a list sorted by key \\
\hline
\texttt{items($t$)} & convert dict into (key, value) list sorted by key \\
\hline
\texttt{enumerate($t$)} & like Python enumerate \\
\hline
\texttt{sum($t$)} & return the sum of all elements in $t$\\
\hline
\texttt{qsort($t$)} & sort list $t$ using quicksort\\
\hline
\end{tabular}

\section{The \texttt{set} module}

The \texttt{set} module
\index{set module}%
implements the following methods:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
\texttt{issubset($s$, $t$)} & returns whether $s$ is a subset of $t$ \\
\hline
\texttt{issuperset($s$, $t$)} & returns whether $s$ is a superset of $t$ \\
\hline
\end{tabular}

\vspace{1em}
For Python programmers: note that $s <= t$ does not check if $s$ is a subset of
$t$ when $s$ and $t$ are sets, as ``$<=$'' implements a total order on all Harmony values
including sets.

\section{The \texttt{synch} module}

\index{synch module}%
The \texttt{synch} module provides the following methods:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
\texttt{tas}(\textit{lk}) & test-and-set on \texttt{!}\textit{lk} \\
\hline
\texttt{BinSem}($v$) & return a binary semaphore initialized to $v$ \\
\hline
\texttt{Lock}() & return a binary semaphore initialized to \texttt{False} \\
\hline
\texttt{acquire}(\textit{bs}) & acquire binary semaphore \texttt{!}\textit{bs} \\
\hline
\texttt{release}(\textit{bs}) & release binary semaphore \texttt{!}\textit{bs} \\
\hline
\texttt{Condition}() & return a condition variable \\
\hline
\texttt{wait}($c$, \textit{lk}) & wait on condition variable \texttt{!}$c$
and lock \textit{lk} \\ \hline
\texttt{notify}($c$) & notify a thread waiting on condition variable \texttt{!}$c$ \\
\hline
\texttt{notifyAll}($c$) & notify all threads waiting on condition variable \texttt{!}$c$ \\
\hline
\texttt{Semaphore}(\textit{cnt}) & return a counting semaphore initialized to \textit{cnt} \\
\hline
\texttt{P}(\textit{sema}) & procure \texttt{!}\textit{sema}  \\
\hline
\texttt{V}(\textit{sema}) & vacate \texttt{!}\textit{sema}  \\
\hline
\texttt{Queue}() & return a synchronized queue object \\
\hline
\texttt{dequeue}($q$) & return next element of $q$, blocking if empty \\
\hline
\texttt{enqueue}($q$, \textit{item}) & add \textit{item} to $a$ \\
\hline
\end{tabular}

\chapter{List of Machine Instructions}
\label{ap:harmonybytecode}

{\small
\begin{tabular}{|l|l|}
\hline
Address & compute address from two components \\
\hline
Apply & pop $m$ and $i$ and apply $i$ to $m$, pushing a value \\
\hline
Assert & pop $b$ and check that it is \texttt{True} \\
\hline
AtomicInc & increment the atomic counter of this context \\
\hline
AtomicDec & decrement the atomic counter of this context \\
\hline
Continue & no-op (but causes a context switch) \\
\hline
Choose & choose an element from the set on top of the stack \\
\hline
Cut & pop set or dict and push minimum and remainder \\
\hline
Del [$v$] & delete shared variable $v$ \\
\hline
DelVar [$v$] & delete thread variable $v$ \\
\hline
Dict & pop $n$ and $n$ key/value pairs and push a dictionary \\
\hline
Dup & duplicate the top element of the stack \\
\hline
Frame \texttt{m} $a$ & start method \texttt{m} with arguments $a$,
initializing variables.  \\
\hline
Go & pop context and value, push value on context's stack, and add to context bag \\
\hline
Jump $p$ & set program counter to $p$ \\
\hline
JumpCond $e$ $p$ & pop expression and, if equal to $e$, set program counter to $p$ \\
\hline
Load [$v$] & push the value of a shared variable onto the stack \\
\hline
LoadVar [$v$] & push the value of a thread variable onto the stack \\
\hline
Move $i$ & move stack element at offset $i$ to top of the stack \\
\hline
$n$-ary \textit{op} & apply $n$-ary operator \textit{op} to the top $n$ elements on the stack \\
\hline
Pop & pop a value of the stack and discard it \\
\hline
Push $c$ & push constant $c$ onto the stack \\
\hline
ReadonlyInc & increment the read-only counter of this context \\
\hline
ReadonlyDec & decrement the read-only counter of this context \\
\hline
Return & pop return address, push \texttt{result}, and restore program counter \\
\hline
Set & pop $n$ and $n$ elements and push a set of the elements \\
\hline
SetIntLevel & pop $e$, set interrupt level to $e$, and push old interrupt level \\
\hline
Spawn & pop tag, argument, and method and spawn a new context \\
\hline
Split & pop tuple and push its elements \\
\hline
Stop [$v$] & save context into shared variable $v$ and remove from context bag \\
\hline
Store [$v$] & pop a value from the stack and store it in a shared variable \\
\hline
StoreVar [$v$] & pop a value from the stack and store it in a thread variable \\
\hline
Trap & pop interrupt argument and method \\
\hline
\end{tabular}
}

\newpage
Clarifications:
\begin{itemize}
\item The \texttt{Address} instruction expects two values on the stack.
The top value must be an address value, representing a dictionary 
The other value must be a key into the dictionary.
The instruction then computes the address of the given key.
\item Even though Harmony code does not allow taking addresses of thread variables, both
shared and thread variables can have addresses.
\item The \texttt{Load}, \texttt{LoadVar}, \texttt{Del}, \texttt{DelVar},
and \texttt{Stop} instructions have an optional
variable name: if omitted the top of the stack must contain the address of
the variable.
\item \texttt{Store} and \texttt{StoreVar} instructions have an optional
variable name.  In both cases the value to be assigned is on the top
of the stack.  If the name is omitted, the address is underneath that
value on the stack.
\item The effect of the \texttt{Apply} instructions depends much on $m$.
If $m$ is a dictionary, then \texttt{Apply} finds $i$ in the dictionary
and pushes the value.
If $m$ is a program counter, then \texttt{Apply} invokes method $m$ by
pushing the current program counter and setting the program counter to
$m$.  $m$ is supposed to leave the result on the stack.
\item The \texttt{Frame} instruction pushes the value of the thread
register (\emph{i.e.}, the values of the thread variables) onto the
stack.  It initializes the \texttt{result} variable to the empty dictionary.
The \texttt{Return} instruction restores the thread register by popping
its value of the stack.
\item All method calls have exactly one argument, although it sometimes
appears otherwise:
\begin{itemize}
\item \texttt{m()} invokes method \texttt{m} with the empty dictionary \texttt{()} as argument;
\item \texttt{m($a$)} invokes method \texttt{m} with argument $a$;
\item \texttt{m($a$, $b$, $c$)} invokes method \texttt{m} with tuple \texttt{($a$, $b$, $c$)} as argument.
\end{itemize}
The \texttt{Frame} instruction unpacks the argument to the method and places them into thread variables by the given names.
\item Every \texttt{Stop} instruction must immediately be followed by a 
\texttt{Continue} instruction.
\end{itemize}

\chapter{Contexts and Threads}
\label{app:context}

A context captures the state of a thread.  Each time the thread
executes an instruction, it goes from one context to another.
All instructions update the program counter (\texttt{Jump} instructions
are not allowed to jump to their own locations), and so no instruction
leaves the context the same.
There may be multiple threads
with the same state at the same time.
A context consists of the following:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
name tag & a dictionary with atoms \texttt{.name} and \texttt{.tag} \\
\hline
program counter & an integer value pointing into the code \\
\hline
frame pointer & an integer value pointing into the stack \\
\hline
atomic & if non-zero, the thread is in atomic mode \\
\hline
readonly & if non-zero, the thread is in read-only mode \\
\hline
stack & a list of Harmony values \\
\hline
register & a dictionary mapping atoms (names of variables) to values \\
\hline
stopped & a boolean indicating if the context is stopped \\
\hline
failure & if not None, string that describes how the thread failed \\
\hline
\end{tabular}
\vspace{1em}

Details:
\begin{itemize}
\item The name in a name tag is the name of the method that the thread
is executing;
\item The tag in a name tag the argument to the method by default.  Optionally
\texttt{spawn} allows the tag to be specified explicitly;
\item The frame pointer points to the current \emph{stack frame},
which consists of the caller's frame pointer and variables, the argument to
the method, an ``invocation type atom'' (\texttt{normal}, \texttt{interrupt}, or
\texttt{thread}), and the return address (in case of \texttt{normal}).
\item A thread terminates when it reaches the \texttt{Return} instruction
of the top-level method (when the stack frame is of type \texttt{thread})
or when it hits an exception.  Exceptions include divide by zero,
reading a non-existent key in a dictionary, accessing a non-existent
variable, infinite loops of microsteps, as well as when an assertion fails;
\item The execution of a thread in \emph{atomic mode} does not get interleaved
with that of other threads.
\item The execution of a thread in \emph{read-only mode} is not allowed
to update shared variables of spawn threads.
\item The register of a thread always contains a dictionary, mapping
atoms to arbitrary values.  The atoms correspond to the variable names
in a Harmony program.
\end{itemize}

\chapter{The Harmony Virtual Machine}

The Harmony Virtual Machine (HVM) has the following state:

\vspace{1em}
\begin{tabular}{|l|l|}
\hline
code & a list of HVM machine instructions \\
\hline
labels & a dictionary of atoms to program counters \\
\hline
variables & a dictionary mapping atoms to values \\
\hline
ctxbag & a bag of non-stopped contexts \\
\hline
stopbag & a bag of stopped contexts \\
\hline
choosing & if not \texttt{None}, indicates a context that is choosing \\
\hline
initializing & boolean indicating that the state is not fully initialized. \\
\hline
\end{tabular}
\vspace{1em}

There is initially a single context with nametag
\texttt{\_\_init\_\_/()} and program counter 0.  It starts executing
in atomic mode until it finishes executing the last instruction in
the code.  All states until then are \texttt{initializing} states.
Other threads, created through \texttt{spawn} statements, do not
start executing until then.

A \emph{micro step}
is the execution of a single HVM machine instruction
by a context.
Each micro step generates a new state.
When there are multiple contexts, the HVM can interleave them.
However, trying to interleave every microstep would be needlessly expensive,
as many micro steps involve changes to a context that are invisible to
other contexts.

A \emph{macro step}
\index{macro step}%
can involve multiple micro steps.  The following
instructions start a new macro step: \texttt{Load}, \texttt{Store},
\texttt{AtomicInc}, and \texttt{Continue}.  The HVM
interleaves macro steps, not micro steps.  Like micro steps, each
macro step involves a single context.  Unlike a micro step, a macro
step can leave the state unchanged.

Executing a Harmony program results in a graph where the nodes are Harmony
states and the edges are macro steps.
When a state is \texttt{choosing}, the edges from that state are
by a single context, one for each choice.  If not, the edges from
the state are one per context.

Consecutive macro steps by the same thread are called \emph{mega steps}.
Each state maintains the shortest path to it from the initial state in terms
of mega steps.
The diameter of the graph is the length of the longest path found.

If some states have a problem, the state with the shortest path is reported.
Problematic states include states that experienced exceptions.
If there are no exceptions, Harmony computes the strongly connected components (SCCs)
of the graph (the number of such components are printed as part of the output).
The sink SCCs should each consist of a terminal state without any threads.
If not, again the state with the shortest path is reported.

If there are no problematic states, Harmony reports ``no issues found'' and outputs
in the HTML file the state with the longest path.

\chapter{Harmony Language Details}
\label{ap:details}

The Harmony language borrows heavily from Python.  However, there are
some important differences that we will describe in this chapter.

\section{Harmony is not object-oriented}

Python is object-oriented, but Harmony is not.  This can lead to some
unexpected differences.  For example, consider the following code:

\begin{code}
\harmonysource{oo}
\end{code}

In Python, lists are objects.  Thus $x$ and $y$ point to the same list,
and the assertion would fail if executed by Python.
In Harmony, lists are values.  So when $x$ is updated in Line~3, it does
not affect the value of $y$.  The assertion succeeds.
Harmony supports references to values (\autoref{ch:method}),
allowing programs to implement shared objects.

Because Harmony does not have objects, it also does not have object methods.
However, Harmony methods and lambdas are program counter constants.
These constants can be added to dictionaries.
For example, in Figure~\ref{fig:petersonmethods}
you can add the \texttt{P\_enter} and
\texttt{P\_exit} methods to the \texttt{P\_mutex} dictionary
like so:
\begin{code}
\begin{verbatim}
dict{ .turn: 0, .flags: [ False, False ], .enter: P_enter, .exit: P_exit }
\end{verbatim}
\end{code}
That would allow you to simulate object methods.

\section{Constants, Global and Local Variables}

Each (non-reserved) identifier in a Harmony program refers to either
a constant, a global variable, or a local variable.
Constants are declared using \texttt{const} statements.
Those constants are computed at compile-time.

Local variables all declared.  They can be declared in
\texttt{def} statements (i.e., arguments),
\texttt{let} statements, and in \texttt{for} loops.
Also, each method has an implicitly declared
\textit{result} variable.
Local variables are tightly scoped and cannot be shared
between threads.
While in theory one method can be declared within another,
they cannot share variables either.
All other variables are global and must be initialized
before any threads are spawned.

While arguments to a method and variables in \texttt{for} loops
can be modified, we discourage it for improved code readability.

\section{Operator Precedence}

In Harmony, there is no syntactic difference between applying an argument to a function
or an index to a dictionary.  Both use the syntax \textit{a b c ...}.
We call this \emph{application}, and application is left-associative.
So \textit{a b c} is interpreted as (\textit{a b}) $c$: $b$ is applied to $a$,
and then $c$ is applied to the result.
For readability, it may help to write $a(b)$ for function application and
$a[b]$ for indexing.  In case $b$ is an atom, you can also write $a.b$ for indexing.

There are three classes of precedence.
Application has the highest precedence.  So \texttt{!}\textit{a b} is interpreted as
\texttt{!}(\textit{a b}) and \textit{a b} \texttt{+} \textit{c d} is interpreted as
(\textit{a b}) \texttt{+} (\textit{c d}).
Unary operators are have the next highest precedence,
and the remaining operators have the lowest precedence.
So $-2 + 3$ evaluates to 1, not $-5$.

Associative operators ($+$, $*$, $|$, $\string&$, $\string^$, \textbf{and}, \textbf{or})
are interpreted as general $n$-ary operators, and you are allowed to write
$a + b + c$.  However, $a - b - c$ is illegal, as is any combination of operators with an
arity larger than one, such as $a + b < c$.
In such cases you have to add parentheses or brackets to indicate what
the intended evaluation order is, such as $(a + b) < c$.

In almost all expressions, subexpressions are evaluated left to right.  So $a[b] + c$
first evaluates $a$, then $b$ (and then applies $b$ to $a$), and then $c$.  The one
exception is the expression $a$ \textbf{if} $c$ \textbf{else} $b$, where $c$ is evaluated
first.  In that expression, only $a$ or $b$ is evaluated depending on the value of $c$.
In the expression $a$ \textbf{and} $b$ \textbf{and} $...$, evaluation is left
to right but stops once one of the subexpressions evaluates to \textbf{False}.
Similarly for \textbf{or}, where evaluation stops once one of the subexpressions
evaluates to \texttt{True}.

As an aside:
the expression $a$ \textbf{not in} $b$ is equivalent to \textbf{not} ($a$ \textbf{in} $b$).
Harmony generalizes this construct for any pair of a unary (except '$-$') and a binary operator.
In particular, $a$ \textbf{not and} $b$ is the same as \textbf{not} ($a$ \textbf{and} $b$).  
For those familiar with logic gates, \textbf{not and} is the equivalent of \texttt{NAND}.

\section{Tuples, Lists, and Pattern Matching}

Harmony's tuples and, equivalently, lists, are just special cases of dictionaries.
They can be bracketed either by '(' and ')' or by '[' and ']', but
the brackets are often optional.  Importantly, with a singleton list, the
one element must be followed by a comma.
So the statement \texttt{$x$ = 1,;} assigns a singleton tuple (or list) to $x$.

Because tuples and lists are dictionaries, the \texttt{del} statement is
different than in Python.  For example, if $x$ = [.a, .b, .c], then
\texttt{del} $x$[1] results in $x$ = \texttt{dict}\{ 0:.a, 2:.c \}, \emph{not}
$x$ = [.a, .c].
Harmony also does not support special slicing syntax like Python.
To modify lists, use the \texttt{subseq} method in the \texttt{list} module
(\autoref{ap:list}).

Harmony allows pattern matching against nested tuples in various
language constructs.
\index{pattern matching}%
The following are the same in Python and Harmony:
\begin{itemize}
\item \texttt{$x,$ = 1,}: assigns 1 to $x$;
\item \texttt{$x$, $y$ = 1, (2, 3)}: assigns 1 to $x$ and (2, 3) to $y$;
\item \texttt{$x$, ($y$, $z$) = 1, (2, 3)}: assigns 1 to $x$, 2 to $y$, and
3 to $z$;
\item \texttt{$x$, ($y$, $z$) = 1, 2;} generates an runtime error because 2 cannot
be matched with ($y$, $z$);
\item \texttt{$x[0]$, $x[1]$ = $x[1]$, $x[0]$;} swaps the first two elements of list $x$.
\end{itemize}

As in Python, pattern matching can also be used in \texttt{for} statements.
For example:
\begin{code}
\texttt{for} key, value in [ (1, 2), (3, 4) ]:
    ...
\end{code}

Harmony (but not Python)
also allows pattern matching in defining and invoking methods.
For example, you can write:
\begin{code}
\texttt{def} f[$a$, ($b$, $c$)]: ...
\end{code}
and then call \texttt{f[1, (2, 3)]}.
Note that the more familiar: \texttt{def} g($a$) defines a method $g$ with
a single argument $a$.  Invoking g(1, 2) would assign the tuple (1, 2) to
$a$.  This is not consistent with Python syntax.  For single argument methods,
you may want to declare as follows: \texttt{def} g($a,$).
Calling g(1,) assigns 1 to $a$, while calling g(1, 2) would result in a
runtime error as (1, 2) cannot be matched with ($a$,).

Pattern matching can also be used in \texttt{const} and \texttt{let}
statements.

\section{For Loops and Comprehensions}

While Harmony does not support general iterators such as Python does,
Harmony allows iterating over sets and dictionaries (and thus lists
and tuples).  The details are a little different from Python:

\begin{itemize}
\item When iterating over a set, the set is always traversed in order
(see \autoref{ap:values} for how Harmony values are ordered);
\item In case of a dictionary, the iteration is over the \emph{values} of the
dictionary, but in the order of the keys.  In the case of lists, this
works much the same as in Python, but in the case of general dictionaries,
Python iterates over the keys rather than the values;
\item If you want to iterate over the keys of a dictionary $d$, use
\texttt{for} $k$ \texttt{in} \texttt{keys} $d$;
\item The \texttt{list} module (\autoref{ap:list}) provides methods \texttt{values}(),
\texttt{items}(), \texttt{enumerate}(), and \texttt{reversed}() for
other types of iteration supported by Python.
\end{itemize}

Harmony supports nesting and filtering in \texttt{for loops}.
For example:
\begin{code}
\texttt{for} $i$ in \{ 1..10 \}
\texttt{for} $j$ in \{ 1..10 \}
\texttt{such that} $i < j$: ...
\end{code}

Harmony also supports set, list, and dictionary comprehensions.
\index{comprehensions}%
Set and list comprehensions are similar to Python, except that filtering uses
the keywords \texttt{such that} instead of \texttt{if}.
Dictionary comprehensions are a little different.
The dictionary comprehension \texttt{dict}\{ f($i$) \texttt{for} $i$ \texttt{in} $I$ \}
creates a dictionary of $i$:f($i$) entries.

\section{Dynamic Allocation}

\begin{figure}
\begin{code}
\harmonysource{stacktest}
\end{code}
\caption{\harmonylink{code/stacktest.hny} Testing a stack implementation.}
\label{fig:stacktest}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{stack1}
\end{code}
\caption{\harmonylink{code/stack1.hny} Stack implemented using a dynamically updated list.}
\label{fig:stack1}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{stack2}
\end{code}
\caption{\harmonylink{code/stack2.hny} Stack implemented using static lists.}
\label{fig:stack2}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{stack3}
\end{code}
\caption{\harmonylink{code/stack3.hny} Stack implemented using a recursive tuple data structure.}
\label{fig:stack3}
\end{figure}

\begin{figure}
\begin{code}
\harmonysource{stack4}
\end{code}
\caption{\harmonylink{code/stack4.hny} Stack implemented using a linked list.}
\label{fig:stack4}
\end{figure}

Harmony supports various options for dynamic allocation.
By way of example, consider a stack.
\autoref{fig:stacktest} presents a test program for a stack.
We present four different stack implementations to illustrate
options for dynamic allocation:
\begin{enumerate}
\item[] \autoref{fig:stack1} uses a single list to represent the
stack.  It is updated to perform \texttt{push} and \texttt{pop} operations;
\item[] \autoref{fig:stack2} also uses a list but, instead of updating
the list, it replaces the list with a new one for each operation;
\item[] \autoref{fig:stack3} represents a stack as a recursively nested tuple
$(v, f)$, where $v$ is the element on top of the stack and $r$ is a stack
that is the remainder;
\item[] \autoref{fig:stack4} implements a stack as a linked list with nodes
allocated using the \texttt{alloc} module.
\end{enumerate}

While the last option is the most versatile (it allows cyclic
data structures), Harmony does not support garbage collection
for memory allocated this way and so allocated memory that is no
longer in use must be explicitly released using \texttt{free}.

\section{Comments}

Harmony supports the same commenting conventions as Python.
In addition, Harmony supports nested multi-line comments
of the form \texttt{(* comment *)}.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Acknowledgments}
\chapter*{Acknowledgments}

I received considerable help and inspiration from various people
while writing this book.

First and foremost I would like to think my student Haobin Ni with
whom I've had numerous discussions about the design of Harmony.
Haobin even contributed some code to the Harmony compiler.

Most of what I know about concurrent programming I learned from
my colleague Fred Schneider.  He suggested I write this book after
demonstrating Harmony to him.

Leslie Lamport introduced me to using model checking to test properties
of a concurrent system.  My experimentation with using TLC on Peterson's
Algorithm became an aha moment for me.

I first demonstrated Harmony to the students in my CS6480 class on systems
and formal verification and
received valuable feedback from them.

The following people contributed by making comments on or finding bugs in
early drafts of the book:
Alex Chang,
Anneke van Renesse,
Brendon Nguyen,
Heather Zheng,
Saleh Hassen,
Sunwook Kim,
Trishita Tiwari,
Yidan Wang,
Zhuoyu Xu,
and
Zoltan Csaki.

Finally, I would like to thank my family who had to suffer as I obsessed
over writing the code and the book during the turbulent months of
May and June 2020.

% I wrote the Harmony compiler and this book
% during the pandemic and the George Floyd demonstrations.

\cleardoublepage
\phantomsection
\printindex

\cleardoublepage
\phantomsection
\printglossaries

\end{document}
